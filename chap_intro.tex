\chapter{Introduction}


\section{Statement of Aims} 

This body of work is concerned with the methods of \textit{inverse problems} \citep{Tarantola2005,Aster2013,Menke2012,Kaipio2006,Biegler2010,Idier2013}. Other disciplines also refer to inverse problems as parameter inference or parameter estimation \citep{Box1973,Sprott2008,Casella1993,Cox2007}. Here they are considered synonymous. The methods of inverse problems underpin many critical experiments across disciplines. For geophysics it is the most important method which allows data to be transformed from surface observations into subsurface images. In response to limitations in traditional methods of geophysical parameter inference, probabilistic parameter inference has demonstrated utility in imaging the subsurface. With that in mind, this project seeks to explore the geophysical applicability of a recently developed method of probabilistic parameter inference which has emerged from applied challenges in genetics, \underline{A}pproximate \underline{B}ayesian \underline{C}omputation, (ABC). This project was started on January 5th, 2018 and concluded with the submission of this document on the 14th of October, 2018.

%Recently, methods of probabilistic parameter inference have risen to prominence within the geophysics community.


\section{Parameter inference}

Model parameter inference based on observable data is a pursuit which transcends scientific disciplines. Here, the unknown parameters, $\bm{\theta} = \{\theta_1,...,\theta_N\}$, for a causative model, $\mathcal{M}$, are sought based on observed data, $\bm{y} = \{y_1,...,y_M\}$. The relationship between model parameters and data may be highly non-linear and is represented through the operator $\bm{g}$:
\begin{equation}
\bm{y} = \bm{g}(\bm{\theta})
\label{basic_data_parameters}
\end{equation}	
In geophysics we are interested in defining the physical properties  of the subsurface (e.g. seismic velocity, density) from experimental data, $\bm{y}$, observed at the surface (e.g. seismic signals, gravitational acceleration). By using physical models, $\mathcal{M}$, which link a parameterized subsurface and resulting simulated data, $\bm{y'}$, the pursuit of imaging the subsurface collapses to a parameter inference problem \citep[p.1-2]{Tarantola2005}.\par

Geophysical parameter inference problems are commonly referred to as \textit{inverse problems} \citep{Tarantola2005,Aster2013,Menke2012}. They start with the experimental data, $\bm{y}$, and try to infer the causative model parameters, $\bm{\theta}$. As opposed to \textit{forward problems}, which start with a given set of model parameters, $\bm{\theta'}$, and produce a set of simulated data, $\bm{y'}$. Both areas are subject to considerable research and development. The ability to solve the forward problem is required to solve the inverse problem.
\begin{equation}
\text{The inverse problem:}\ \bm{y} \rightarrow \bm{\theta}
\label{inverse_problem}
\end{equation}
\begin{equation}
\text{The forward problem:}\ \bm{\theta'} \rightarrow \bm{y'}
\label{forward_problem}
\end{equation}


\section{Probabilistic formulation}

Geophysical inverse problems come with their own set of challenges. Often the experimental data has significant levels of uncertainty, the number of unknown model parameters may far exceed the amount of constraining data and the uncertainty in the physical model (the forward problem) may itself be large. These conditions weaken robust traditional methods of parameter estimation.\par

To quantify uncertainty in the data and forward problem, scrutinize model space non-uniqueness and constrain the solution with alternate information, probabilistic formulations to geophysical inverse problems have been developed, based on Bayes' theorem, equation \ref{bayes} \citep{Tarantola1982a,Mosegaard1995,Mosegaard2002,Tarantola2005}. Here, geophysical inverse problems receive a full statistical treatment of uncertainty. A probability density function (PDF) encodes the uncertainty, information and belief in the data, physical model and solution. The Bayesian method is fundamentally about embracing uncertainty and making use of all available information to update our state of knowledge. Bayes' theorem relies on the notion of conditional probabilities. $p(a|b)$ is the conditional probability of $a$ given event $b$ has occurred. Bayes' theorem is used to define the solution to the inverse problem as the PDF of the model parameters $\bm{\theta}$ conditioned on the observed data $\bm{y}$, $p(\bm{\theta}|\bm{y})$. This PDF is referred to as the \textit{posterior}. For the case of experimental data, $\bm{y}$, and unknown model parameters, $\bm{\theta}$, Bayes' theorem can be applied in the form:
\begin{equation}
p(\bm{\theta}|\bm{y}) = \frac{p(\bm{\theta}) p(\bm{y}|\bm{\theta})}{p(\bm{y})}
\label{bayes}
\end{equation}
Equation \ref{bayes} states that the prior distribution, $p(\bm{\theta})$, multiplied by the likelihood, $p(\bm{y}|\bm{\theta})$, over a normalization constant, $p(\bm{y})$, is equal to the posterior distribution, $p(\bm{\theta}|\bm{y})$.\par

The prior, $p(\bm{\theta})$, is a user-defined probability distribution which incorporates what is already known about the model parameters, $\bm{\theta}$, independent of the experimental data, $\bm{y}$. When trying to understand subsurface structure with surface data there may be the additional prior knowledge. For example, the rock on the left of a vertical fault has a density of 2570 kg/m$^3$, or, the thickness of layers in a vertical stack is distributed according to a exponential probability distribution \citep{Mosegaard1995}. This type of independent information can be encoded as the prior PDF.\par

The likelihood, $p(\bm{y}|\bm{\theta})$, allows the data to dictate the plausibility of different sets of model parameters. Through the likelihood we learn about the support the data has for different sets of model parameters. If for two possible sets of model parameters $\bm{\theta_1}$ and $\bm{\theta_2}$ we have $p(\bm{y}|\bm{\theta_1}) > p(\bm{y}|\bm{\theta_2})$ then $\bm{y}$ is more likely to occur under $\bm{\theta_1}$ than $\bm{\theta_2}$. Hence the term likelihood. The likelihood is a central part of this thesis, it is expanded upon in the next section, as well as \hyperref[tf1]{Technical figure 1} and \hyperref[tf2]{Technical figure 2}. \par

The normalization constant $p(\bm{y}) = \int p(\bm{\theta}) p(\bm{y}|\bm{\theta})\ \text{d}\bm{\theta}$ is generally unconsidered. Statistics about the posterior such as maximum posterior value, mean, and standard deviation can still be computed from an unnormalized posterior density. The consequence of leaving $p(\bm{y})$ unevaluated is simply that single point values of the posterior have no meaning. Instead relative values must be evaluated, where ratios cancel the normalization constant. Hence, Bayes' theorem is practically applied in the form:
\begin{equation}
p(\bm{\theta}|\bm{y}) \propto p(\bm{\theta}) p(\bm{y}|\bm{\theta})
\label{applied_bayes}	
\end{equation}
The posterior distribution, $p(\bm{\theta}|\bm{y})$, is the solution to a probabilistic formulation of an inverse problem. It is the complete state of knowledge about $\bm{\theta}$ given $\bm{y}$. Complete as it incorporates both what is already known, $p(\bm{\theta})$, as well as new experimental data, $\bm{y}$, through the likelihood. The probabilistic solution offers a philosophical shift compared to traditional solutions. A probability distribution over all model parameters is recovered, instead of a discrete set of model parameters, offering in-built quantification of uncertainty. \par

For non-trivial problems of scientific interest there is no analytical solution for the posterior. One of the possible methods to then define the posterior $p(\bm{\theta}|\bm{y})$ is stochastic sampling such as Monte Carlo or Markov chain Monte Carlo (MCMC). For problems where there are many unknown model parameters and each sample is computationally expensive, highly efficient sampling methods are needed for the posterior to be computationally tractable. \par

The probabilistic solution to inverse problems outlined above has been successfully applied to tomography \citep{Zhao1996,Sambridge1999,Shapiro2002,Trampert2004,Khan2011}. Probabilistic multiobservable tomography has also been developed  and applied \citep{khan2007joint,Moorkamp2010,Bodin2012,Shen2012,afonso2013a,afonso2013b,afonso2016}. Multiobservable methods take into account more constraining data and hence minimize the range of acceptable models consistent with the data. Different observables and their associated physical models are sensitive to different parts of the system under study and can be brought in on the basis that they better constrain $\bm{\theta}$. This is important due to the inherent non-uniqueness and large uncertainties of some traditional tomography methods. The result has been more informed inference and a better understanding of the uncertainty in conclusions drawn.

\section{The likelihood}

The likelihood is central to both Bayesian and frequentest statistical parameter inference. Both specifications of the prior distribution and likelihood are at the heart of Bayesian inference, the probabilistic framework adopted here. In practice, due attention should be paid to both, as they both play central roles in the posterior, equation \ref{applied_bayes}. However, as will be introduced in depth later, Approximate Bayesian Computation diverges from traditional Bayesian inference by circumventing calculation of the likelihood, instead using an approximation to the likelihood. Given ABC will move away from its use, it is important to understand what the likelihood offered and how it functioned. A particularly important component is the assumptions made in constructing the likelihood. \par

The likelihood is used after $\bm{y}$ is available to describe the plausibility of $\bm{\theta}$. In this light, $p(\bm{y}|\bm{\theta})$ can be considered as a conditional probability given $\bm{y}$, for varying values of $\bm{\theta}$ \citep[p.10]{Box1973}. Following \citet{Fisher1922}, this is referred to as the likelihood and is commonly denoted $\mathcal{L}(\bm{\theta}|\bm{y})$. This maps out a distribution of relative plausibility over $\bm{\theta}$ in describing the observed data $\bm{y}$. The likelihood is not necessarily a PDF and hence point values contain no inherent information. Only through comparison of relative values does the likelihood gain meaning \citep[p.11]{Box1973}.\par

%The likelihood is used after $\bm{y}$ is available to describe the plausibility of $\bm{\theta}$. It is a function of the parameters $\bm{\theta}$ to the model $\mathcal{M}$, given observed data $\bm{y}$. Hence it is denoted $\mathcal{L}(\bm{\theta}|\bm{y})$. However, the plausibility of $\bm{\theta}$ given $\bm{y}$ is proportional to the PDF of $\bm{y}$ given $\bm{\theta}$. This gives us the paradoxical statement:
%\begin{equation}
%\mathcal{L}(\bm{\theta}|\bm{y}) \propto \pi(\bm{y}|\bm{\theta})
%\label{likelihood_def}
%\end{equation}
%However this is resolved by considering $\pi(\bm{y}|\bm{\theta})$ at the observed $\bm{y}$, hence fixed, for varying $\bm{\theta}$. This maps out a distribution of relative plausibility over $\bm{\theta}$. The likelihood is not necessarily a PDF and hence point values contain no inherent information. Only through comparison of relative values does the likelihood gain meaning.\par

A full derivation of the likelihood is featured in \hyperref[tf1]{Technical figure 1} and \hyperref[tf1]{Technical figure 2}, from first assumptions (\hyperref[tf1]{Technical figure 1}) to the final forms which are commonly featured in geophysical inversions (\hyperref[tf1]{Technical figure 2}).\par

Generally, a given observed data point, $y_i$, is considered to be equal to simulated data from a parameterized deterministic physical model, $\bm{g}(\bm{\theta})$, with statistical uncertainty from both the measurement process, $e^{\mathcal{D}}_i$, and the modelization process, $e^{\mathcal{M}}_i$:
\begin{equation}
y_i = z_i + e^{\mathcal{D}}_i
\end{equation}
\begin{equation}
z_i = \bm{g}(\bm{\theta}) + e^{\mathcal{M}}_i
\end{equation}
\begin{equation}
y_i = \bm{g}(\bm{\theta}) + e^{\mathcal{M}}_i + e^{\mathcal{D}}_i
\end{equation}
For the case of estimating $\bm{\theta} = \{\theta_1,...,\theta_N\}$ given $\bm{y} = \{y_1,...,y_M\}$, if it is assumed that the nature of the statistical uncertainty of measurement and modelization is i.i.d from a Gaussian distribution then the likelihood takes the form \citet[p.91-92]{gregory2005bayesian}: 
\begin{equation}
\begin{split}
\mathcal{L}(\bm{\theta}|\bm{y}) &= p(\bm{y}|\bm{\theta}) = p(y_1,...,y_m|\bm{\theta})\\
&\propto  \text{exp}\bigg[\sum_{i = 1}^{M}\frac{-(y_i-\bm{g}({\bm{\theta}}))^2}{2\big((\sigma^{\mathcal{M}}_i)^2+(\sigma^{\mathcal{D}}_i)^2\big)}\bigg]
\end{split}
\label{likelihood-1}
\end{equation}

% In case in need to include the above likelihood scaling
% (2\pi)^{M/2}\Big(\prod_{i = 1}^{M}\big((\sigma^{\mathcal{M}}_i)^2+(\sigma^{\mathcal{D}}_i)^2\big)^{1/2}\Big)\


Otherwise, if the uncertainty is correlated in some manner, but still distributed according to a Gaussian distribution, multivariate Gaussian distributions with covariance matrices $C_{\mathcal{D}}$ and $C_{\mathcal{M}}$ are combined to quantify the total statistical uncertainty and define a likelihood \citet[p.35-36]{Tarantola2005}:
\begin{equation}
\mathcal{L}(\bm{\theta}|\bm{y}) \propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}-\bm{g}(\bm{\theta}))^T(C_{\mathcal{M}}+C_{\mathcal{D}})^{-1}(\bm{y}-\bm{g}(\bm{\theta}))\bigg]
\label{likelihood-2}
\end{equation}
\newpage
\input{technical_figure1.tex}
If we had continued further and assumed more complicated models for our uncertainty than Gaussian distributions, then access to our current inference may close entirely. For example, if it was assumed that the statistical uncertainty in the measurement process was described by a uniform distribution and the modelization process described by an exponential distribution, then making the same derivations as is done in (\hyperref[tf1]{Technical figure 2}) may not be possible. Our problem would have an intractable likelihood. It is this bottleneck which was encountered in genetics, and lead to the development of the likelihood-free methods of Approximate Bayesian Computation. Here, the forward model is fundamentally random. The coalescent \citep{Marjoram2006} describes how gene variants are passed down from a common ancestor, where coalescent events are approximated by an exponential distribution. This model is then overprinted by mutations, described by a Poisson distribution, for example. As the number of samples of DNA expanded, it was not possible to formulate a likelihood to describe the time to the most recent common ancestor for the set. The likelihood was intractable.\par

It should also be noted that many of the assumptions made about the nature of statistical uncertainty form scientifically testable hypothesis. For example, it should be possible to determine whether the noise from the measurement process conforms to an i.i.d Gaussian distribution, or whether there is some degree of correlation. Likewise, it can be probed as to whether the noise conforms to a Gaussian, or perhaps some other distribution is more appropriate.\par

The stable computation of equation \ref{likelihood-1} and equation \ref{likelihood-2} is an important aspect of running algorithms which sample the posterior distribution. Often likelihood values can be extremely small, to the point where they approach and cross the limits of CPU memory. Crossing this limit would lead to a divergence from posterior sampling due to erraneous algorithm steps. \hyperref[tf3]{Technical figure 3} expands on this topic. It covers the form in which the calculations take in order to be numerically stable.\par

The likelihood for a single dataset can be built upon to form a joint likelihood. As long as the two datasets are independent then equations \ref{likelihood-1} and \ref{likelihood-2} can be expanded for two datasets, here $a$ and $b$. Consider the joint form of equation \ref{likelihood-2}:
\begin{multline}
\mathcal{L}(\bm{\theta}|\bm{y}^a,\bm{y}^b) = \mathcal{L}(\bm{\theta}|\bm{y}^a)\ \mathcal{L}(\bm{\theta}|\bm{y}^b)
\propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}^a-\bm{g}^a(\bm{\theta}))^T(C_{\mathcal{M}^a}+C_{\mathcal{D}^a})^{-1}(\bm{y}^a-\bm{g}^a(\bm{\theta}))\bigg]\ \\
\text{exp}\bigg[-\frac{1}{2}(\bm{y}^b-\bm{g}^b(\bm{\theta}))^T(C_{\mathcal{M}^b}+C_{\mathcal{D}^b})^{-1}(\bm{y}^b-\bm{g}^b(\bm{\theta}))\bigg]
\label{joint-likelihood}
\end{multline}
As a result of the diverging scale of various dataset residuals, $(\bm{y}-\bm{g}(\bm{\theta}))^T(C_{\mathcal{M}}+C_{\mathcal{D}})^{-1}(\bm{y}-\bm{g}(\bm{\theta}))$, the joint likelihood equation \ref{joint-likelihood} can be overwhelmed or dominated by a system with significantly more data points or larger residual values. This impact can also extend through to the prior, where a joint or single likelihood can overwhelm a prior. This problem called for a solution in the form of a weighting scheme to balance the input of the different datasets. Some common forms of weighting schemes for two datasets are:
\begin{equation}
\text{residual}^a, \frac{1}{c}\ \text{residual}^b
\end{equation}
\begin{equation}
c\ \text{residual}^a, 1-c\ \text{residual}^b
\end{equation}
where $c$ is the weighting factor to be defined. The most common scheme for defining the weighting factors is ad-hoc. It relies on running synthetic tests to determine when the solution is giving a balance between the two datasets. This approach is likely to be biased by our expectations of synthetic tests. The resulting parameter uncertainties may not truly reflect the uncertainty associated with the combination of the two complimentary datasets. \par

%At this stage this ad-hoc weighting is deemed necessary. There is no other obvious way to balance the impact each dataset has in the currently established likelihood framework. New methods in statistics, which circumvent evaluating the likelihood may offer improved statistical properties in light of the shortfalls in combining datasets with traditional likelihood machinery. \par

%The likelihood in its current form, mixes and dilutes the information about closeness and model performances into a single metric. While this is necessary for calculation of the M-H acceptance ratio, this may not be the best way to guide an inversion scheme. More aspects of the model can be leveraged to explore areas of the parameter space which are deemed important, and this can be fed from the residuals to the proposal distribution so as to avoid blind, wasteful updates in an inversion scheme. Simulated datasets give us a lot of information which can never be captured by a single metric of mixed residuals. As such, it may be very worthwhile trying to leverage the available information.\par

%Other disciplines have found Approximate Bayesian Computation offers an alternative to traditional likelihood machinery which can utilize more of what we know about the structure, physics and nature of the problems at hand. This is contrast to the simplifications which have been implicit in the construction of Bayesian inference and the form of the algorithms. Ideally we should be able to leverage the true complexity of the problem and the breadth of information we already know. In light of this goal Approximate Bayesian Computation is introduced in the next section.  

The likelihood, while underpinning a system which is formally strong and practically useful for geophysics, does suffer some weaknesses which are implicit in its construction and application. The likelihood limits the number of models (combined deterministic forward and uncertainty) which can be considered and applied. It relies on informal tuning for joint inversions which may compromise the strict formal statistical properties which the Bayesian method guarantees to deliver. The likelihood also mixes and dilutes the available information about the data fit and model into a single metric. While necessary in the likelihood framework, this may not be the best way to drive an inversion scheme. Other disciplines have found Approximate Bayesian Computation offers an alternative to traditional likelihood machinery which can utilize more of what is known about the structure, physics and nature of the parameter inference problems at hand \cite{Tavare1997,Ratmann2009,vrugt2013toward}. It opens inference to a broader range of models and likewise offers formal statistical guarantees \citep{Sunnaker2013}. In light of this potential build upon the progress made in probabilistic tomography, Approximate Bayesian Computation is introduced in the next section. \\

\input{technical_figure2.tex}

\pagebreak
\input{technical_figure3.tex}


\section{Approximate Bayesian Computation}
\label{ApproximateBayesianComputation}

Likelihood-free methods for Bayesian inference have been developed in response to parameter estimation problems of scientific interest where it is not possible to formulate or justify an explicit likelihood function \citep{Tavare1997,Fu1997,Weiss1998a,Pritchard1999a,Beaumont2002,Marjoram2003}. Instead, likelihood-free methods target the same posterior distribution without evaluating a likelihood function. The algorithms and methods of likelihood-free Bayesian inference have been termed \textit{Approximate Bayesian Computation} (ABC). Generally, ABC simply requires the ability to simulate data given model parameters. Problems ripe for attack by ABC are common in science due to the breadth of models which have been developed to describe natural systems. It is frequently the case that data can be simulated rapidly but explicit formulas for probability distributions are difficult to formulate, expensive to evaluate, impossible to justify, or do not exist. Hence, traditional likelihood machinery becomes infeasible. ABC is a means to overcome these issues, it is backed by a sound theoretical underpinning and is subject to a rapidly expanding set of literature \citep{Ratmann2009,Blum2010,vrugt2013toward,Sunnaker2013,Blum2013,Sadegh2014,Pudlo2015,meeds2015hamiltonian,Lintusaari2016,gutmann2016bayesian,sisson2016handbook,Li2017}. \par

The premise of ABC begins with the notion that a set of model parameters, $\bm{\theta}$, is a sample from the posterior if the observed data, $\bm{y}$, and a simulated dataset, $\bm{y^*}$, are equal. Algorithm \ref{basicalg} generates i.i.d samples from the posterior $p(\bm{\theta}|\bm{y})$ (cf. \citet{Marjoram2003}).

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Generate $\bm{\theta^*}$ as a random sample from $p(\bm{\theta})$		
		\State 2. Simulate $\bm{y^*}$ from the forward operator $\bm{g_s}(\bm{\theta^*})$		
		\State 3. Accept $\bm{\theta^*}$ as a posterior sample if $\bm{y^*} = \bm{y}$		
		\State 4. Repeat
	\end{algorithmic}
	\label{basicalg}
\end{algorithm}

The operator $\bm{g_s}$ in Algorithm \ref{basicalg}, and all of likelihood-free inference, differs from the forward operators we are used to thinking about in geophysics. Traditionally, forward operators in geophysics are \textit{deterministic}. That is, for a given set of model parameters, $\bm{\theta}$, the output of the forward operator $\bm{g}(\bm{\theta})$ will always be the same. The uncertainty in both data and model, are then later built into the construction of a likelihood function. The ABC forward operator which simulates data, $\bm{g_s}$, is \textit{stochastic}. That is, the uncertainty from both data and modelization is built into the simulation process. As a result of this modification, algorithm \ref{basicalg} does not require the formulation or evaluation of a likelihood function. The ABC forward operator $\bm{g_s}$ allows a practitioner to build in whatever uncertainty is justified for the scientific problem at hand. \par

Algorithm \ref{basicalg} is acceptable for basic problems where the datasets $\bm{y}$ and $\bm{y^*}$ are discrete \citep{Tavare1997,Fu1997}. However, for large and continuous datasets the probability of generating a sample where the acceptance criteria, $\bm{y^*} = \bm{y}$, is met diminishes to levels unacceptable for parameter inference. ABC relaxes the problematic requirement of equality by accepting samples when the distance between $\bm{y^*}$ and $\bm{y}$, $\text{d}(\bm{y},\bm{y^*})$, is less than a tolerance value $\epsilon$ \citep{Weiss1998a}. This method, algorithm \ref{ABCfulldatatolerance}, does not target our true posterior of interest, but instead, the ABC posterior $p(\bm{\theta}|\text{d}(\bm{y},\bm{y^*})\leq\epsilon)$ which approximates the true posterior. 

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Generate $\bm{\theta^*}$ as a random sample from $p(\bm{\theta})$		
		\State 2. Simulate $\bm{y}$ from the forward operator $\bm{g_s}(\bm{\theta^*})$		
		\State 3. Accept $\bm{\theta^*}$ as a posterior sample if $\text{d}(\bm{y},\bm{y^*})\leq\epsilon$		
		\State 4. Repeat
	\end{algorithmic}
	\label{ABCfulldatatolerance}
\end{algorithm}

Algorithm \ref{ABCfulldatatolerance} requires a user specified metric, $\text{d}$, which defines the distance between datasets. Common choices are the absolute-value norm, Euclidean distance and Mahalanobis distance. Likewise, the value for the tolerance $\epsilon$ must be user specified. As $\epsilon \rightarrow \infty$, the sampled target distribution of algorithm \ref{ABCfulldatatolerance} $p(\bm{\theta}|\text{d}(\bm{y},\bm{y^*})\leq\epsilon) \rightarrow p(\bm{\theta})$.  Algorithm \ref{ABCfulldatatolerance} simply recovers the prior distribution. However, as $\epsilon \rightarrow 0$ the sampled distribution $p(\bm{\theta}|\text{d}(\bm{y},\bm{y^*})\leq\epsilon) \rightarrow p(\bm{\theta}|\bm{y})$. The exact posterior is recovered. Encoded in the value of $\epsilon$ is a trade-off between acceptance rate and accuracy. As $\epsilon$ increases so does the efficiency of the sampler, but the accuracy of the recovered distribution is increasingly eroded \citep{Sisson2010a}. Conversely, as $\epsilon$ decreases the accuracy of the recovered distribution converges to the true posterior, but the acceptance rate approaches computationally infeasible levels. This trade-off for computational efficiency at the cost of accuracy is where ABC derives the name \textit{Approximate} Bayesian Computation, as the samplers recover a distribution which approximates the true posterior.\par

% Write about summary statistics
Rejection sampler inference in the form of algorithm \ref{ABCfulldatatolerance} can run into efficiency problems as the dimensionality (size) of the datasets grow. In this case the probability of sampling $\text{d}(\bm{y},\bm{y^*})\leq\epsilon$ diminishes as the size of the dataset grows. Since first application (\citep{Tavare1997}) likelihood-free methods have adopted the use of low-dimensional summary statistics about the data in the evaluation of distance, $\text{d}$. A metric over a vector of summary statistics is evaluated, $\text{d}(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))$. This leads to the most common form of an ABC rejection sampling scheme of algorithm \ref{ABCrejectionsampler} \citep{Pritchard1999a}. This rejection sampler generates i.i.d samples from the distribution $p(\bm{\theta}|\text{d}(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))\leq\epsilon)$.

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Generate $\bm{\theta^*}$ as a random sample from $p(\bm{\theta})$		
		\State 2. Simulate $\bm{y}$ from the forward operator $\bm{g_s}(\bm{\theta^*})$		
		\State 3. Compute summary statistics $\bm{S}(\bm{y})$ and $\bm{S}(\bm{y^*})$		
		\State 4. Accept $\bm{\theta^*}$ as a posterior sample if $\text{d}(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))\leq\epsilon$		
		\State 5. Repeat
	\end{algorithmic}
	\label{ABCrejectionsampler}
\end{algorithm}

In the first application of likelihood-free inference, \citet{Tavare1997} replace the full sequences of DNA with the number of sites which differ between DNA samples. The idea being that as long as the statistics used are \textit{sufficient} there is no information loss for parameter inference. As a result of sufficiency the posterior computed with statistics will be equivalent to the the posterior computed by the full dataset, i.e $p(\bm{\theta}|\bm{S}(\bm{y})) = p(\bm{\theta}|\bm{y})$. In reality, no truly sufficient statistics exist for problems of scientific interest. Instead practitioners settle for a reasonably sufficient low-dimensional set of summary statistics. This lack of sufficiency introduces a second bias, tolerance being the first, into the ABC-posterior $p(\bm{\theta}|\text{d}(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))\leq\epsilon)$ due to the information lost by summarizing the full data set with statistics. It is, however, thanks to summary statistics that likelihood-free inference owes it's origin and can proceed. Summary statistics have opened parameter inference to problems as challenging as noisy near-chaotic ecology populations \citep{Wood2010} and have been shown to offer superior power to diagnose model insufficiency \citep{Ratmann2009,vrugt2013toward}.\par

% talk about MCMC-ABC
Seminal developments in ABC have considered algorithms and equations in the form which have been introduced so far \citep{Fu1997,Pritchard1999a,Beaumont2002,Marjoram2003}. However, ABC can be cast in another mathematical light which will enable straight forward implementation into advanced sampling routines such as MCMC. Here, the stochastic forward simulations from ABC, $\bm{y^*}$, are viewed as an auxiliary parameter which is introduced into the posterior to facilitate computation \citep{Sisson2010a}. This process changes the computed posterior from the traditional $p(\bm{\theta}|\bm{y}) \propto p(\bm{y}|\bm{\theta})p(\bm{\theta})$ to the ABC posterior:

\begin{equation}
p_{ABC}(\bm{\theta},\bm{y^*}|\bm{y}) \propto p(\bm{y}|\bm{y^*},\bm{\theta}) p(\bm{y^*}|\bm{\theta}) p(\bm{\theta})
\label{eqABCposterior}
\end{equation}

where $\bm{y^*}$ is viewed as a realization from the density $p(\bm{y^*}|\bm{\theta})$. $p(\bm{y}|\bm{y^*},\bm{\theta})$ is introduced to serve the same role as the accept/reject step in Algorithms \ref{ABCfulldatatolerance} and \ref{ABCrejectionsampler}. $p(\bm{y}|\bm{y^*},\bm{\theta})$ is chosen to weight the posterior with high values when the observed and simulated datasets are close. However, the form of equation \ref{eqABCposterior} allows the mathematics of kernel densities to be introduced into $p(\bm{y}|\bm{y^*},\bm{\theta})$ such that \citep{Sisson2010a}:
\begin{equation}
p(\bm{y}|\bm{y^*},\bm{\theta}) = \frac{1}{\epsilon} K \Big(\frac{\text{d}(\bm{y},\bm{y^*})}{\epsilon}\Big)
\label{generic-weighting-kernel}
\end{equation}
Where $K$ is some standard kernel, and the tolerance $\epsilon$ serves as the kernel bandwidth. \par

Thinking of equation \ref{eqABCposterior} in algorithm form still allows a straightforward understanding: 1. a set of parameters is simulated from $p(\bm{\theta})$ 2. a dataset is stochastically simulated from $p(\bm{y^*}|\bm{\theta})$ 3. The distance between simulated and observed datasets is evaluated 4. The parameter set is accepted with probability equal to $p(\bm{y}|\bm{y^*},\bm{\theta})$\par

While equation \ref{eqABCposterior} targets the joint posterior density of simulations and model parameters, marginalization to our posterior of interest, $p_{abc}(\bm{\theta}|\bm{y})$, is done numerically by discarding the simulations recovering:
\begin{equation}
p_{ABC}(\bm{\theta}|\bm{y}) \propto p(\bm{\theta}) \int_{\bm{y^*}} p(\bm{y}|\bm{y^*},\bm{\theta}) p(\bm{y^*}|\bm{\theta})\ \text{d}\bm{y^*}
\label{eqABCtargetPosterior}
\end{equation}\par

Equation \ref{eqABCtargetPosterior} can be adjusted to rely on summary statistics. The density $p(\bm{S}(\bm{y^*})|\bm{\theta})$ is introduced as the density implied from taking summary statistics about $p(\bm{y^*}|\bm{\theta})$ and $p(\bm{S}(\bm{y})|\bm{S}(\bm{y^*}),\bm{\theta})$ is a kernel over the distance between summary statistics. Our approximate Bayesian posterior distribution becomes:
\begin{equation}
p_{ABC}(\bm{\theta}|\bm{S}(\bm{y})) \propto p(\bm{\theta}) \int_{\bm{S}(\bm{y^*})} p(\bm{S}(\bm{y})|\bm{S}(\bm{y^*}),\bm{\theta})\  p(\bm{S}(\bm{y^*})|\bm{\theta})\ \text{d}\bm{S}(\bm{y^*})
\label{summary-stat-abc-posterior}
\end{equation}

\citet{Marjoram2003} first demonstrated a Markov chain Monte Carlo (MCMC) scheme to sample the ABC posterior distribution. A simple MCMC algorithm with a Metropolis-Hastings (MH) acceptance probability is demonstrated in algorithm \ref{ABC-MCMC}. The M-H acceptance probability $\alpha$ to recover the ABC posterior is:
\begin{equation}
\alpha_{ABC} = \frac{p(\bm{S}(\bm{y})|\bm{S}(\bm{y^*}),\bm{\theta})\ p(\bm{\theta^*})\ q(\bm{\theta^*},\bm{\theta_{t-1}})} {p(\bm{S}(\bm{y})|\bm{S}(\bm{y_{t-1}}),\bm{\theta})\ p(\bm{\theta_{t-1}})\ q(\bm{\theta_{t-1}},\bm{\theta^*})}
\label{M-H-acce}
\end{equation}
Where $\bm{t}$ denotes the time step in the chain, and $\bm{\theta^*}$ is used to represent a candidate move from the proposal distribution $q(\bm{\theta_{t-1}},\cdot)$.

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Start from an initial state $\bm{\theta^0}$ and select a proposal distribution $q(\cdot,\cdot)$
		\State 2. At each step where the current state is $\bm{\theta_{t-1}}$, propose a candidate 	move $\bm{\theta^*}$ from the distribution $q(\bm{\theta_{t-1}},\cdot)$		
		\State 3. If the candidate state is better than the previous state, $\alpha_{ABC} > 1$, then the candidate state is accepted unconditionally meaning $\bm{\theta_t} = \bm{\theta^*}$
		\State 4. If the candidate move is not better in the above sense, then $\bm{\theta^*}$ is accepted with probability equal to $\alpha_{ABC}$		
		\State 5. If the candidate move is not accepted, then the chain remains in its current state, meaning $\bm{\theta_{t}} = \bm{\theta_{t-1}}$		
		\State 6. Repeat the simulation steps 2-5 until enough values have been generated
	\end{algorithmic}
	\label{ABC-MCMC}
\end{algorithm}

% Give a general history and propose a goal and oulook for the analysis
ABC originated from applied problems where a likelihood was not available. As we saw in the last section, and expanded upon in \hyperref[tf2]{technical figure 2}, geophysics has been able to leverage likelihoods, for example, in the form of equations \ref{likelihood-1} and \ref{likelihood-2}. This access to a likelihood is the direct result of the assumptions about the statistical properties of the measurement and modelization uncertainty. However, under more complex models for uncertainty, an analytical formula for the likelihood may not be accessible. This is where ABC algorithms have traditionally been able to step in, bypassing using a likelihood, and in the process opening parameter inference a range of more complex models. \par

%Our hope is that ABC can offer improvements over several limiting aspects of traditional likelihood based Bayesian inference. These improvements may be focused around several aspects. One is computing a joint likelihood in a balanced and statistically uncompromisable manner. Another is making inversion schemes more diagnostic. At each step in an algorithm we can leverage the full dataset to decide what is the next appropriate move. Truly realistic uncertainty can be introduced, free from simplifications required for formulating the analytical PDFs or needed for computational simplicity. Inversion schemes can potentially balance the trade off between excessively wasteful Monte Carlo methods, requiring millions of forward simulations and the extraordinarily stringent budget of linearized methods. Also, the freedom ABC allows may facilitate introducing some components of existing geophysical knowledge into the the inversion scheme where appropriate. This may be gradients of relevant functions for suggesting model updates or sensitivity kernels for the evaluation of fitness. In short, the freedom of ABC may allow us to open our eyes to alternatives which diverge from the current paradigm of joint inversion schemes. \par

The main postulate of this thesis is that ABC can offer improvements over some limiting aspect of traditional likelihood based Bayesian inference. \par

Many of the above possibilities are outside the scope of an 9 month Masters' project. However these will serve as guiding aims and potential for future exploration. My goal here is to simply demonstrate some advantage of a geophysical ABC scheme, which will motivate future investigation.