%------------------------------------------------------
% QUOTE
% If you really feel like adding a quote
% to your page, uncomment out the following.
%------------------------------------------------------
%\begin{savequote}[70mm]
%When theory and experiment agree, 
%that is the time to be especially suspicious. 
%\qauthor{Niels Bohr}
%\end{savequote}
%------------------------------------------------------
% QUOTE
% If you really feel like adding a quote
% to your page, uncomment out the previous.
%------------------------------------------------------

\chapter{Introduction}

\section{Statement of Aims}

"Physical theories allow us to make predictions: given a complete description of a physical system, we can predict the outcome of some experiments. This problem of predicting the result of measurements is called the \textit{modelization problem}, the \textit{simulation problem}, or the \textit{forward problem}. The \textit{inverse problem} consists of using the actual result of some measurements to infer the values of the parameters which characterize that system." \citep{Tarantola2005}\\

This body of work is concerned with the methods of \textit{inverse problems}. Other disciplines may also refer to the inverse problem as parameter inference or parameter estimation. The methods of inverse problems underpin many critical experiments across disciplines. For geophysics it is the method which allows data to be transformed from surface observations into subsurface images. Recently, methods of probabilistic parameter inference have risen to prominence within the geophysics community. With that in mind, this project seeks to explore the geophysical applicability of a recently developed method of probabilistic parameter inference which has emerged from applied challenges in genetics, \underline{A}pproximate \underline{B}ayesian \underline{C}omputation, (ABC). This project was begun on January 2nd, 2018 and concluded with the submission of this document on the 11th of October, 2018.

\section{Parameter inference}

Model parameter estimation based on observable data is a pursuit which transcends scientific disciplines. Here, the unknown parameters, $\bm{\theta} = \{\theta_1,...,\theta_N\}$, for a causative model, $\mathcal{M}$, are sought based on observed data, $\bm{y} = \{y_1,...,y_M\}$. The relationship between model parameters and data may be highly non-linear and is represented through the operator $\bm{g}$:
\begin{equation}
\bm{y} = \bm{g}(\bm{\theta})
\label{basic_data_parameters}
\end{equation}	
In geophysics we are interested in defining the physical properties  of the subsurface (e.g. seismic velocity, density) from experimental data, $\bm{y}$, observed at the surface (e.g. seismic signals, gravitational acceleration). By using physical models, $\mathcal{M}$, which link a parameterized subsurface and resulting simulated data, $\bm{y'}$, the pursuit of imaging the subsurface collapses to a parameter inference problem.\\

Geophysical parameter estimation problems are refereed to as \textit{inverse problems}. They start with the experimental data, $\bm{y}$, and try to infer the causative model parameters, $\bm{\theta}$. As opposed to \textit{forward problems}, which start with a given set of model parameters, $\bm{\theta'}$, and produce a set of simulated data, $\bm{y'}$. Both areas are subject to considerable research and development. The ability to solve the forward problem is required to solve inverse problems.\\
\begin{equation}
\text{The inverse problem:}\ \bm{y} \rightarrow \bm{\theta}
\label{inverse_problem}
\end{equation}
\begin{equation}
\text{The forward problem:}\ \bm{\theta'} \rightarrow \bm{y'}
\label{forward_problem}
\end{equation}

\section{Probabilistic formulation}

Solving the geophysical inverse problem comes with its own set of challenges. Often the experimental data has significant levels of noise, the unknown parameters may far outweigh the constraining data and the uncertainty in the physical model (the forward problem) may itself be large. Under these conditions traditional parameter estimation methods are ineffective.\\

To overcome these issues, probabilistic formulations to geophysical inverse problems have been developed, based on Bayes' theorem, equation \ref{bayes} \citep{Tarantola1982a,Mosegaard1995,Mosegaard2002,Tarantola2005}. Here, the geophysical inverse problem receives a full statistical treatment of uncertainty. All unknown quantities are described by probability distributions. Hence, a probability density function (PDF) is used to define the data, physical model and solution. The Bayesian method is fundamentally about embracing uncertainty and making use of all available information to constrain the solution. Bayes' theorem relies on the notion of conditional probabilities. $p(a|b)$ is the conditional probability of $a$ given event $b$ has occurred. Bayes' theorem is used to define the solution to the inverse problem as the PDF of the model parameters $\bm{\theta}$ conditioned on the observed data $\bm{y}$, $p(\bm{\theta}|\bm{y})$. This PDF, $p(\bm{\theta}|\bm{y})$, is referred to as the \textit{posterior}. For the case of experimental data, $\bm{y}$, and unknown model parameters, $\bm{\theta}$, Bayes' theorem can be applied in the form:
\begin{equation}
p(\bm{\theta}|\bm{y}) = \frac{p(\bm{\theta}) p(\bm{y}|\bm{\theta})}{p(\bm{y})}
\label{bayes}
\end{equation}
Equation \ref{bayes} states that the prior distribution, $p(\bm{\theta})$, multiplied by the likelihood, $p(\bm{y}|\bm{\theta})$, over a normalization constant, $p(\bm{y})$, is equal to the posterior distribution, $p(\bm{\theta}|\bm{y})$. \\

The prior, $p(\bm{\theta})$, is a user-defined probability distribution which incorporates what is already known about the model parameters, $\bm{\theta}$, independent of the experimental data, $\bm{y}$.\\

The likelihood, $p(\bm{y}|\bm{\theta})$, allows the data to dictate the plausibility of different sets of model parameters. Through the likelihood we learn about the support the data has for different sets of model parameters. If for two possible sets of model parameters $\bm{\theta_1}$ and $\bm{\theta_2}$ we have $p(\bm{y}|\bm{\theta_1}) > p(\bm{y}|\bm{\theta_2})$ then $\bm{y}$ is more likely to happen under $\bm{\theta_1}$ than $\bm{\theta_2}$. Hence the term likelihood.\\

The normalization constant $p(\bm{y}) = \int p(\bm{\theta}) p(\bm{y}|\bm{\theta})\ \text{d}\bm{\theta}$ is generally unconsidered. Statistics about the posterior such as maximum posterior value, mean, and standard deviation can still be computed from an unnormalized posterior density. The consequence of leaving $p(\bm{y})$ unevaluated is simply that single point values of the posterior have no meaning. Instead relative values must be evaluated, where ratios cancel the normalization constant. Hence, Bayes' theorem is practically applied in the form:
\begin{equation}
p(\bm{\theta}|\bm{y}) \propto p(\bm{\theta}) p(\bm{y}|\bm{\theta})
\label{applied_bayes}	
\end{equation}
The posterior distribution, $p(\bm{\theta}|\bm{y})$, is the solution to a probabilistic formulation of an inverse problem. It is the complete state of knowledge about $\bm{\theta}$ given $\bm{y}$. Complete as it incorporates both what is already known, $p(\bm{\theta})$, as well as new experimental data, $\bm{y}$, through the likelihood. The probabilistic solution offers a philosophical shift compared to traditional solutions. A probability distribution over all model parameters is recovered, instead of a discrete set of model parameters, offering in-built quantification of uncertainty. \\

For non-trivial problems of scientific interest there is no analytical solution for the posterior. This necessitates stochastic sampling to define the posterior $p(\bm{\theta}|\bm{y})$ (e.g. rejection sampling, Monte Carlo sampling, Markov chain Monte Carlo (MCMC) sampling). In high-dimensional search spaces, many unknown model parameters, highly efficient sampling methods are needed if the curse of dimensionality is to be overcome.\\

The probabilistic solution to inverse problems outlined above has been successfully applied to tomography. Probabilistic multiobservable tomography has also been developed  and applied \citep{khan2007joint,afonso2013a,afonso2013b,afonso2016}. Multiobservable methods take into account more constraining data and hence minimize the range of acceptable models consistent with the data. Different observables and their associated physical models are sensitive to different parts of the system under study and can be brought in on the basis that they better constrain $\bm{\theta}$. This is important due to the inherent non-uniqueness and large uncertainties of some traditional tomography methods. The result has been more informed inference and a better understanding of the uncertainty in conclusions drawn.\\

\section{The likelihood}
The likelihood is central to both Bayesian and frequentist statistical inference. Both specification of a prior distribution and likelihood are at the heart of Bayesian inference, the probabilistic framework adopted here. In practice, due attention should be paid to both likelihood and prior. However, the focus of this thesis surrounds the likelihood. As such considerable weight is given to this topic here. \\

The likelihood is used after $\bm{y}$ is available to describe the plausibility of $\bm{\theta}$. It is a function of the parameters $\bm{\theta}$ to the model $\mathcal{M}$, given observed data $\bm{y}$. Hence it is denoted $\mathcal{L}(\bm{\theta}|\bm{y})$. However, the plausability of $\bm{\theta}$ given $\bm{y}$ is proportional to the PDF of $\bm{y}$ given $\bm{\theta}$. This gives us the paradoxical statement:
\begin{equation}
\mathcal{L}(\bm{\theta}|\bm{y}) \propto \pi(\bm{y}|\bm{\theta})
\label{likelihood_def}
\end{equation}
However this is resolved by considering $\pi(\bm{y}|\bm{\theta})$ at the observed $\bm{y}$, hence fixed, for varying $\bm{\theta}$. This maps out a distribution of relative plausibility over $\bm{\theta}$. The likelihood is not necessarily a PDF and hence point values contain no inherent information. Only through comparison of relative values does the likelihood gain meaning.\\

\subsection{General likelihood}
Several authors \citet{gregory2005bayesian} have arrived at a general equation for $\mathcal{L}(\bm{\theta}|\bm{y})$ by considering:
\begin{equation}
y_i = z_i + e_i
\label{general_form_inference}
\end{equation}
$e_i$ is the uncertainty in the data, and $z_i$ is forward relation:
\begin{equation}
z_i = \bm{g}(\bm{\theta})
\end{equation}
Both $z_i$ and $e_i$ are represented by distributions:
\begin{equation}
\pi(z_i|\bm{\theta}) = f_Z(z_i)
\label{fz_dist}
\end{equation}
\begin{equation}
\pi(e_i|\bm{\theta}) = f_E(e_i)
\label{fe_dist}
\end{equation}
We are critically interested in arriving in an equation for $\pi(y_i|\bm{\theta})$, which will define $\mathcal{L}(\bm{\theta}|\bm{y})$. However, as the relationship in equation \ref{general_form_inference} specifies, $y_i$ depends on both $z_i$ and $e_i$. Hence, to arrive at $\pi(y_i|\bm{\theta})$ we must consider the distributions of equations \ref{fz_dist} \& \ref{fe_dist}. If we first consider the joint distribution $\pi(y_i,z_i,e_i|\bm{\theta})$ then our likelihood can be found by integrating out, 'marginalizing', $\pi(z_i|\bm{\theta})$ and $\pi(e_i|\bm{\theta})$ to leave a distribution for $\pi(y_i|\bm{\theta})$:
\begin{equation}
\pi(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ \pi(y_i,z_i,e_i|\bm{\theta})
\label{most_general_L}
\end{equation}
The definition of conditional probability allows equation \ref{most_general_L} to be rewritten as:
\begin{equation}
\pi(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ \pi(z_i,e_i|\bm{\theta})\ \pi(y_i|z_i,e_i,\bm{\theta})
\end{equation}
If we assume $z_i$ and $e_i$ are independent:
\begin{equation}
\pi(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ \pi(z_i|\bm{\theta})\ \pi(e_i|\bm{\theta})\ \pi(y_i|z_i,e_i,\bm{\theta})
\label{halfway_through_derivation}
\end{equation}
From the relationship in equation \ref{general_form_inference}, $y_i-z_i-e_i = 0$. This relation considered, $\pi(y_i|z_i,e_i,\bm{\theta})$ can be reasonably represented as a dirac-delta function such that:
\begin{equation}
\pi(y_i|z_i,e_i,\bm{\theta}) = \delta(y_i-z_i-e_i)
\label{dirac_equality}
\end{equation}
This is a natural definition, as the probability density of $y_i$ given $z_i$ and $e_i$ should be focused when the exact relation of equation \ref{general_form_inference} is met. The dirac-delta function serves this role perfectly while still exhibiting properties of a PDF, non-negative and $\int_{-\infty}^{\infty}\delta = 1$. Considering equation \ref{dirac_equality}, equation \ref{halfway_through_derivation} becomes:
\begin{equation}
\pi(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ \pi(z_i|\bm{\theta})\ \pi(e_i|\bm{\theta})\ \delta(y_i-z_i-e_i)
\end{equation}
Adopting the form from equations \ref{fz_dist} \& \ref{fe_dist}:
\begin{equation}
\pi(y_i|\bm{\theta}) = \int \text{d}z_i\ f_Z(z_i) \int \text{d}e_i\ f_E(e_i) \delta(y_i-z_i-e_i)
\label{almost_general_form}
\end{equation}
The dirac-delta function in the second integral of equation \ref{almost_general_form} serves to isolate the value/values of $f_E(e_i)$ when $e_i = y_i - z_i$. Hence, it is equivalent to consider:
\begin{equation}
\int \text{d}e_i\ f_E(e_i) \delta(y_i-z_i-e_i) = f_E(y_i - z_i)
\end{equation}
This leaves a general form for the likelihood $\pi(y_i|\bm{\theta})$ as: 
\begin{equation}
\pi(y_i|\bm{\theta}) = \mathcal{L}(\bm{\theta}|y_i) = \int \text{d}z_i\ f_Z(z_i)\ f_E(y_i - z_i)
\label{general_likelihood}
\end{equation}
However this general equation is not in a form which can be practically applied to inference problems. To achieve a practical form further assumptions must be made about the distributions $f_Z(z_i)$ and $f_E(e)$. This is done by assigning parametric distributions. As we will see in the next section, assigning Gaussian distributions will allow an analytical solution to equation \ref{general_likelihood} as our general likelihood definition takes the form of a convolution. 

\subsection{Practical likelihood form}
It is assumed that both $y_i$ and $z_i$ contain statistical uncertainty, denoted $e_i$ and $\epsilon_i$ respectively. As a result: 
\begin{equation}
y_i = z_i + e_i
\end{equation}
\begin{equation}
z_i = \bm{g}(\bm{\theta}) + \epsilon_i
\end{equation}	
\begin{equation}
\therefore	y_i = \bm{g}(\bm{\theta}) + \epsilon_i + e_i
\end{equation}
$\epsilon_i$ and $e_i$ are assumed to be uncorrelated. If it is assumed that $\epsilon_i$ is described as independent and identically distributed (i.i.d) from a Gaussian with a predetermined standard deviation (s.t.d), $\sigma_{\epsilon}$:
\begin{equation}
\pi(z_i|\bm{\theta}) = f_Z(z_i) = \frac{1}{\sqrt{2\pi\sigma_{\epsilon}^2}}\ \text{exp}\bigg[\frac{-\epsilon_i^2}{2\sigma_{\epsilon}^2} \bigg]
\end{equation} 
Likewise, if $e_i$ is assumed to be described as i.i.d from a Gaussian with predefined s.t.d $\sigma_{e}$:
\begin{equation}
\pi(e_i|\bm{\theta}) = f_E(e_i) = f_E(y_i-z_i) = \frac{1}{\sqrt{2\pi\sigma_{e}^2}}\ \text{exp}\bigg[\frac{-e^2}{2\sigma_{e}^2} \bigg]
\end{equation}
As a result of defining parametric distributions for $f_Z(z_i)$ and $f_E(y_i-z_i)$ the general definition of $\mathcal{L}(\bm{\theta}|y_i)$, equation \ref{general_likelihood}, can be evaluated. This is the convolution of the two distributions with the closed-form expression:
\begin{equation}
\mathcal{L}(\theta|y_i) = \frac{1}{\sqrt{2\pi}\sqrt{\sigma_{\epsilon}^2+\sigma_{e}^2}} \text{exp}\bigg[\frac{-(y_i-\bm{g}(\bm{\theta}))^2}{2(\sigma_{\epsilon}^2+\sigma_{e}^2)}\bigg]
\end{equation}

If you have a set of data $\bm{y} = \{y_1,...,y_M\}$, where each $y_i$ term is independent then, considering uncertainty in the data and model as i.i.d Gaussian:
\begin{equation}
\begin{split}
\pi(\bm{y}|\bm{\theta}) &= \pi(y_1,...,y_m|\bm{\theta})\\
&= (2\pi)^{M/2}(\prod_{i = 1}^{M}(\sigma_{e}^2+\sigma_{\epsilon}^2)^{1/2})\ \text{exp}\bigg[\sum_{i = 1}^{M}\frac{-(y_i-\bm{g}({\bm{\theta}}))^2}{2(\sigma_{e}^2+\sigma_{\epsilon}^2)}\bigg]\\
\label{independant_data_likelihood}
\end{split}
\end{equation}
Least-squares, the sum of squares of residuals, is a standard technique for regression of over-determined problems, more data than model parameters. For a set of data $\bm{y}$ and some specific model parameters $\bm{\theta'}$
\begin{equation}
\text{least-squares} = LS = \sum_{i = 1}^{M} (y_i - \bm{g}(\bm{\theta'}))^2
\end{equation}
The relationship between least-squares and a likelihood of the form of equation \ref{independant_data_likelihood} can be seen as:
\begin{equation}
\text{exp}\bigg[\sum_{i = 1}^{M}\frac{-(y_i-\bm{g}({\bm{\theta'}}))^2}{2(\sigma_{e}^2+\sigma_{\epsilon}^2)}\bigg] = \text{exp}\bigg[\frac{-LS}{2(\sigma_{e}^2+\sigma_{\epsilon}^2)}\bigg]
\end{equation}\\

For the case where vectors of data and model parameters are considered together as represented by a multivariate Gaussian distribution, defined by covariance matrices $C_d$ and $C_m$ respectively, such that:
\begin{equation}
f_Z(z) \propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}-\bm{g}(\bm{\theta}))^TC_m^{-1}(\bm{y}-\bm{g}(\bm{\theta}))\bigg]
\end{equation}
\begin{equation}
f_E(e) \propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}-\bm{g}(\bm{\theta}))^TC_d^{-1}(\bm{y}-\bm{g}(\bm{\theta}))\bigg]
\end{equation}
Then the likelihood involves a combination of their covariance matrices:
\begin{equation}
\mathcal{L}(\bm{\theta}|\bm{y}) \propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}-\bm{g}(\bm{\theta}))^T(C_d+C_m)^{-1}(\bm{y}-\bm{g}(\bm{\theta}))\bigg]
\end{equation}
Finally if it is assumed that there is no correlations between the errors in the model and within the data then the respective covariance matrices can be represented by identity matrices and the likelihood collapses to:
\begin{equation}
\mathcal{L}(\bm{\theta}|\bm{y}) \propto \text{exp}\bigg[-\frac{1}{2}LS\bigg]
\end{equation}

\subsection{Critique of likelihood assumptions}



\section{Approximate Bayesian Computation}

Likelihood-free methods for Bayesian inference have been developed in response to parameter estimation problems of scientific interest where it is not possible to formulate or justify an explicit likelihood function. Instead, likelihood-free methods target the same posterior distribution without evaluating a likelihood function. The algorithms and methods of likelihood-free Bayesian inference have been grouped under the umbrella term \textit{approximate Bayesian computation} (ABC). Generally, ABC simply requires the ability to simulate data given model parameters. Problems ripe for attack by ABC are common in science due to the breadth of models which have been developed to describe natural systems. This leverages the frequent case where it is possible to rapidly simulate data, but the explicit formulas for associated uncertainty are difficult to formulate, expensive to evaluate, impossible to justify, or do not exist. Hence, traditional likelihood machinery becomes infeasible. ABC is a means to overcome these issues, it is backed by a sound theoretical underpinning and is subject to a rapidly expanding set of literature. \\

The premise of ABC begins with the notion that a set of model parameters, $\bm{\theta}$, is a sample from the posterior if the observed data, $\bm{y}$, and a simulated dataset are equal, $\bm{y^*}$. Algorithm \ref{basicalg} generates i.i.d samples from the posterior $p(\bm{\theta}|\bm{y})$.

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Generate $\bm{\theta^*}$ as a random sample from $p(\bm{\theta})$		
		\State 2. Simulate $\bm{y^*}$ from the forward operator $\bm{g_s}(\bm{\theta^*})$		
		\State 3. Accept $\bm{\theta^*}$ as a posterior sample if $\bm{y^*} = \bm{y}$		
		\State 4. Repeat
	\end{algorithmic}
	\label{basicalg}
\end{algorithm}

The operator $\bm{g_s}$ in Algorithm \ref{basicalg}, and all of likelihood-free inference, differs from the forward operators we are used to thinking about in geophysics. Traditionally, forward operators in geophysics are \textit{deterministic}. That is, for a given set of model parameters, $\bm{\theta}$, the output of the forward operator $\bm{g}(\bm{\theta})$ will always be the same. The uncertainty in both data and model, are then later built into the construction of a likelihood function. The ABC forward operator which simulates data, $\bm{g_s}$, is \textit{stochastic}. That is, the uncertainty from both data and modelization is built into the simulation process. As a result of this modification, algorithm \ref{basicalg} does not require the formulation or evaluation of a likelihood function. The ABC forward operator $\bm{g_s}$ allows the practitioner to build in whatever uncertainty is justified for the scientific problem at hand. There is no necessity to make assumptions about Gaussian distributed and i.i.d uncertainty. \\

Algorithm \ref{basicalg} is acceptable for basic problems where the datasets $\bm{y}$ and $\bm{y^*}$ are discrete. However, for large and continuous datasets the probability of generating a sample where the acceptance criteria, $\bm{y^*} = \bm{y}$, is met diminishes to levels unacceptable for parameter inference. ABC relaxes the problematic requirement of equality by accepting samples when the distance between $\bm{y^*}$ and $\bm{y}$, $\rho(\bm{y},\bm{y^*})$, is less than a tolerance value $\epsilon$. This method, algorithm \ref{ABCfulldatatolerance}, does not target our true posterior of interest, but instead, the ABC posterior $p(\bm{\theta}|\rho(\bm{y},\bm{y^*})\leq\epsilon)$ which approximates the true posterior. 

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Generate $\bm{\theta^*}$ as a random sample from $p(\bm{\theta})$		
		\State 2. Simulate $\bm{y}$ from the forward operator $\bm{g_s}(\bm{\theta^*})$		
		\State 3. Accept $\bm{\theta^*}$ as a posterior sample if $\rho(\bm{y},\bm{y^*})\leq\epsilon$		
		\State 4. Repeat
	\end{algorithmic}
	\label{ABCfulldatatolerance}
\end{algorithm}

Algorithm \ref{ABCfulldatatolerance} requires a user specified metric, $\rho$, which defines the distance between datasets. Common choices are the absolute-value norm, Euclidean distance and Mahalanobis distance. Likewise, the value for the tolerance $\epsilon$ must be user specified. As $\epsilon \rightarrow \infty$, the sampled target distribution of algorithm \ref{ABCfulldatatolerance} $p(\bm{\theta}|\rho(\bm{y},\bm{y^*})\leq\epsilon) \rightarrow p(\bm{\theta})$.  Algorithm \ref{ABCfulldatatolerance} simply recovers the prior distribution. However, as $\epsilon \rightarrow 0$ the sampled distribution $p(\bm{\theta}|\rho(\bm{y},\bm{y^*})\leq\epsilon) \rightarrow p(\bm{\theta}|\bm{y})$. The exact posterior is recovered. Encoded in the value of $\epsilon$ is a trade-off between acceptance rate and accuracy. As $\epsilon$ increases so does the efficiency of the sampler, but the accuracy of the recovered distribution is increasingly eroded. Conversely, as $\epsilon$ decreases the accuracy of the recovered distribution converges to the true posterior, but the acceptance rate approaches computationally infeasible levels. This trade-off for computational efficiency at the cost of accuracy is where ABC derives the name \textit{approximate} Bayesian computation, as the samplers recover a distribution which approximates the true posterior.\\

% Write about summary statistics
Rejection sampler inference in the form of algorithm \ref{ABCfulldatatolerance} can run into efficiency problems as the dimensionality (size) of the datasets grow. In this case the probability of sampling $\rho(\bm{y},\bm{y^*})\leq\epsilon$ diminishes as the size of the dataset grows. Since first application, \citet{Tavare1997}, likelihood-free methods have adopted the use of low-dimensional summary statistics about the data in the evaluation of distance, $\rho$. A metric over a vector of summary statistics is evaluated, $\rho(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))$. This leads to the most common form of an ABC rejection sampling scheme of algorithm \ref{ABCrejectionsampler}. This rejection sampler generates i.i.d samples from the distribution $p(\bm{\theta}|\rho(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))\leq\epsilon)$.

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Generate $\bm{\theta^*}$ as a random sample from $p(\bm{\theta})$		
		\State 2. Simulate $\bm{y}$ from the forward operator $\bm{g_s}(\bm{\theta^*})$		
		\State 3. Compute summary statistics $\bm{S}(\bm{y})$ and $\bm{S}(\bm{y^*})$		
		\State 4. Accept $\bm{\theta^*}$ as a posterior sample if $\rho(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))\leq\epsilon$		
		\State 5. Repeat
	\end{algorithmic}
	\label{ABCrejectionsampler}
\end{algorithm}

Given the size and complexity of some modern datasets, it can be impractical to leverage the full dataset for inference. For example, in first application, \citet{Tavare1997} replace the full sequences of DNA with the number of sites which differ between DNA samples. The idea being that as long as the statistics used are \textit{sufficient} there is no information loss for parameter inference. As a result of sufficiency the posterior computed with statistics will be equivalent to the the posterior computed by the full dataset, i.e $p(\bm{\theta}|\bm{S}(\bm{y})) = p(\bm{\theta}|\bm{y})$. In reality no truly sufficient statistics exist for problems of scientific interest. Instead practitioners settle for a reasonably sufficient low-dimensional set of summary statistics. This lack of sufficiency introduces a second bias, tolerance being the first, into the ABC-posterior $p(\bm{\theta}|\rho(\bm{S}(\bm{y}),\bm{S}(\bm{y^*}))\leq\epsilon)$ due to the information lost by summarizing the full data set with statistics. It is, however, thanks to summary statistics that likelihood-free inference owes it's origin and can proceed. Summary statistics have opened parameter inference to problems as challenging as noisy near-chaotic ecology populations \citep{Wood2010} and have been shown to offer superior power to diagnose model insufficiency \citep{Ratmann2009,vrugt2013toward}.\\

% talk about MCMC-ABC
Canonical interpretations of ABC have considered algorithms and equations in the form which have been introduced so far. However, ABC can be cast in another mathematical light which will enable straight forward implementation into advanced sampling routines such as MCMC. Here, the stochastic forward simulations from ABC, $\bm{y^*}$, are viewed as an auxiliary parameter which is introduced into the posterior to facilitate computation \citep{Sisson2010a}. This process changes the computed posterior from the traditional $p(\bm{\theta}|\bm{y}) \propto p(\bm{y}|\bm{\theta})p(\bm{\theta})$ to the ABC posterior:

\begin{equation}
p_{ABC}(\bm{\theta},\bm{y^*}|\bm{y}) \propto p(\bm{y}|\bm{y^*},\bm{\theta}) p(\bm{y^*}|\bm{\theta}) p(\bm{\theta})
\label{eqABCposterior}
\end{equation}

$\bm{y^*}$ is viewed as a realization from the density $p(\bm{y^*}|\bm{\theta})$. $p(\bm{y}|\bm{y^*},\bm{\theta})$ is introduced to serve the same role as the tolerance in Algorithms \ref{ABCfulldatatolerance} and \ref{ABCrejectionsampler}. $p(\bm{y}|\bm{y^*},\bm{\theta})$ is chosen to weight the posterior with high values when the observed and simulated datasets are close. However, the form of equation \ref{eqABCposterior} allows the mathematics of kernel densities to be introduced into $p(\bm{y}|\bm{y^*},\bm{\theta})$ such that:
\begin{equation}
p(\bm{y}|\bm{y^*},\bm{\theta}) = \frac{1}{\epsilon} K_h (\frac{\rho(\bm{y},\bm{y^*})}{\epsilon})
\end{equation}
Where $K$ is some standard kernel, and the tolerance $\epsilon$ serves as the kernel bandwidth . \\

Thinking of equation \ref{eqABCposterior} in algorithm form still allows a straightforward understanding 1. a set of parameters is simulated from $ p(\bm{\theta})$ 2. a dataset is stochastically simulated representing a sample from $p(\bm{y^*}|\bm{\theta})$ 3. The distance between datasets if evaluated by the function $p(\bm{y}|\bm{y^*},\bm{\theta})$. \\

While equation \ref{eqABCposterior} targets the joint posterior density of simulations and model parameters, marginalization to our posterior of interest, $p_{abc}(\bm{\theta}|\bm{y})$, is done numerically by discarding the simulations recovering:
\begin{equation}
p_{ABC}(\bm{\theta}|\bm{y}) \propto p(\bm{\theta}) \int_{\bm{y^*}} p(\bm{y}|\bm{y^*},\bm{\theta}) p(\bm{y^*}|\bm{\theta})\ \text{d}\bm{y^*}
\label{eqABCtargetPosterior}
\end{equation}\\

Equation \ref{eqABCtargetPosterior} can be adjusted to rely on summary statistics. The density $p(\bm{S}(\bm{y^*})|\bm{\theta})$ is introduced as the density implied from taking summary statistics about $p(\bm{y^*}|\bm{\theta})$ and $p(\bm{S}(\bm{y})|\bm{S}(\bm{y^*}),\bm{\theta})$ is a kernel over the distance between summary statistics. Our approximate target distribution becomes:
\begin{equation}
p_{ABC}(\bm{\theta}|\bm{S}(\bm{y})) \propto p(\bm{\theta}) \int_{\bm{S}(\bm{y^*})} p(\bm{S}(\bm{y})|\bm{S}(\bm{y^*}),\bm{\theta})\  p(\bm{S}(\bm{y^*})|\bm{\theta})\ \text{d}\bm{S}(\bm{y^*})
\end{equation}

\citet{Marjoram2003} first demonstrated a Markov-chain Monte Carlo (MCMC) scheme to sample the ABC posterior distribution. A simple MCMC algorithm with a Metropolis-Hastings (M-H) acceptance probability is demonstrated in algorithm \ref{ABC-MCMC}. The M-H acceptance probability $\alpha$ to recover the ABC posterior is:
\begin{equation}
\alpha_{ABC} = \frac{p(\bm{S}(\bm{y})|\bm{S}(\bm{y^*}),\bm{\theta})\ p(\bm{\theta^*})\ q(\bm{\theta^*},\bm{\theta^{i-1}})} {p(\bm{S}(\bm{y})|\bm{S}(\bm{y^{i-1}}),\bm{\theta})\ p(\bm{\theta^{i-1}})\ q(\bm{\theta^{i-1}},\bm{\theta^*})}
\end{equation}
Where $\bm{i}$ denotes the time step in the chain, and $\bm{\theta^*}$ is used to represent a candidate move from the proposal distribution $q(\bm{\theta^{i-1}}, . )$.

\begin{algorithm}[H]
	\caption{ }
	\begin{algorithmic}
		\State 1. Start from an initial state $\bm{\theta^0}$ and select a proposal distribution $q$
		\State 2. At each step where the current state is $\bm{\theta^{i-1}}$, propose a candidate 	move $\bm{\theta^*}$ from the distribution $q(\bm{\theta^{i-1}},.)$		
		\State 3. If the candidate state is better than the previous state, i.e $\alpha_{ABC} > 1$, then the candidate state is accepted unconditionally meaning $\bm{\theta^i} = \bm{\theta^*}$
		\State 4. If the candidate move is not better in the above sense, then $\bm{\theta^*}$ is accepted with probability equal to $\alpha_{ABC}$		
		\State 5. If the candidate move is not accepted, then the chain remains in it's current state, meaning $\bm{\theta^{i}} = \bm{\theta^{i-1}}$		
		\State 6. Repeat the simulation steps 2-5 until enough values have been generated
	\end{algorithmic}
	\label{ABC-MCMC}
\end{algorithm}