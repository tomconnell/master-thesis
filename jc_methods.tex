\chapter{Synthetic geophysical experiments}
\label{SGE}
This chapter will outline a series of experiments which will test the main postulate of this thesis, that ABC can offer some improvement over traditional likelihood based Bayesian inference for geophysics. There are many potential avenues to pursue `improvement'. For example, ABC opens parameter inference to models where there is no closed form expression for the likelihood. This problem can stem from joint probabilistic formulations of multiple datasets with dependent, non-Gaussian uncertainties. In this work, however, I focus on other advantageous aspects of ABC, namely its diagnostic potential. Diagnostic parameter inference, which makes use of the information about the model contained in the simulated data, has been developed and applied \citep{Ratmann2009,vrugt2013toward}. Here diagnostic parameter inference for geophysics is explored with the goal of improving the convergence rate of an inversion. The benchmark for this test will be a MCMC algorithm targeting a posterior distribution with an analytically-defined likelihood. Currently, there are many geophysical parameter inference problems where a probabilistic approach is required due to a trade-off between parameters, a non-unique parameter space or the availability of prior information. One example is an inversion for the thermal-chemical state of the lithosphere based on multiple different sets of constraining data \citep{afonso2013a,afonso2013b}. Currently, exploring this 3D structure with high resolution is limited as the problem must be kept within the computational limits of the posterior sampling algorithm. If high resolution, many unknown parameters, and large scale (e.g. the whole mantle) inversions are to be implemented on problems which require a probabilistic formulation, then methods are required which will efficiently find and define the set of low misfit models. Here a diagnostic ABC inversion may be able to improve the search for high probability (low misfit) models by taking into account more of the information available within the simulated dataset. \par

As with the previous chapter, the code to produce all figures in this section can be found at \url{https://github.com/tomconnell/approximate-bayesian-tomography}.\par


\section{Crustal density inversion}

As a first experiment, I consider an inversion for crustal density ($\rho$) with a vertical gravity anomaly dataset ($\Delta g$) for a 2D discretized subsurface \citep[p.184-195,378]{blakely1996}. The dimensionality of the parameter space is kept modest, an 8x4 grid, with an observed data point above each column. The grid is defined over a 160 $km$ by 40 $km$ area. Density is bounded between 2-3.5 $g/cm^3$, the limits for which \citet{Brocher2005} define an empirical relationship between density and compressional-wave velocity ($V_p$). The `true model', Figure \ref{true_model}, which will be the target of our inversion scheme, is kept smooth to allow a prior term, $p(\bm{\theta})$, to be set for smoothness which will limit the inversion to a unique solution. The prior term is:
\begin{equation}
-\text{log}\big(p(\bm{\theta})\big) = \sum_{i = 1}^{N} \Big(\sum_{j} (\rho_i - \rho_j)^2\Big)
\label{smoothness}
\end{equation}
where $j$ describes all blocks in immediate contact with the given block, $i$. The edge effect for the 2D subsurface grid is compensated by adding the vertical gravity anomaly which will result from extending the grid by a width of one on both sides, tripling the total domain width, with a density which is the average of the parameter space, 2.75 $g/cm^3$. The model is assumed to contain no modelization uncertainty and the data is known to be contaminated with noise defined by $\sigma^{\mathcal{D}} = \mathcal{N}(0,\sqrt{2})$.
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{true_model.pdf}
	\caption{The `observed data', vertical component of gravity ($\Delta g$), and `true model', a 2D density ($g/cm^3$) slice, which will be the target of our synthetic geophysical experiments to compare ABC to likelihood based Bayesian inference. A smoothness value, -log($p(\bm{\theta})$), equation \ref{smoothness}, for the true model is plotted for reference to later solutions.}
	\label{true_model}
\end{figure}
The ABC approach will be benchmarked against a MCMC algorithm targeting an analytically defined posterior. Given there is no modelization uncertainty and $\sigma^{\mathcal{D}}$ is known, the negative log-likelihood can be defined by, equation \ref{likelihood-1}:
\begin{equation}
	-l(\bm{\theta^*}|\bm{y}) = \frac{\sum_{i = 1}^{M}(y_i-y^*_i)^2}{(\sigma^{\mathcal{D}})^2}
	\label{analytical-applied-likelihood}
\end{equation}
For this example, only MCMC is considered (i.e. no AM, DR or DRAM). This maintains a consistent sampling benchmark for comparison to the ABC scheme. The proposal distribution for both MCMC sampling of the analytical distribution and ABC-MCMC is held constant for all runs as $q(\cdot,\cdot) = \mathcal{N}(0,I\cdot250)$. The parameter space is subdivided into eight 2x2 blocks. At each time step a random block is selected in the subsurface and updated via $q(\cdot,\cdot)$. As demonstrated in table \ref{sampling-method-comparison}, the number of parameters updated at each time step impacts the acceptance rate and the rate of space exploration of the chain. To keep inference comparable, both the analytical scheme and ABC scheme will update 4 parameters per time step via the proposal distribution. Likewise, for each experiment considered in this section the Markov chain starting position is the same for both the analytical MCMC and ABC-MCMC. The starting position for both is set by sampling a uniform distribution, $\mathcal{U}(2,3.5)$, to define each unknown parameter. \par

A set of summary statistics to describe the data in Figure \ref{true_model} is needed to proceed with ABC. A comprehensive description of the data is defined by the coefficients to a linear model, $m$ and $b$, the sample mean, $\bar{\mu_{\Delta g}}$, sample standard deviation, $\bar{\sigma_{\Delta g}}$, and a fail-safe residual term, $\sum R$, for the difference between a simulation and the observed data for a given $\bm{\theta^*}$, $R = |\bm{y}-\bm{y^*}|$. Initial testing showed the set of statistics $\begin{bmatrix}
\bar{\mu_{\Delta g}}\ \bar{\sigma_{\Delta g}}\ m\ b\ \sum R
\end{bmatrix}^T$ comprehensively describes $\bm{y}$. As with section \ref{banana-section}, it is convenient to normalize the spread of each marginal distance term, $\text{d}_i(S_i(\bm{y}),S_i(\bm{y^*}))$, equation \ref{marginal_distance}, to one. This accounts for the different scales and sensitivities of the summary statistics to the unknown parameters. The normalization term $\bm{\sigma_S}$ is approximated from 10,000 Monte Carlo simulations over parameter range 2-3.5 $g/cm^3$. The tolerance is closely related to the acceptance rate of the algorithm. Given the tolerance is tightly tied to the acceptance rate, the tolerance is tuned to balance accuracy of the posterior and acceptance rate.\par

\citet{Sisson2010a} highlight that acceptance rate and mixing can be improved by increasing the number of simulated datasets, $\bm{y^*}$, for each set of model parameters, $\bm{\theta^*}$. This technique is adopted in applications with large uncertainty in $\bm{g_s}$ \citep{Ratmann2009,Wood2010}. It has a stabilizing impact when the uncertainty in each simulation is large. The set of simulated data make up a Monte Carlo approximation to the true value of the ABC likelihood approximation ($p(\bm{y}|\bm{y^*},\bm{\theta})$) which would be found as the number of simulated datasets tends to $\infty$. The estimate of the ABC likelihood approximation for a given set of parameters becomes less noisy as the number of simulated datasets increases. This technique is used for $k = 100$ simulations from $\bm{g_s}$ in the ABC gravity inversion. Since the uncertainty associated with the measurement process is additive and fast to simulate, there is little computational cost in increasing the number of simulated datasets, $\bm{y^*} \sim \bm{g}_s = \bm{g}(\bm{\theta}) + e^{\mathcal{D}}$, as the deterministic physical model, $\bm{g}(\bm{\theta})$, does not need to be recomputed for each new realization. The distance is then computed based the sample mean of $\bm{y^*_{1:k}}$, $\bm{d} = |\bm{S}(\bm{y})-\bm{S}(\bm{\bar{\mu_{y^*}}})|$. \par

The diagnostic ABC scheme is free to open the likelihood, consider the information available, and use that to make an informed next step in the Markov chain. The ABC scheme moves away from randomly selecting 2x2 blocks in the subsurface to update, as is done in the analytical MCMC scheme. Instead, the update is selected to ensure that it is occurring in a location where the current fit of the simulated data to the observed data is poor. A discrete PDF is defined over each data point which is proportional to the misfit:
\begin{equation}
	p(\text{update}) \propto \text{misfit} = (\bm{y}-\bm{y^*})^2
	\label{misfit}
\end{equation}
A random sample from this PDF is then drawn at each time step to determine where the parameter updates will occur. Once a data point is selected the 4 parameters directly below are updated. The effectiveness of dynamically selecting and localizing the updates relies on our physical intuition about the relationship between the unknown model parameters and the resulting data. In this case, each simulated data point is most sensitive to the parameters which are directly below it, and by focusing model updates on regions which are poorly fitting, the most is made from each move during the initial phase of the algorithm where local transitions are made in search of an area of high posterior density. This change has ideas similar to the way gradient-based linear optimization methods work, however, here I simply rely on physical intuition within a Bayesian framework. \par

As a next step in defining a diagnostic ABC inversion, the proposal distribution is directed to bias the parameter updates in the direction which the simulated data needs to move to lower the misfit. For example, if a data point is selected based on sampling the discrete PDF equation \ref{misfit} and it is found the simulated data point is too large, i.e. the value $y^*_{\Delta g} - y_{\Delta g}$ is positive, then the proposal for the column of parameters at this update step will be biased towards a density lower than the current parameter value. In this application a truncated Gaussian distribution, figure \ref{updates}, is used to bias the proposal updates. For the example where the currently selected data point is too large, the update can be biased toward a lower density by sampling a truncated Gaussian distribution, centered at the current chain location, and truncated at some position greater than the current chain location. A truncated Gaussian is convenient to implement as detailed balance is satisfied without computing the ratio $\frac{q(\bm{\theta^*},\bm{\theta_{t-1}})}{q(\bm{\theta_{t-1}},\bm{\theta^*})}$ at each time step in the Markov chain. \par

The analytical definition for the truncated normal distribution, $f(x)$, used to direct the parameter updates is:
\begin{equation}
f(x) = \begin{cases} 
0 & x < a \\
\frac{1}{\sqrt{2\pi\sigma^2}}\text{exp}\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big) & a\leq x\leq b \\
0 & x > b 
\end{cases}
\label{trunc-eq}
\end{equation}
Here $a$ is the lower cut-off and $b$ is the upper cut-off. In this application the negatively truncated distribution has $a = -\frac{1}{2}\sigma$ and $b = \infty$. While the positively truncated distribution has $a = -\infty$ and $b = \frac{1}{2}\sigma$. For our application $\mu$ is the current chain location and $\sigma$ = 250. \par

\begin{figure}[H]
	\centering
	\includegraphics[scale=1.1]{updates.pdf}
	\caption{The proposal distributions which are used to either positively or negatively bias the parameter update. The negatively truncated Gaussian distribution (a) is used when the parameter values are deemed too low. The positively truncated Gaussian distribution (b) is used when the parameter values are deemed too high. Equation \ref{trunc-eq} defines the truncated Gaussian distribution. During application the distributions are centered on the current chain location and $\sigma = 250$. The cut-off $a$ and $b$ are selected to set the bias at $\sim$70\% to $\sim$30\%.} 
	\label{updates}
\end{figure}


Figure \ref{comparison-1} plots a comparison between the misfit, equation \ref{misfit}, for the analytical scheme described above and the diagnostic ABC scheme during the initial phase (first 5,000 time steps) of their respective Markov chains. Exploiting the diagnostic properties of ABC by dynamically selecting and directing the parameter updates increases the rate at which the ABC algorithm moves to a low misfit model in comparison to the likelihood based MCMC scheme. The integrated auto-correlation time ($\tau$), figure \ref{comparison-1}, estimates the asymptotic variance of Markov chain based estimates relative to a sampler which draws i.i.d samples of the posterior. This result can be interpreted as the diagnostic ABC algorithm drawing more i.i.d samples from the ABC posterior relative to the MCMC sampling of the analytically-defined posterior. Figure \ref{ana-im-1} and \ref{abc-im-1} plot the solution obtained, mean marginal model, for each full Markov chain (100,000 time steps). \par

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{comparison.pdf}
	\caption{The misfit, l2-norm, equation \ref{misfit}, for the chain state during the initial phase (first 5,000 time steps) for an analytically defined posterior compared to diagnostic ABC. The `true model' and observed data is plotted in figure \ref{true_model}. Both posteriors, analytical and ABC, use a parameter space bound between 2-3.5 $g/cm^3$ and a prior smoothness defined by equation \ref{smoothness}. The speed increase in finding low misfit models is a result of opening the likelihood and using the information contained in each iteration to select and direct the next update. The details of the full chain run, 100,000 time steps, are also displayed on the figure. $\tau$ is the integrated auto-correlation time.}
	\label{comparison-1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{ana_im.pdf}
	\caption{The mean of the marginal posterior, $p(\bm{\theta}|\bm{y})$, defined by the prior equation \ref{smoothness} and likelihood \ref{analytical-applied-likelihood} targeting the `true model' and observed data of figure \ref{true_model}. The simulated data generated by this `solution' is plotted, red, compared to the observed data, black. The smoothness, equation \ref{smoothness}, and misfit, equation \ref{misfit}, for this model are displayed alongside the data. The misfit during the initial phase for this chain is plotted in figure \ref{comparison-1}. This model can be compared to the equivalent for the diagnostic ABC inversion, figure \ref{abc-im-1}.}
	\label{ana-im-1}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{abc_im.pdf}
	\caption{The mean of the marginal ABC posterior, $p_{ABC}(\bm{\theta}|\bm{S}(\bm{y}))$, targeting the `true model' and observed data of figure \ref{true_model}. The simulated data generated by this `solution' is plotted, blue, compared to the observed data, black. The smoothness, equation \ref{smoothness}, and misfit, equation \ref{misfit}, for this model are displayed alongside the data. The misfit during the initial phase for this chain is plotted in figure \ref{comparison-1}. This model can be compared to the equivalent for the analytically defined posterior, figure \ref{ana-im-1}.}
	\label{abc-im-1}
\end{figure}


\section{Crustal density joint inversion}
\label{JI}

As a second experiment I extend the inversion for crustal density ($\rho$) for a 2D discretized subsurface to a joint inversion with vertical gravity anomaly ($\Delta g$) and travel times ($\Delta t$) for compressional-waves $V_p$. Both datasets share the same underlying parameter space in $\rho$. The calculation for $V_p$ is made through the empirical relationship for $V_p$ as a function of $\rho$ defined by \citet{Brocher2005}:
\begin{equation}
	V_p\ (km/sec) = 0.8228\rho^5 - 9.1819\rho^4 + 37.0.83\rho^3 - 63.064\rho^2 + 39.128\rho
	\label{rhotovp}
\end{equation}
This relationship is valid for crustal rocks with $\rho$ in the range of 2-3.5 $g/cm^3$. A uniform prior distribution bounds the parameter space, $p(\bm{\theta}) = \mathcal{U}(2,3.5)$. As with the first example the `true model' is kept smooth, figure \ref{true-model-large-grav}. The parameter space is expanded to a 20x6 grid over the same 160 $km$ by 40 $km$ area. There is a $\Delta g$ measurement made above each of the 20 grid columns. Both the $\Delta g$ and $\Delta t$ datasets are contaminated with measurement uncertainty defined by $\sigma^{\mathcal{D}} = \mathcal{N}(0,\sqrt{2})$. As before, our ABC-tomography algorithm will be compared to a MCMC algorithm targeting an analytically-defined posterior. The likelihood for this problem is based on \ref{joint-likelihood}, with a negative log-likelihood defined as:
\begin{equation}
	-l(\bm{\theta}|\bm{y}) = \frac{c(\bm{y_{\Delta g}}-\bm{y^*_{\Delta g}})^2}{(\sigma^{\mathcal{D}})^2} + \frac{(\bm{y_{V_p}}-\bm{y^*_{V_p}})^2}{(\sigma^{\mathcal{D}})^2}
\end{equation}
where $c$ is a constant defined so as the contribution to the negative log-likelihood of both datasets is of the same order of magnitude. The proposal distribution is held as $q(\cdot,\cdot) = N(0,I\cdot250)$ however, given the increased parameter space the subsurface is divided into 20 2x3 blocks. A random block is selected for update at each time step in the analytical scheme. The starting position is initialized from $\mathcal{U}(2,3.5)$  and once again held constant between both the analytical Markov chain and ABC-tomography. \par

I once again seek a comprehensive set of summary statistics to describe $\bm{y_{\Delta g}}$ and $\bm{y_{V_p}}$. The set $\begin{bmatrix}
\bar{\mu_{\Delta g}}\ \bar{\sigma_{\Delta g}}\ m\ b\ \sum R
\end{bmatrix}^T$ is used for $\bm{y_{\Delta g}}$. Given $\bm{y_{V_p}}$ is a vector of arrival times for all sources (12 sources in this application), instead of fitting statistics to the larger $\bm{y_{V_p}}$, $\bm{y_{V_p}}$ is decomposed to the set of data for each source, 1 to $l$, where $l$ is the number of sources. The arrivals from each individual source are then described by $\begin{bmatrix}
m\ b\ \sum R
\end{bmatrix}^T$. The weighting kernel for $\bm{y_{V_p}}$ then becomes:
\begin{equation}
	p(\bm{y_{V_p}}|\bm{y^*_{V_p}},\bm{\theta^*}) = \prod_{i = 1}^{l} p(\bm{y_i}|\bm{y^*_i},\bm{\theta^*})
\end{equation}
A normalization term $\bm{\sigma_S}$ is approximated for both datasets from 10,000 Monte Carlo simulations over the prior parameter range 2-3.5 $g/cm^3$. \par

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{true_model_gravity.pdf}
	\caption{The `observed data', vertical component of gravity ($\Delta g$), and `true model', a 2D density ($g/cm^3$) slice, which will be the target of our synthetic geophysical experiments to compare ABC to likelihood based Bayesian inference. A smoothness value, log($p(\bm{\theta})$), equation \ref{smoothness}, for the true model is plotted for reference to later solutions.}
	\label{true-model-large-grav}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.75]{observed_tomography.pdf}
	\caption{The `true model', a 2D $Vp$ ($km/s$) slice, converted from $\rho$ by the empirical relationship defined by equation \ref{rhotovp}. The black lines across the model define the rays from 12 sources on their path to 10 receivers. The arrival time $\Delta t$ of these rays at the receivers defines the `observed data'. A sample of observed data is plotted for 4 sources. The sources are numbered 1-12 starting from the top left and going to the top right, with 4 sources per side.}
	\label{true-model-tom}
\end{figure}

The ABC-tomography inversion adopts the same diagnostic scheme used in the previous section. At each time step in the chain, a discrete PDF is defined by the misfit, equation \ref{misfit}, between simulated data based on the current chain location, $\bm{y^*} \sim p(\bm{y^*}|\bm{\theta_{t-1}})$,  and observed data, $\bm{y}$. A random sample is then drawn, distributed according to this discrete PDF, to determine a column of parameters to update. The proposal distribution is then biased to direct the parameter updates based on whether the location sampled from the discrete PDF is deemed to be a poor fit due to the parameter values being too high, $y^*_{\Delta g} - y_{\Delta g} > 0$, or too low,  $y^*_{\Delta g} - y_{\Delta g} < 0$. The biased update is encoded by sampling a positively or negatively truncated Gaussian distribution, figure \ref{updates}, as the proposal distribution, $q(\cdot,\cdot)$.\par

Figure \ref{comparison-trunc} plots a comparison between the normalized misfit across both datasets, $\Delta g$ and $\Delta t$, for the analytical scheme, red, and diagnostic ABC-tomography, blue, during the initial phase of their respective Markov chains. This demonstrates a diagnostic ABC joint inversion can improve convergence to a low misfit model. While acceptance rate is particularly low compared to sampling of the analytically defined posterior, each move in the ABC scheme is more informed and as a result higher quality (i.e the misfit is lower). In light of this there is also scope for the joint implementation of Approximate and analytical Bayesian inference. Figure \ref{grav-trunc} and \ref{tom-trunc} plot the mean marginal model for the full chain length (100,000 time steps). Figure \ref{pdf-reg-tol} (\hyperref[supplementary-material]{Supplementary material}) plots a comparison between the sampled marginal ABC posterior and marginal analytical-posterior for a sub-set of the parameter space. Likewise, figure \ref{marginal-reg-tol} plots a comparison of the Markov chain trace for the diagnostic ABC scheme and the analytical posterior for a sub-set of the parameter space. The over-estimation of uncertainty in the ABC posterior (figure \ref{pdf-reg-tol} and \ref{marginal-reg-tol}) is a result of approximation introduced by the tolerance. This can be seen by comparing the marginal ABC posterior for a tolerance of $\epsilon = \vec{1}\cdot0.1$, figure \ref{pdf-reg-tol}, to the ABC posterior for a tolerance of $\epsilon= \vec{1}\cdot0.05$, figure \ref{pdf-half-tol}, and a tolerance of $\epsilon= \vec{1}\cdot0.025$, figure \ref{pdf-quart-tol}. As $\epsilon$ is decreased the posterior density becomes tightly clustered around the true parameter value. However, as tolerance is decreased the sampler efficiency is increasingly eroded, with lowered acceptance rates and increased $\tau$, figure \ref{comparison-half-tol} and \ref{comparison-quart-tol}. As discussed in chapter \ref{chapter2}, this trade-off can be addressed with improved sampling routines such as AM, DR and DRAM. ABC samplers are an active area and efficient new algorithms such as Hamiltonian ABC \citep{meeds2015hamiltonian} offer potential to sample increasingly accurate posteriors.\par

As an alternative to using a truncated Gaussian distribution to bias the parameter update, it is also possible to use a skew Gaussian distribution. This is applied in the \hyperref[supplementary-material]{Supplementary material}, figure \ref{comparison-skew}, \ref{grav-skew} and \ref{tom-skew}. The analytical definition for the skew normal distribution, $f(x)$, used to direct the parameter updates is:
\begin{equation}
\begin{split}
\phi(x) = \frac{1}{\sqrt{2\pi}}\text{exp}\Big(-\frac{x^2}{2}\Big) \\
\Phi(x) = \frac{1}{2}\bigg(1 + \text{erf}\Big(\frac{x}{\sqrt{2}}\Big)\bigg)\\
f(x) = \frac{2}{\omega} \phi\bigg(\frac{x-\xi}{\omega}\bigg)\Phi\bigg(\alpha \Big(\frac{x-\xi}{\omega}\Big)\bigg)
\end{split}
\end{equation}
here erf is the error function, $\xi$ is the location, $\omega$ is the scale and $\alpha$ is a shape parameter. In this application  $\xi$ is the current chain location, $\omega$ is held at 250, and $\alpha$ is either 1, for a positively skewed distribution, or -1, for a negatively skewed distribution. It should be noted that when using the skew normal distribution the ratio $\frac{q(\bm{\theta^*},\bm{\theta_{t-1}})}{q(\bm{\theta_{t-1}},\bm{\theta^*})}$ must be computed at each time step to ensure detailed balance is maintained.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{comparison-trunc.pdf}
	\caption{Truncated Gaussian scheme. The normalized misfit of the chain state during the initial phase (first 10,000 steps) for both datasets, $\Delta g$ and $\Delta t$, and for both methods, an analytically defined posterior sampled by MCMC and ABC-tomography. The `true model' and observed data is plotted in figure \ref{true-model-large-grav} and \ref{true-model-tom}. The increase in convergence to a low misfit model under ABC-tomography is a result of dynamically selecting, localizing and directly the update at each time step in the Markov chain. The details of the full chain run (100,000 time steps), are also displayed on the figure. $\tau$ is the integrated auto-correlation time. The mean marginal models from ABC-tomography using truncated Gaussian updates is plotted in figure \ref{grav-trunc} and \ref{tom-trunc}.}
	\label{comparison-trunc}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{abc_im-trunc.pdf}
	\caption{Truncated Gaussian scheme. The mean of the marginal ABC posterior, $p_{ABC}(\bm{\theta}|\bm{S}(\bm{y}))$, targeting the `true model' and observed data, figure \ref{true-model-large-grav}. The simulated data generated by this `solution', is plotted, blue, compared to the observed data, black. The misfit during the initial phase for this chain is plotted in figure \ref{comparison-trunc}. The corresponding mean marginal $V_p$ model is plotted in figure \ref{tom-trunc}.}
	\label{grav-trunc}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.7]{abc-trunc}
	\caption{Truncated Gaussian scheme. The mean of the marginal ABC posterior, $p_{ABC}(\bm{\theta}|\bm{S}(\bm{y}))$, targeting the `true model' and observed data of figure \ref{true-model-tom}. The simulated data generated by this `solution', is plotted, blue, compared to the observed data, black. The misfit during the initial phase for this chain is plotted in figure \ref{comparison-trunc}. The corresponding mean marginal $\rho$ model is plotted in figure \ref{grav-trunc}.}
	\label{tom-trunc}
\end{figure}