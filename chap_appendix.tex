\chapter{Appendix A}
\label{AppendixA}

\subsection{General likelihood}
\citet[p.90-92]{gregory2005bayesian} outline constructing a likelihood, $\mathcal{L}(\bm{\theta}|\bm{y})$, from starting assumptions. Firstly, a model for the data is assumed:
\begin{equation}
y_i = z_i + e_i
\label{general_form_inference}
\end{equation}
where $e_i$ is the uncertainty in the data, and $z_i$ is the data simulation from the deterministic forward relation:
\begin{equation}
z_i = \bm{g}(\bm{\theta})
\end{equation}
Both $z_i$ and $e_i$ are represented by distributions:
\begin{equation}
p(z_i|\bm{\theta}) = f_Z(z_i)
\label{fz_dist}
\end{equation}
\begin{equation}
p(e_i|\bm{\theta}) = f_E(e_i)
\label{fe_dist}
\end{equation}
We are critically interested in arriving in an equation for $p(y_i|\bm{\theta})$, which will define $\mathcal{L}(\bm{\theta}|\bm{y})$. However, as the relationship in equation \ref{general_form_inference} specifies, $y_i$ depends on both $z_i$ and $e_i$. Hence, to arrive at $p(y_i|\bm{\theta})$ we must consider the distributions of equations \ref{fz_dist} \& \ref{fe_dist}. If we first consider the joint distribution $p(y_i,z_i,e_i|\bm{\theta})$ then our likelihood can be found by integrating out, 'marginalizing', $p(z_i|\bm{\theta})$ and $p(e_i|\bm{\theta})$ to leave a distribution for $p(y_i|\bm{\theta})$:
\begin{equation}
p(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ p(y_i,z_i,e_i|\bm{\theta})
\label{most_general_L}
\end{equation}
The definition of conditional probability allows equation \ref{most_general_L} to be rewritten as:
\begin{equation}
p(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ p(z_i,e_i|\bm{\theta})\ p(y_i|z_i,e_i,\bm{\theta})
\end{equation}
If we assume $z_i$ and $e_i$ are independent:
\begin{equation}
p(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ p(z_i|\bm{\theta})\ p(e_i|\bm{\theta})\ p(y_i|z_i,e_i,\bm{\theta})
\label{halfway_through_derivation}
\end{equation}
From the relationship in equation \ref{general_form_inference}, $y_i-z_i-e_i = 0$. This relation considered, $p(y_i|z_i,e_i,\bm{\theta})$ can be reasonably represented as a dirac-delta function such that:
\begin{equation}
p(y_i|z_i,e_i,\bm{\theta}) = \delta(y_i-z_i-e_i)
\label{dirac_equality}
\end{equation}
This is a natural definition, as the probability density of $y_i$ given $z_i$ and $e_i$ should be focused when the exact relation of equation \ref{general_form_inference} is met. The dirac-delta function serves this role perfectly while still exhibiting properties of a PDF, non-negative and $\int_{-\infty}^{\infty}\delta = 1$. Considering equation \ref{dirac_equality}, equation \ref{halfway_through_derivation} becomes:
\begin{equation}
p(y_i|\bm{\theta}) = \int \int \text{d}z_i\ \text{d}e_i\ p(z_i|\bm{\theta})\ p(e_i|\bm{\theta})\ \delta(y_i-z_i-e_i)
\end{equation}
Adopting the form from equations \ref{fz_dist} \& \ref{fe_dist}:
\begin{equation}
p(y_i|\bm{\theta}) = \int \text{d}z_i\ f_Z(z_i) \int \text{d}e_i\ f_E(e_i) \delta(y_i-z_i-e_i)
\label{almost_general_form}
\end{equation}
The dirac-delta function in the second integral of equation \ref{almost_general_form} serves to isolate the value/values of $f_E(e_i)$ when $e_i = y_i - z_i$. Hence, it is equivalent to consider:
\begin{equation}
\int \text{d}e_i\ f_E(e_i) \delta(y_i-z_i-e_i) = f_E(y_i - z_i)
\end{equation}
This leaves a general form for the likelihood $\pi(y_i|\bm{\theta})$ as: 
\begin{equation}
p(y_i|\bm{\theta}) = \mathcal{L}(\bm{\theta}|y_i) = \int \text{d}z_i\ f_Z(z_i)\ f_E(y_i - z_i)
\label{general_likelihood}
\end{equation}
However this general equation is not in a form which can be practically applied to inference problems. To achieve a practical form further assumptions must be made about the distributions $f_Z(z_i)$ and $f_E(e)$. This is done by assigning parametric distributions. As we will see in the next section, assigning Gaussian distributions will allow an analytical solution to equation \ref{general_likelihood} as our general likelihood definition takes the form of a convolution. 

\subsection{Practical likelihood form}
It is assumed that both $y_i$ and $z_i$ contain statistical uncertainty, denoted $e_i$ and $\epsilon_i$ respectively. As a result: 
\begin{equation}
y_i = z_i + e_i
\end{equation}
\begin{equation}
z_i = \bm{g}(\bm{\theta}) + \epsilon_i
\end{equation}	
\begin{equation}
\therefore	y_i = \bm{g}(\bm{\theta}) + \epsilon_i + e_i
\end{equation}
$\epsilon_i$ and $e_i$ are assumed to be uncorrelated. If it is assumed that $\epsilon_i$ is described as independent and identically distributed (i.i.d) from a Gaussian with a predetermined standard deviation (s.t.d), $\sigma_{\epsilon}$:
\begin{equation}
p(z_i|\bm{\theta}) = f_Z(z_i) = \frac{1}{\sqrt{2\pi\sigma_{\epsilon}^2}}\ \text{exp}\bigg[\frac{-\epsilon_i^2}{2\sigma_{\epsilon}^2} \bigg]
\end{equation} 
Likewise, if $e_i$ is assumed to be described as i.i.d from a Gaussian with predefined standard deviation $\sigma_{e}$:
\begin{equation}
p(e_i|\bm{\theta}) = f_E(e_i) = f_E(y_i-z_i) = \frac{1}{\sqrt{2\pi\sigma_{e}^2}}\ \text{exp}\bigg[\frac{-e^2}{2\sigma_{e}^2} \bigg]
\end{equation}
As a result of defining parametric distributions for $f_Z(z_i)$ and $f_E(y_i-z_i)$ the general definition of $\mathcal{L}(\bm{\theta}|y_i)$, equation \ref{general_likelihood}, can be evaluated. This is the convolution of the two distributions with the closed-form expression:
\begin{equation}
\mathcal{L}(\theta|y_i) = \frac{1}{\sqrt{2\pi}\sqrt{\sigma_{\epsilon}^2+\sigma_{e}^2}} \text{exp}\bigg[\frac{-(y_i-\bm{g}(\bm{\theta}))^2}{2(\sigma_{\epsilon}^2+\sigma_{e}^2)}\bigg]
\end{equation}

If you have a set of data $\bm{y} = \{y_1,...,y_M\}$, where each $y_i$ term is independent then, considering uncertainty in the data and model as i.i.d Gaussian:
\begin{equation}
\begin{split}
p(\bm{y}|\bm{\theta}) &= p(y_1,...,y_m|\bm{\theta})\\
&= (2\pi)^{M/2}(\prod_{i = 1}^{M}(\sigma_{e}^2+\sigma_{\epsilon}^2)^{1/2})\ \text{exp}\bigg[\sum_{i = 1}^{M}\frac{-(y_i-\bm{g}({\bm{\theta}}))^2}{2(\sigma_{e}^2+\sigma_{\epsilon}^2)}\bigg]\\
\label{independant_data_likelihood}
\end{split}
\end{equation}
Least-squares, the sum of squares of residuals, is a standard technique for regression of over-determined problems, more data than model parameters. For a set of data $\bm{y}$ and some specific model parameters $\bm{\theta'}$
\begin{equation}
\text{least-squares} = LS = \sum_{i = 1}^{M} (y_i - \bm{g}(\bm{\theta'}))^2
\end{equation}
The relationship between least-squares and a likelihood of the form of equation \ref{independant_data_likelihood} can be seen as:
\begin{equation}
\text{exp}\bigg[\sum_{i = 1}^{M}\frac{-(y_i-\bm{g}({\bm{\theta'}}))^2}{2(\sigma_{e}^2+\sigma_{\epsilon}^2)}\bigg] = \text{exp}\bigg[\frac{-LS}{2(\sigma_{e}^2+\sigma_{\epsilon}^2)}\bigg]
\end{equation}\\

For the case where vectors of data and model parameters are considered together as represented by a multivariate Gaussian distribution, defined by covariance matrices $C_d$ and $C_m$ respectively, such that:
\begin{equation}
f_Z(z) \propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}-\bm{g}(\bm{\theta}))^TC_m^{-1}(\bm{y}-\bm{g}(\bm{\theta}))\bigg]
\end{equation}
\begin{equation}
f_E(e) \propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}-\bm{g}(\bm{\theta}))^TC_d^{-1}(\bm{y}-\bm{g}(\bm{\theta}))\bigg]
\end{equation}
Then the likelihood involves a combination of their covariance matrices:
\begin{equation}
\mathcal{L}(\bm{\theta}|\bm{y}) \propto \text{exp}\bigg[-\frac{1}{2}(\bm{y}-\bm{g}(\bm{\theta}))^T(C_d+C_m)^{-1}(\bm{y}-\bm{g}(\bm{\theta}))\bigg]
\end{equation}
Finally if it is assumed that there is no correlations between the errors in the model and within the data then the respective covariance matrices can be represented by identity matrices and the likelihood collapses to:
\begin{equation}
\mathcal{L}(\bm{\theta}|\bm{y}) \propto \text{exp}\bigg[-\frac{1}{2}LS\bigg]
\end{equation}