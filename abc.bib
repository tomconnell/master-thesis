Automatically generated by Mendeley Desktop 1.19.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{meeds2015hamiltonian,
author = {Meeds, Edward and Leenders, Robert and Welling, Max},
journal = {arXiv preprint arXiv:1503.01916},
title = {{Hamiltonian ABC}},
year = {2015}
}
@article{afonso2016,
author = {Afonso, Juan Carlos and Rawlinson, Nicholas and Yang, Yingjie and Schutt, Derek L and Jones, Alan G and Fullea, Javier and Griffin, William L},
journal = {Journal of Geophysical Research: Solid Earth},
number = {10},
pages = {7337--7370},
publisher = {Wiley Online Library},
title = {{3-D multiobservable probabilistic inversion for the compositional and thermal structure of the lithosphere and upper mantle: III. Thermochemical tomography in the Western-Central US}},
volume = {121},
year = {2016}
}
@incollection{Prangle2017,
abstract = {This document is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) edited by S. Sisson, Y. Fan, and M. Beaumont. Since the earliest work on ABC, it has been recognised that using summary statistics is essential to produce useful inference results. This is because ABC suffers from a curse of dimensionality effect, whereby using high dimensional inputs causes large approximation errors in the output. It is therefore crucial to find low dimensional summaries which are informative about the parameter inference or model choice task at hand. This chapter reviews the methods which have been proposed to select such summaries, extending the previous review paper of Blum et al. (2013) with recent developments. Related theoretical results on the ABC curse of dimensionality and sufficiency are also discussed.},
archivePrefix = {arXiv},
arxivId = {1512.05633},
author = {Prangle, Dennis},
booktitle = {Handbook of approximate Bayesian computation},
eprint = {1512.05633},
pages = {320},
title = {{Summary statistics in approximate Bayesian computation}},
url = {http://arxiv.org/abs/1512.05633},
year = {2017}
}
@book{Tarantola2005,
author = {Tarantola, Albert},
isbn = {0898715725},
publisher = {SIAM},
title = {{Inverse problem theory and methods for model parameter estimation}},
type = {Book},
year = {2005}
}
@article{Dutta2017,
abstract = {A numerical model that quantitatively describes how platelets in a shear flow adhere and aggregate on a deposition surface has been recently developed in Chopard et al. (2015); Chopard et al. (2017). Five parameters specify the deposition process and are relevant for a biomedical understanding of the phenomena. Experiments give observations, at five time intervals, on the average size of the aggregation clusters, their number per {\$}\backslashmbox{\{}mm{\}}{\^{}}2{\$}, the number of platelets and the ones activated per {\$}\backslashmu\backslashell{\$} still in suspension. By comparing {\$}in-vitro{\$} experiments with simulations, the model parameters can be manually tuned (Chopard et al. (2015); Chopard et al. (2017)). Here, we use instead approximate Bayesian computation (ABC) to calibrate the parameters in a data-driven automatic manner. ABC requires a prior distribution for the parameters, which we take to be uniform over a known range of plausible parameter values. ABC requires the generation of many pseudo-data by expensive simulation runs, we have thus developed an high performance computing (HPC) ABC framework, taking into account accuracy and scalability. The present approach can be used to build a new generation of platelets functionality tests for patients, by combining {\$}in-vitro{\$} observation, mathematical modeling, Bayesian inference and high performance computing.},
archivePrefix = {arXiv},
arxivId = {1710.01054},
author = {Dutta, Ritabrata and Chopard, Bastien and L{\"{a}}tt, Jonas and Dubois, Frank and Boudjeltia, Karim Zouaoui and Mira, Antonietta},
eprint = {1710.01054},
journal = {Arxiv},
pages = {1--15},
title = {{Parameter estimation of platelets deposition: Approximate Bayesian computation with high performance computing}},
url = {http://arxiv.org/abs/1710.01054},
year = {2017}
}
@article{Wood2010,
abstract = {Chaotic ecological dynamic systems defy conventional statistical analysis. Systems with near-chaotic dynamics are little better. Such systems are almost invariably driven by endogenous dynamic processes plus demographic and environmental process noise, and are only observable with error. Their sensitivity to history means that minute changes in the driving noise realization, or the system parameters, will cause drastic changes in the system trajectory. This sensitivity is inherited and amplified by the joint probability density of the observable data and the process noise, rendering it useless as the basis for obtaining measures of statistical fit. Because the joint density is the basis for the fit measures used by all conventional statistical methods, this is a major theoretical shortcoming. The inability to make well-founded statistical inferences about biological dynamic models in the chaotic and near-chaotic regimes, other than on an ad hoc basis, leaves dynamic theory without the methods of quantitative validation that are essential tools in the rest of biological science. Here I show that this impasse can be resolved in a simple and general manner, using a method that requires only the ability to simulate the observed data on a system from the dynamic model about which inferences are required. The raw data series are reduced to phase-insensitive summary statistics, quantifying local dynamic structure and the distribution of observations. Simulation is used to obtain the mean and the covariance matrix of the statistics, given model parameters, allowing the construction of a 'synthetic likelihood' that assesses model fit. This likelihood can be explored using a straightforward Markov chain Monte Carlo sampler, but one further post-processing step returns pure likelihood-based inference. I apply the method to establish the dynamic nature of the fluctuations in Nicholson's classic blowfly experiments.},
author = {Wood, Simon N.},
doi = {10.1038/nature09319},
isbn = {1476-4687},
issn = {00280836},
journal = {Nature},
number = {7310},
pages = {1102--1104},
pmid = {20703226},
title = {{Statistical inference for noisy nonlinear ecological dynamic systems}},
volume = {466},
year = {2010}
}
@book{Gilks1995,
author = {Gilks, Walter R and Richardson, Sylvia and Spiegelhalter, David},
isbn = {0412055511},
publisher = {CRC press},
title = {{Markov chain Monte Carlo in practice}},
type = {Book},
year = {1995}
}
@article{Blum2013,
abstract = {Approximate Bayesian computation (ABC) methods make use of comparisons between simulated and observed summary statistics to over-come the problem of computationally intractable likelihood functions. As the practical implementation of ABC requires computations based on vectors of summary statistics, rather than full data sets, a central question is how to derive low-dimensional summary statistics from the observed data with min-imal loss of information. In this article we provide a comprehensive review and comparison of the performance of the principal methods of dimension reduction proposed in the ABC literature. The methods are split into three nonmutually exclusive classes consisting of best subset selection methods, projection techniques and regularization. In addition, we introduce two new methods of dimension reduction. The first is a best subset selection method based on Akaike and Bayesian information criteria, and the second uses ridge regression as a regularization procedure. We illustrate the performance of these dimension reduction techniques through the analysis of three challeng-ing models and data sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.3819v3},
author = {Blum, M. G. B. and Nunes, M. A. and Prangle, D. and Sisson, S. A.},
doi = {10.1214/12-STS406},
eprint = {arXiv:1202.3819v3},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
number = {2},
pages = {189--208},
title = {{A Comparative Review of Dimension Reduction Methods in Approximate Bayesian Computation}},
url = {http://projecteuclid.org/euclid.ss/1369147911},
volume = {28},
year = {2013}
}
@article{Robert2010,
abstract = {The new perspectives on ABC and Bayesian model criticisms presented in Ratmann et al.(2009) are challenging standard approaches to Bayesian model choice. We discuss here some issues arising from the authors' approach, including prior influence, model assessment and criticism, and the meaning of error in ABC.},
archivePrefix = {arXiv},
arxivId = {0909.5673},
author = {Robert, C. P. and Mengersen, K. and Chen, C.},
doi = {10.1073/pnas.0911260107},
eprint = {0909.5673},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {3},
pages = {E5--E5},
pmid = {20080613},
title = {{Model choice versus model criticism}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0911260107},
volume = {107},
year = {2010}
}
@misc{Hartig2011,
abstract = {Statistical models are the traditional choice to test scientific theories when observations, processes or boundary conditions are subject to stochasticity. Many important systems in ecology and biology, however, are difficult to capture with statistical models. Stochastic simulation models offer an alternative, but they were hitherto associated with a major disadvantage: their likelihood functions can usually not be calculated explicitly, and thus it is difficult to couple them to well-established statistical theory such as maximum likelihood and Bayesian statistics. A number of new methods, among them Approximate Bayesian Computing and Pattern-Oriented Modelling, bypass this limitation. These methods share three main principles: aggregation of simulated and observed data via summary statistics, likelihood approximation based on the summary statistics, and efficient sampling. We discuss principles as well as advantages and caveats of these methods, and demonstrate their potential for integrating stochastic simulation models into a unified framework for statistical modelling.},
author = {Hartig, Florian and Calabrese, Justin M. and Reineking, Bj{\"{o}}rn and Wiegand, Thorsten and Huth, Andreas},
booktitle = {Ecology Letters},
doi = {10.1111/j.1461-0248.2011.01640.x},
isbn = {1461-0248},
issn = {1461023X},
keywords = {Bayesian statistics,Indirect inference,Intractable likelihood,Inverse modelling,Likelihood approximation,Likelihood-free inference,Maximum likelihood,Model selection,Parameter estimation,Stochastic simulation},
number = {8},
pages = {816--827},
pmid = {21679289},
title = {{Statistical inference for stochastic simulation models - theory and application}},
volume = {14},
year = {2011}
}
@article{Sisson2010a,
abstract = {To appear to MCMC handbook, S. P. Brooks, A. Gelman, G. Jones and X.-L. Meng (eds), Chapman {\&} Hall.},
archivePrefix = {arXiv},
arxivId = {arXiv:1001.2058v1},
author = {Sisson, S a and Fan, Y},
eprint = {arXiv:1001.2058v1},
journal = {Handbook of Markov chain Monte Carlo},
number = {Mcmc},
title = {{Likelihood-free Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1001.2058},
year = {2010}
}
@article{Lintusaari2016,
abstract = {—Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible. We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.},
author = {Lintusaari, Jarno and Gutmann, Michael U and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
doi = {10.1093/sysbio/syw077},
issn = {1063-5157},
journal = {Syst. Biol},
keywords = {Bayesian inference,[ABC,approximate Bayesian computation,likelihood-free inference,phylogenetics,simulator-based models,stochastic simulation models,tree-based models]},
number = {0},
pages = {1--17},
pmid = {27798401},
title = {{Fundamentals and Recent Developments in Approximate Bayesian Computation}},
volume = {00},
year = {2016}
}
@article{Blum2010,
abstract = {Approximate Bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood is either mathematically or computa-tionally intractable. However the methods that use rejection suffer from the curse of dimensionality when the number of summary statistics is increased. Here we propose a machine-learning approach to the estimation of the posterior density by introducing two innovations. The new method fits a non-linear conditional heteroscedastic regression of the parame-ter on the summary statistics, and then adaptively improves estimation using importance sampling. The new algorithm is compared to the state-of-the-art approximate Bayesian methods, and achieves considerable reduction of the com-putational burden in two examples of inference in statistical genetics and in a queueing model.},
archivePrefix = {arXiv},
arxivId = {arXiv:0809.4178v2},
author = {Blum, Michael G B and Fran{\c{c}}ois, Olivier and Blum, M G B and Fran{\c{c}}ois, O},
doi = {10.1007/s11222-009-9116-0},
eprint = {arXiv:0809.4178v2},
isbn = {0960-3174$\backslash$n1573-1375},
issn = {09603174},
journal = {Stat Comput},
keywords = {Approximate,Bayesian computation {\textperiodcentered},Coalescent models {\textperiodcentered},Conditional density estimation {\textperiodcentered},Curse of dimensionality {\textperiodcentered},Feed forward neural networks {\textperiodcentered},Heteroscedasticity {\textperiodcentered},Implicit statistical models {\textperiodcentered},Importance sampling {\textperiodcentered},Indirect inference,Likelihood-free inference {\textperiodcentered},Non-linear regression {\textperiodcentered}},
pages = {63--73},
pmid = {273403900006},
title = {{Non-linear regression models for Approximate Bayesian Computation}},
url = {http://dx.doi.org/10.1007/s11222-009-9116-0},
volume = {20},
year = {2010}
}
@article{gutmann2016bayesian,
author = {Gutmann, Michael U and Corander, Jukka and Others},
journal = {Journal of Machine Learning Research},
publisher = {MICROTOME PUBL},
title = {{Bayesian optimization for likelihood-free inference of simulator-based statistical models}},
year = {2016}
}
@article{Beaumont2002,
author = {Beaumont, Mark A and Zhang, Wenyang and Balding, David J},
journal = {Genetics},
number = {4},
pages = {2025--2035},
title = {{Approximate Bayesian Computation in Population Genetics}},
type = {Journal Article},
volume = {162},
year = {2002}
}
@article{Sadegh2014,
abstract = {Sadegh, M., and J. A. Vrugt (2014), Approximate Bayesian Computation using Markov Chain Monte Carlo simulation: DREAM(ABC), Water Resour. Res., 50, 6767–6787, doi:10.1002/ 2014WR015386. Received},
author = {Sadegh, M. and Vrugt, J. A.},
doi = {10.1002/2014WR015386.Received},
issn = {00431397},
journal = {Water Resources Research},
number = {2},
pages = {6767--6787},
title = {{Approximate Bayesian Computation using Markov Chain Monte Carlo simulation}},
volume = {10},
year = {2014}
}
@article{Tarantola1982a,
abstract = {We examine the general non-linear inverse problem with a finite number of parameters. In order to permit the incorporation of any a priori information about parameters and any distribution of data (not only of gaussian type) we propose to formulate the problem not using single quantities (such as bounds, means, etc.) but using probability density functions for data and parameters. We also want our formulation to allow for the incorporation of theoretical errors, i.e. non-exact theoretical relationships between data and parameters (due to discretization, or incomplete theoretical knowledge); to do that in a natural way we propose to define general theoretical relationships also as probability density functions. We show then that the inverse problem may be formulated as a problem of combination of information: the experimental information about data, the a priori information about parameters, and the theoretical information. With this approach, the general solution of the non-linear inverse problem is unique and consistent (solving the same problem, with the same data, but with a different system of parameters does not change the solution).},
author = {Tarantola, Albert and Valette, Bernard},
doi = {10.1038/nrn1011},
isbn = {0340-062X},
issn = {0340062X},
journal = {Journal of Geophysics},
keywords = {Information,Inverse problems,Pattern recognition,Probability},
number = {3},
pages = {159--170},
pmid = {12511864},
title = {{Inverse Problems = Quest for Information}},
url = {http://www.ipgp.fr/{~}tarantola/Files/Professional/Papers{\_}PDF/IP{\_}QI{\_}latex.pdf},
volume = {50},
year = {1982}
}
@article{Sadegh2015,
abstract = {Many watershed models used within the hydrologic research community assume (by default) stationary conditions, that is, the key watershed properties that control water flow are considered to be time invariant. This assumption is rather convenient and pragmatic and opens up the wide arsenal of (multivariate) statistical and nonlinear optimization methods for inference of the (temporally fixed) model parameters. Several contributions to the hydrologic literature have brought into question the continued usefulness of this stationary paradigm for hydrologic modeling. This paper builds on the likelihood-free diagnostics approach of Vrugt and Sadegh (2013) and uses a diverse set of hydrologic summary metrics to test the stationary hypothesis and detect changes in the watersheds response to hydroclimatic forcing. Models with fixed parameter values cannot simulate adequately temporal variations in the summary statistics of the observed catchment data, and consequently, the DREAM(ABC) algorithm cannot find solutions that sufficiently honor the observed metrics. We demonstrate that the presented methodology is able to differentiate successfully between watersheds that are classified as stationary and those that have undergone significant changes in land use, urbanization, and/or hydroclimatic conditions, and thus are deemed nonstationary.},
archivePrefix = {arXiv},
arxivId = {10.1002/2014WR016527},
author = {Sadegh, Mojtaba and Vrugt, Jasper A. and Xu, Chonggang and Volpi, Elena},
doi = {10.1002/2014WR016805},
eprint = {2014WR016527},
isbn = {1944-7973},
issn = {19447973},
journal = {Water Resources Research},
keywords = {Nonstationarity,approximate Bayesian computation,process-based model evaluation},
number = {11},
pages = {9207--9231},
pmid = {1000287943},
primaryClass = {10.1002},
title = {{The stationarity paradigm revisited: Hypothesis testing using diagnostics, summary metrics, and DREAM(ABC)}},
volume = {51},
year = {2015}
}
@article{raynal2017abc,
author = {Raynal, Louis and Marin, Jean-Michel and Pudlo, Pierre and Ribatet, Mathieu and Robert, Christian P and Estoup, Arnaud},
journal = {arXiv preprint arXiv:1605.05537},
title = {{ABC random forests for Bayesian parameter inference}},
year = {2017}
}
@article{Ratmann2010,
abstract = {In their letter to PNAS and a comprehensive set of notes on arXiv [arXiv:0909.5673v2], Christian Robert, Kerrie Mengersen and Carla Chen (RMC) represent our approach to model criticism in situations when the likelihood cannot be computed as a way to "contrast several models with each other". In addition, RMC argue that model assessment with Approximate Bayesian Computation under model uncertainty (ABCmu) is unduly challenging and question its Bayesian foundations. We disagree, and clarify that ABCmu is a probabilistically sound and powerful too for criticizing a model against aspects of the observed data, and discuss further the utility of ABCmu.},
archivePrefix = {arXiv},
arxivId = {0912.3182},
author = {Ratmann, O. and Andrieu, C. and Wiuf, C. and Richardson, S.},
doi = {10.1073/pnas.0912887107},
eprint = {0912.3182},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {3},
pages = {E6--E7},
title = {{Reply to Robert et al.: Model criticism informs model choice and model comparison}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0912887107},
volume = {107},
year = {2010}
}
@article{Li2017,
abstract = {Approximate Bayesian computation (ABC) refers to a family of inference methods used in the Bayesian analysis of complex models where evaluation of the likelihood is difficult. Conventional ABC methods often suffer from the curse of dimensionality, and a marginal adjustment strategy was recently introduced in the literature to improve the performance of ABC algorithms in high-dimensional problems. The marginal adjustment approach is extended using a Gaussian copula approximation. The method first estimates the bivariate posterior for each pair of parameters separately using a 2-dimensional Gaussian copula, and then combines these estimates together to estimate the joint posterior. The approximation works well in large sample settings when the posterior is approximately normal, but also works well in many cases which are far from that situation due to the nonparametric estimation of the marginal posterior distributions. If each bivariate posterior distribution can be well estimated with a low-dimensional ABC analysis then this Gaussian copula method can extend ABC methods to problems of high dimension. The method also results in an analytic expression for the approximate posterior which is useful for many purposes such as approximation of the likelihood itself. This method is illustrated with several examples.},
archivePrefix = {arXiv},
arxivId = {1504.04093},
author = {Li, J. and Nott, D. J. and Fan, Y. and Sisson, S. A.},
doi = {10.1016/j.csda.2016.07.005},
eprint = {1504.04093},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate Bayesian Computation (ABC),Gaussian copula,Likelihood free inference,Marginal adjustment,Regression adjustment ABC},
pages = {77--89},
title = {{Extending approximate Bayesian computation methods to high dimensions via a Gaussian copula model}},
volume = {106},
year = {2017}
}
@misc{Marjoram2006,
abstract = {An explosive growth is occurring in the quantity, quality and complexity of molecular variation data that are being collected. Historically, such data have been analysed by using model-based methods. Models are useful for sharpening intuition, for explanation and for prediction: they add to our understanding of how the data were formed, and they can provide quantitative answers to questions of interest. We outline some of these model-based approaches, including the coalescent, and discuss the applicability of the computational methods that are necessary given the highly complex nature of current and future data sets.},
author = {Marjoram, Paul and Tavar{\'{e}}, Simon},
booktitle = {Nature Reviews Genetics},
doi = {10.1038/nrg1961},
isbn = {1471-0056},
issn = {14710056},
number = {10},
pages = {759--770},
pmid = {16983372},
title = {{Modern computational approaches for analysing molecular genetic variation data}},
volume = {7},
year = {2006}
}
@article{Leuenberger2010a,
abstract = {Until recently, the use of Bayesian inference was limited to a few cases because for many realistic probability models the likelihood function cannot be calculated analytically. The situation changed with the advent of likelihood-free inference algorithms, often subsumed under the term approximate Bayesian computation (ABC). A key innovation was the use of a postsampling regression adjustment, allowing larger tolerance values and as such shifting computation time to realistic orders of magnitude. Here we propose a reformulation of the regression adjustment in terms of a general linear model (GLM). This allows the integration into the sound theoretical framework of Bayesian statistics and the use of its methods, including model selection via Bayes factors. We then apply the proposed methodology to the question of population subdivision among western chimpanzees, Pan troglodytes verus.},
author = {Leuenberger, Christoph and Wegmann, Daniel},
doi = {10.1534/genetics.109.109058},
isbn = {0016-6731},
issn = {00166731},
journal = {Genetics},
number = {1},
pages = {243--252},
pmid = {19786619},
title = {{Bayesian computation and model selection without likelihoods}},
volume = {184},
year = {2010}
}
@article{blum2017regression,
author = {Blum, Michael G B},
journal = {arXiv preprint arXiv:1707.01254},
title = {{Regression approaches for Approximate Bayesian Computation}},
year = {2017}
}
@article{afonso2013a,
author = {Afonso, J C and Fullea, J and Griffin, W L and Yang, Y and Jones, A G and {D Connolly}, J A and O'Reilly, S Y},
journal = {Journal of Geophysical Research: Solid Earth},
number = {5},
pages = {2586--2617},
publisher = {Wiley Online Library},
title = {{3-D multiobservable probabilistic inversion for the compositional and thermal structure of the lithosphere and upper mantle. I: A priori petrological information and geophysical observables}},
volume = {118},
year = {2013}
}
@article{Marjoram2003,
abstract = {Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion.},
author = {Marjoram, Paul and Molitor, John and Plagnol, Vincent and Tavare, Simon},
doi = {10.1073/pnas.0306899100},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Biological,Biological Evolution,Computer Simulation,DNA,DNA: genetics,Genetics,Humans,Likelihood Functions,Markov Chains,Mitochondrial,Mitochondrial: genetics,Models,Monte Carlo Method,Population,Stochastic Processes},
number = {26},
pages = {15324--8},
pmid = {14663152},
title = {{Markov chain Monte Carlo without likelihoods.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=307566{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {100},
year = {2003}
}
@article{Akeret2015,
abstract = {Bayesian inference is often used in cosmology and astrophysics to derive constraints on model parameters from observations. This approach relies on the ability to compute the likelihood of the data given a choice of model parameters. In many practical situations, the likelihood function may however be unavailable or intractable due to non-gaussian errors, non-linear measurements processes, or complex data formats such as catalogs and maps. In these cases, the simulation of mock data sets can often be made through forward modeling. We discuss how Approximate Bayesian Computation (ABC) can be used in these cases to derive an approximation to the posterior constraints using simulated data sets. This technique relies on the sampling of the parameter set, a distance metric to quantify the difference between the observation and the simulations and summary statistics to compress the information in the data. We first review the principles of ABC and discuss its implementation using a Population Monte-Carlo (PMC) algorithm and the Mahalanobis distance metric. We test the performance of the implementation using a Gaussian toy model. We then apply the ABC technique to the practical case of the calibration of image simulations for wide field cosmological surveys. We find that the ABC analysis is able to provide reliable parameter constraints for this problem and is therefore a promising technique for other applications in cosmology and astrophysics. Our implementation of the ABC PMC method is made available via a public code release.},
archivePrefix = {arXiv},
arxivId = {1504.07245},
author = {Akeret, Jo?l and Refregier, Alexandre and Amara, Adam and Seehars, Sebastian and Hasner, Caspar},
doi = {10.1088/1475-7516/2015/08/043},
eprint = {1504.07245},
issn = {14757516},
journal = {Journal of Cosmology and Astroparticle Physics},
keywords = {cosmological simulations,non-gaussianity,weak gravitational lensing},
number = {8},
title = {{Approximate Bayesian computation for forward modeling in cosmology}},
volume = {2015},
year = {2015}
}
@article{Mosegaard1995,
abstract = {Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines a priori informa- tion with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the a posteriori probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.). When analyzing an inverse problem, obtaining a maximum likelihood model is usu- ally not sufficient, as we normally also wish to have infor- mation on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distri- bution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be ac- complished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available. The most well known impor- tance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows anal- ysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.},
author = {Mosegaard, Klaus and Tarantola, Albert},
doi = {10.1029/94JB03097},
isbn = {0148-0227},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
number = {B7},
pages = {12431--12447},
title = {{Monte Carlo sampling of solutions to inverse problems}},
url = {http://doi.wiley.com/10.1029/94JB03097},
volume = {100},
year = {1995}
}
@article{Weiss1998a,
abstract = {We introduce an approach to revealing the likelihood of different population histories that utilizes an explicit model of sequence evolution for the DNA segment under study. Based on a phylogenetic tree reconstruction method we show that a Tamura-Nei model with heterogeneous mutation rates is a fair description of the evolutionary process of the hypervariable region I of the mitochondrial DNA from humans. Assuming this complex model still allows the estimation of population history parameters, we suggest a likelihood approach to conducting statistical inference within a class of expansion models. More precisely, the likelihood of the data is based on the mean pairwise differences between DNA sequences and the number of variable sites in a sample. The use of likelihood ratios enables comparison of different hypotheses about population history, such as constant population size during the past or an increase or decrease of population size starting at some point back in time. This method was applied to show that the population of the Basques has expanded, whereas that of the Biaka pygmies is most likely decreasing. The Nuu-Chah-Nulth data are consistent with a model of constant population.},
author = {Weiss, G and von Haeseler, A},
isbn = {0016-6731 (Print)},
issn = {0016-6731},
journal = {Genetics},
number = {July},
pages = {1539--1546},
pmid = {9649540},
title = {{Inference of population history using a likelihood approach.}},
volume = {149},
year = {1998}
}
@article{Tavare1997,
author = {Tavare, S and Balding, D J and Griffiths, R C and Donnelly, P},
issn = {0016-6731},
journal = {Genetics},
number = {2},
pages = {505--518},
title = {{Inferring Coalescence Times from DNA Sequence Data}},
type = {Journal Article},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1207814/},
volume = {145},
year = {1997}
}
@misc{sisson2016handbook,
author = {Sisson, S A and Fan, Y and Beaumont, M},
publisher = {Taylor {\&} Francis},
title = {{Handbook of Approximate Bayesian Computation}},
year = {2016}
}
@article{Sunnaker2013,
abstract = {Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences (e.g., in population genetics, ecology, epidemiology, and systems biology).},
archivePrefix = {arXiv},
arxivId = {0811.3355},
author = {Sunn{\aa}ker, Mikael and Busetto, Alberto Giovanni and Numminen, Elina and Corander, Jukka and Foll, Matthieu and Dessimoz, Christophe},
doi = {10.1371/journal.pcbi.1002803},
eprint = {0811.3355},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {1},
pmid = {23341757},
title = {{Approximate Bayesian Computation}},
volume = {9},
year = {2013}
}
@article{Ishida2015,
abstract = {Approximate Bayesian Computation (ABC) enables parameter inference for complex physical systems in cases where the true likelihood function is unknown, unavailable, or computationally too expensive. It relies on the forward simulation of mock data and comparison between observed and synthetic catalogues. Here we present cosmoabc, a Python ABC sampler featuring a Population Monte Carlo variation of the original ABC algorithm, which uses an adaptive importance sampling scheme. The code is very flexible and can be easily coupled to an external simulator, while allowing to incorporate arbitrary distance and prior functions. As an example of practical application, we coupled cosmoabc with the numcosmo library and demonstrate how it can be used to estimate posterior probability distributions over cosmological parameters based on measurements of galaxy clusters number counts without computing the likelihood function. cosmoabc is published under the GPLv3 license on PyPI and GitHub and documentation is available at http://goo.gl/SmB8EX.},
archivePrefix = {arXiv},
arxivId = {1504.06129},
author = {Ishida, E. E.O. and Vitenti, S. D.P. and Penna-Lima, M. and Cisewski, J. and de Souza, R. S. and Trindade, A. M.M. and Cameron, E. and Busti, V. C.},
doi = {10.1016/j.ascom.2015.09.001},
eprint = {1504.06129},
isbn = {1553-7358},
issn = {22131337},
journal = {Astronomy and Computing},
keywords = {(cosmology:) large-scale structure of universe,Galaxies: statistics},
pages = {1--11},
title = {{Cosmoabc: Likelihood-free inference via Population Monte Carlo Approximate Bayesian Computation}},
volume = {13},
year = {2015}
}
@article{khan2007joint,
author = {Khan, Amir and Connolly, J A D and Maclennan, J and Mosegaard, Klaus},
journal = {Geophysical Journal International},
number = {1},
pages = {243--258},
publisher = {Blackwell Publishing Ltd Oxford, UK},
title = {{Joint inversion of seismic and gravity data for lunar composition and thermal state}},
volume = {168},
year = {2007}
}
@article{Naesseth2017,
abstract = {Variational inference underlies many recent advances in large scale probabilistic modeling. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions; and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, variational sequential Monte Carlo (VSMC), and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. VSMC is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters.},
archivePrefix = {arXiv},
arxivId = {1705.11140},
author = {Naesseth, Christian A. and Linderman, Scott W. and Ranganath, Rajesh and Blei, David M.},
eprint = {1705.11140},
journal = {arXiv},
title = {{Variational Sequential Monte Carlo}},
url = {http://arxiv.org/abs/1705.11140},
year = {2017}
}
@article{Pudlo2015,
abstract = {Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques. We propose a novel approach based on a machine learning tool named random forests to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with random forests and postponing the approximation of the posterior probability of the predicted MAP for a second stage also relying on random forests. Compared with earlier implementations of ABC model choice, the ABC random forest approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least fifty), and (iv) it includes an approximation of the posterior probability of the selected model. The call to random forests will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. The proposed methodologies are implemented in the R package abcrf available on the CRAN.},
archivePrefix = {arXiv},
arxivId = {1406.6288},
author = {Pudlo, Pierre and Marin, Jean Michel and Estoup, Arnaud and Cornuet, Jean Marie and Gautier, Mathieu and Robert, Christian P.},
doi = {10.1093/bioinformatics/btv684},
eprint = {1406.6288},
issn = {14602059},
journal = {Bioinformatics},
number = {6},
pages = {859--866},
pmid = {26589278},
title = {{Reliable ABC model choice via random forests}},
volume = {32},
year = {2015}
}
@article{Ratmann2009,
abstract = {Mathematical models are an important tool to explain and comprehend complex phenomena, and unparalleled computational advances enable us to easily explore them without any or little understanding of their global properties. In fact, the likelihood of the data under complex stochastic models is often analytically or numerically intractable in many areas of sciences. This makes it even more important to simultaneously investigate the adequacy of these models-in absolute terms, against the data, rather than relative to the performance of other models-but no such procedure has been formally discussed when the likelihood is intractable. We provide a statistical interpretation to current developments in likelihood-free Bayesian inference that explicitly accounts for discrepancies between the model and the data, termed Approximate Bayesian Computation under model uncertainty (ABCmicro). We augment the likelihood of the data with unknown error terms that correspond to freely chosen checking functions, and provide Monte Carlo strategies for sampling from the associated joint posterior distribution without the need of evaluating the likelihood. We discuss the benefit of incorporating model diagnostics within an ABC framework, and demonstrate how this method diagnoses model mismatch and guides model refinement by contrasting three qualitative models of protein network evolution to the protein interaction datasets of Helicobacter pylori and Treponema pallidum. Our results make a number of model deficiencies explicit, and suggest that the T. pallidum network topology is inconsistent with evolution dominated by link turnover or lateral gene transfer alone.},
author = {Ratmann, O. and Andrieu, C. and Wiuf, C. and Richardson, S.},
doi = {10.1073/pnas.0807882106},
isbn = {1091-6490},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {26},
pages = {10576--10581},
pmid = {19525398},
title = {{Model criticism based on likelihood-free inference, with an application to protein network evolution}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0807882106},
volume = {106},
year = {2009}
}
@article{Fu1997,
abstract = {We present a simple Monte Carlo method for estimating the age of the most recent common ancestor (MRCA) of a sample of DNA sequences. We show that Templeton's (1993) estimator of the age of the MRCA based on the maximum number of nucleotide differences between two sequences in a sample is inaccurate, and we demonstrate the new method by reanalyzing a sample of DNA sequences from human Y chromosomes and a sample of human Alu sequences.},
author = {Fu, Yun Xin and Li, Wen Hsiung},
doi = {10.1093/oxfordjournals.molbev.a025753},
isbn = {0737-4038},
issn = {07374038},
journal = {Molecular Biology and Evolution},
keywords = {Alu sequence,Y chromosome,age estimation,coalescent approach,common ancestor},
number = {2},
pages = {195--199},
pmid = {9029798},
title = {{Estimating the age of the common ancestor of a sample of DNA sequences}},
volume = {14},
year = {1997}
}
@article{Pritchard1999a,
abstract = {We use variation at a set of eight human Y chromosome microsatellite loci to investigate the demographic history of the Y chromosome. Instead of assuming a population of constant size, as in most of the previous work on the Y chromosome, we consider a model which permits a period of recent population growth. We show that for most of the populations in our sample this model fits the data far better than a model with no growth. We estimate the demographic parameters of this model for each population and also the time to the most recent common ancestor. Since there is some uncertainty about the details of the microsatellite mutation process, we consider several plausible mutation schemes and estimate the variance in mutation size simultaneously with the demographic parameters of interest. Our finding of a recent common ancestor (probably in the last 120,000 years), coupled with a strong signal of demographic expansion in all populations, suggests either a recent human expansion from a small ancestral population, or natural selection acting on the Y chromosome.},
author = {Pritchard, J. K. and Seielstad, M. T. and Perez-Lezaun, A. and Feldman, M. W.},
doi = {10.1093/oxfordjournals.molbev.a026091},
isbn = {0737-4038},
issn = {0737-4038},
journal = {Molecular Biology and Evolution},
number = {12},
pages = {1791--1798},
pmid = {10605120},
title = {{Population growth of human Y chromosomes: a study of Y chromosome microsatellites}},
url = {https://academic.oup.com/mbe/article-lookup/doi/10.1093/oxfordjournals.molbev.a026091},
volume = {16},
year = {1999}
}
@article{vrugt2013toward,
author = {Vrugt, Jasper A and Sadegh, Mojtaba},
journal = {Water Resources Research},
number = {7},
pages = {4335--4345},
publisher = {Wiley Online Library},
title = {{Toward diagnostic model calibration and evaluation: Approximate Bayesian computation}},
volume = {49},
year = {2013}
}
@article{afonso2013b,
author = {Afonso, J C and Fullea, J and Yang, Y and Connolly, J A D and Jones, A G},
journal = {Journal of Geophysical Research: Solid Earth},
number = {4},
pages = {1650--1676},
publisher = {Wiley Online Library},
title = {{3-D multi-observable probabilistic inversion for the compositional and thermal structure of the lithosphere and upper mantle. II: General methodology and resolution analysis}},
volume = {118},
year = {2013}
}
@article{Bortot2007,
abstract = {In the production of clean steels, the occurrence of imperfections - so-called "inclusions" - is unavoidable. The strength of a clean steel block is largely dependent on the size of the largest imperfection that it contains, so inference on extreme inclusion size forms an important part of quality control. Sampling is generally done by measuring imperfections on planar slices, leading to an extreme value version of a standard stereological problem: how to make inference on large inclusions using only the sliced observations. Under the assumption that inclusions are spherical, this problem has been tackled previously using a combination of extreme value models, stereological calculations, a Bayesian hierarchical model, and standard Markov chain Monte Carlo (MCMC) techniques. Our objectives in this article are twofold: (1) to assess the robustness of such inferences with respect to the assumption of spherical inclusions, and (2) to develop an inference procedure that is valid for nonspherical inclusions. We investigate both of these aspects by extending the spherical family for inclusion shapes to a family of ellipsoids. We then address the issue of robustness by assessing the performance of the spherical model when fitted to measurements obtained from a simulation of ellipsoidal inclusions. The issue of inference is more difficult, because likelihood calculation is not feasible for the ellipsoidal model. To handle this aspect, we propose a modification to a recently developed likelihood-free MCMC algorithm. After verifying the viability and accuracy of the proposed algorithm through a simulation study, we analyze a real inclusion dataset. comparing the inference obtained under the ellipsoidal inclusion model with that previously obtained assuming spherical inclusions. {\textcopyright} 2007 American Statistical Association.},
author = {Bortot, P. and Coles, S. G. and Sisson, S. A.},
doi = {10.1198/016214506000000988},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Approximate bayesian computation,Extreme value theory,Markov chain Monte Carlo,Simulated tempering,Steel inclusion,Stereology},
number = {477},
pages = {84--92},
title = {{Inference for stereological extremes}},
volume = {102},
year = {2007}
}
@article{Mosegaard2002,
author = {Mosegaard, Klaus and Tarantola, Albert},
issn = {0074-6142},
journal = {International Geophysics},
pages = {237--265},
title = {{Probabilistic approach to inverse problems}},
type = {Journal Article},
volume = {81},
year = {2002}
}
@article{Beaumont2009,
abstract = {Sequential techniques can enhance the efficiency of the approximate Bayesian computation algorithm, as in Sisson et al.'s (2007) partial rejection control version. While this method is based upon the theoretical works of Del Moral et al. (2006), the application to approximate Bayesian computation results in a bias in the approximation to the posterior. An alternative version based on genuine importance sampling arguments bypasses this difficulty, in connection with the population Monte Carlo method of Capp´e et al. (2004), and it includes an automatic scaling of the forward kernel. When applied to a population genetics example, it compares favourably with two other versions of the approximate algorithm.},
archivePrefix = {arXiv},
arxivId = {0805.2256},
author = {Beaumont, M. A. and Cornuet, J.-M. and Marin, J.-M. and Robert, C. P.},
doi = {10.1093/biomet/asp052},
eprint = {0805.2256},
isbn = {0006-3444},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {983--990},
pmid = {272179100018},
title = {{Adaptive approximate Bayesian computation}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asp052},
volume = {96},
year = {2009}
}
@article{Sisson2007,
abstract = {Recent new methods in Bayesian simulation have provided ways of evaluating posterior distributions in the presence of analytically or computationally intractable likelihood functions. Despite representing a substantial methodological advance, existing methods based on rejection sampling or Markov chain Monte Carlo can be highly inefficient and accordingly require far more iterations than may be practical to implement. Here we propose a sequential Monte Carlo sampler that convincingly overcomes these inefficiencies. We demonstrate its implementation through an epidemiological study of the transmission rate of tuberculosis.},
author = {Sisson, S A and Fan, Y and Tanaka, Mark M},
doi = {10.1073/pnas.0607208104},
isbn = {0607208104},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {6},
pages = {1760--5},
pmid = {17264216},
title = {{Sequential Monte Carlo without likelihoods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17264216{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1794282},
volume = {104},
year = {2007}
}
@book{gregory2005bayesian,
author = {Gregory, Phil},
publisher = {Cambridge University Press},
title = {{Bayesian Logical Data Analysis for the Physical Sciences: A Comparative Approach with Mathematica{\textregistered}Support}},
year = {2005}
}
