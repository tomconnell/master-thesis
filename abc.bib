Automatically generated by Mendeley Desktop 1.19.2
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Fisher1922,
abstract = {IX. On the illathematical Foundations of Theoretical Statistics.  1By RA , MA, Fellow of Gonville and Caims College, Cambridge, Chief Statistician, Rothamsted Experimental Station, Harpenden.  5. Examples of the Use of Criterion of Consistency ...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fisher, R. A.},
doi = {10.1098/rsta.1922.0009},
eprint = {arXiv:1011.1669v3},
isbn = {02643952},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
pmid = {25246403},
title = {{On the Mathematical Foundations of Theoretical Statistics}},
year = {1922}
}
@article{Tarantola1982a,
abstract = {We examine the general non-linear inverse problem with a finite number of parameters. In order to permit the incorporation of any a priori information about parameters and any distribution of data (not only of gaussian type) we propose to formulate the problem not using single quantities (such as bounds, means, etc.) but using probability density functions for data and parameters. We also want our formulation to allow for the incorporation of theoretical errors, i.e. non-exact theoretical relationships between data and parameters (due to discretization, or incomplete theoretical knowledge); to do that in a natural way we propose to define general theoretical relationships also as probability density functions. We show then that the inverse problem may be formulated as a problem of combination of information: the experimental information about data, the a priori information about parameters, and the theoretical information. With this approach, the general solution of the non-linear inverse problem is unique and consistent (solving the same problem, with the same data, but with a different system of parameters does not change the solution).},
author = {Tarantola, Albert and Valette, Bernard},
doi = {10.1038/nrn1011},
isbn = {0340-062X},
issn = {0340062X},
journal = {Journal of Geophysics},
keywords = {Information,Inverse problems,Pattern recognition,Probability},
number = {3},
pages = {159--170},
pmid = {12511864},
title = {{Inverse Problems = Quest for Information}},
url = {http://www.ipgp.fr/{~}tarantola/Files/Professional/Papers{\_}PDF/IP{\_}QI{\_}latex.pdf},
volume = {50},
year = {1982}
}
@article{Pritchard1999a,
abstract = {We use variation at a set of eight human Y chromosome microsatellite loci to investigate the demographic history of the Y chromosome. Instead of assuming a population of constant size, as in most of the previous work on the Y chromosome, we consider a model which permits a period of recent population growth. We show that for most of the populations in our sample this model fits the data far better than a model with no growth. We estimate the demographic parameters of this model for each population and also the time to the most recent common ancestor. Since there is some uncertainty about the details of the microsatellite mutation process, we consider several plausible mutation schemes and estimate the variance in mutation size simultaneously with the demographic parameters of interest. Our finding of a recent common ancestor (probably in the last 120,000 years), coupled with a strong signal of demographic expansion in all populations, suggests either a recent human expansion from a small ancestral population, or natural selection acting on the Y chromosome.},
author = {Pritchard, J. K. and Seielstad, M. T. and Perez-Lezaun, A. and Feldman, M. W.},
doi = {10.1093/oxfordjournals.molbev.a026091},
isbn = {0737-4038},
issn = {0737-4038},
journal = {Molecular Biology and Evolution},
number = {12},
pages = {1791--1798},
pmid = {10605120},
title = {{Population growth of human Y chromosomes: a study of Y chromosome microsatellites}},
url = {https://academic.oup.com/mbe/article-lookup/doi/10.1093/oxfordjournals.molbev.a026091},
volume = {16},
year = {1999}
}
@article{Sisson2007,
abstract = {Recent new methods in Bayesian simulation have provided ways of evaluating posterior distributions in the presence of analytically or computationally intractable likelihood functions. Despite representing a substantial methodological advance, existing methods based on rejection sampling or Markov chain Monte Carlo can be highly inefficient and accordingly require far more iterations than may be practical to implement. Here we propose a sequential Monte Carlo sampler that convincingly overcomes these inefficiencies. We demonstrate its implementation through an epidemiological study of the transmission rate of tuberculosis.},
author = {Sisson, S A and Fan, Y and Tanaka, Mark M},
doi = {10.1073/pnas.0607208104},
isbn = {0607208104},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {6},
pages = {1760--5},
pmid = {17264216},
title = {{Sequential Monte Carlo without likelihoods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17264216{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1794282},
volume = {104},
year = {2007}
}
@article{Ishida2015,
abstract = {Approximate Bayesian Computation (ABC) enables parameter inference for complex physical systems in cases where the true likelihood function is unknown, unavailable, or computationally too expensive. It relies on the forward simulation of mock data and comparison between observed and synthetic catalogues. Here we present cosmoabc, a Python ABC sampler featuring a Population Monte Carlo variation of the original ABC algorithm, which uses an adaptive importance sampling scheme. The code is very flexible and can be easily coupled to an external simulator, while allowing to incorporate arbitrary distance and prior functions. As an example of practical application, we coupled cosmoabc with the numcosmo library and demonstrate how it can be used to estimate posterior probability distributions over cosmological parameters based on measurements of galaxy clusters number counts without computing the likelihood function. cosmoabc is published under the GPLv3 license on PyPI and GitHub and documentation is available at http://goo.gl/SmB8EX.},
archivePrefix = {arXiv},
arxivId = {1504.06129},
author = {Ishida, E. E.O. and Vitenti, S. D.P. and Penna-Lima, M. and Cisewski, J. and de Souza, R. S. and Trindade, A. M.M. and Cameron, E. and Busti, V. C.},
doi = {10.1016/j.ascom.2015.09.001},
eprint = {1504.06129},
isbn = {1553-7358},
issn = {22131337},
journal = {Astronomy and Computing},
keywords = {(cosmology:) large-scale structure of universe,Galaxies: statistics},
pages = {1--11},
title = {{Cosmoabc: Likelihood-free inference via Population Monte Carlo Approximate Bayesian Computation}},
volume = {13},
year = {2015}
}
@article{Shen2012,
abstract = {A non-linear Bayesian Monte-Carlo method is presented to estimate a Vsv model beneath stations by jointly interpreting Rayleigh wave dispersion and receiver functions and associated uncertainties. The method is designed for automated application to large arrays of broad-band seismometers. As a testbed for the method, 185 stations from the USArray Transportable Array are used in the Intermountain West, a region that is geologically diverse and structurally complex. Ambient noise and earthquake tomography are updated by applying eikonal and Helmholtz tomography, respectively, to construct Rayleigh wave dispersion maps from 8 to 80 s across the study region with attendant uncertainty estimates. A method referred to as  harmonic stripping method' is described and applied as a basis for quality control and to generate backazimuth independent receiver functions for a horizontally layered, isotropic effective medium with uncertainty estimates for each station. A smooth parametrization between (as well as above and below) discontinuities at the base of the sediments and crust suffices to fit most features of both data types jointly across most of the study region. The effect of introducing receiver functions to surface wave dispersion data is quantified through improvements in the posterior marginal distribution of model variables. Assimilation of receiver functions quantitatively improves the accuracy of estimates of Moho depth, improves the determination of the Vsv contrast across Moho, and improves uppermost mantle structure because of the ability to relax a priori constraints. The method presented here is robust and can be applied systematically to construct a 3-D model of the crust and uppermost mantle across the large networks of seismometers that are developing globally, but also provides a framework for further refinements in the method.},
author = {Shen, W. and Ritzwoller, M. H. and Schulte-Pelkum, V. and Lin, F.-C.},
doi = {10.1093/gji/ggs050},
isbn = {0956-540X},
issn = {0956-540X},
journal = {Geophysical Journal International},
keywords = {1 i n t,and uppermost mantle velocity,crustal,inverse theory,models,north america,oscillations,ro d u c,seismic tomography,structure,surface waves and free,t i o n,the construction of crustal},
title = {{Joint inversion of surface wave dispersion and receiver functions: a Bayesian Monte-Carlo approach}},
year = {2012}
}
@article{Marjoram2003,
abstract = {Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion.},
author = {Marjoram, Paul and Molitor, John and Plagnol, Vincent and Tavare, Simon},
doi = {10.1073/pnas.0306899100},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Algorithms,Biological,Biological Evolution,Computer Simulation,DNA,DNA: genetics,Genetics,Humans,Likelihood Functions,Markov Chains,Mitochondrial,Mitochondrial: genetics,Models,Monte Carlo Method,Population,Stochastic Processes},
number = {26},
pages = {15324--8},
pmid = {14663152},
title = {{Markov chain Monte Carlo without likelihoods.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=307566{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {100},
year = {2003}
}
@article{afonso2013b,
author = {Afonso, J C and Fullea, J and Yang, Y and Connolly, J A D and Jones, A G},
journal = {Journal of Geophysical Research: Solid Earth},
number = {4},
pages = {1650--1676},
publisher = {Wiley Online Library},
title = {{3-D multi-observable probabilistic inversion for the compositional and thermal structure of the lithosphere and upper mantle. II: General methodology and resolution analysis}},
volume = {118},
year = {2013}
}
@book{gregory2005bayesian,
author = {Gregory, Phil},
publisher = {Cambridge University Press},
title = {{Bayesian Logical Data Analysis for the Physical Sciences: A Comparative Approach with Mathematica{\textregistered}Support}},
year = {2005}
}
@book{Aster2013,
abstract = {Parameter Estimation and Inverse Problems, 2e provides geoscience students and professionals with answers to common questions like how one can derive a physical model from a finite set of observations containing errors, and how one may determine the quality of such a model. This book takes on these fundamental and challenging problems, introducing students and professionals to the broad range of approaches that lie in the realm of inverse theory. The authors present both the underlying theory and practical algorithms for solving inverse problems. The authors' treatment is appropriate for geoscience graduate students and advanced undergraduates with a basic working knowledge of calculus, linear algebra, and statistics. Parameter Estimation and Inverse Problems, 2e introduces readers to both Classical and Bayesian approaches to linear and nonlinear problems with particular attention paid to computational, mathematical, and statistical issues related to their application to geophysical problems. The textbook includes Appendices covering essential linear algebra, statistics, and notation in the context of the subject. A companion website features computational examples (including all examples contained in the textbook) and useful subroutines using MATLAB. Includes appendices for review of needed concepts in linear, statistics, and vector calculus. Companion website contains comprehensive MATLAB code for all examples, which readers can reproduce, experiment with, and modify. Online instructor's guide helps professors teach, customize exercises, and select homework problems Accessible to students and professionals without a highly specialized mathematical background. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
author = {Aster, Richard C. and Borchers, Brian and Thurber, Clifford H.},
booktitle = {Parameter Estimation and Inverse Problems},
doi = {10.1016/C2009-0-61134-X},
isbn = {9780123850485},
issn = {14337851},
pmid = {23138097},
title = {{Parameter Estimation and Inverse Problems}},
year = {2013}
}
@article{Sunnaker2013,
abstract = {Approximate Bayesian computation (ABC) constitutes a class of computational methods rooted in Bayesian statistics. In all model-based statistical inference, the likelihood function is of central importance, since it expresses the probability of the observed data under a particular statistical model, and thus quantifies the support data lend to particular values of parameters and to choices among different models. For simple models, an analytical formula for the likelihood function can typically be derived. However, for more complex models, an analytical formula might be elusive or the likelihood function might be computationally very costly to evaluate. ABC methods bypass the evaluation of the likelihood function. In this way, ABC methods widen the realm of models for which statistical inference can be considered. ABC methods are mathematically well-founded, but they inevitably make assumptions and approximations whose impact needs to be carefully assessed. Furthermore, the wider application domain of ABC exacerbates the challenges of parameter estimation and model selection. ABC has rapidly gained popularity over the last years and in particular for the analysis of complex problems arising in biological sciences (e.g., in population genetics, ecology, epidemiology, and systems biology).},
archivePrefix = {arXiv},
arxivId = {0811.3355},
author = {Sunn{\aa}ker, Mikael and Busetto, Alberto Giovanni and Numminen, Elina and Corander, Jukka and Foll, Matthieu and Dessimoz, Christophe},
doi = {10.1371/journal.pcbi.1002803},
eprint = {0811.3355},
isbn = {1553-7358 (Electronic)$\backslash$r1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {1},
pmid = {23341757},
title = {{Approximate Bayesian Computation}},
volume = {9},
year = {2013}
}
@article{Ratmann2009,
abstract = {Mathematical models are an important tool to explain and comprehend complex phenomena, and unparalleled computational advances enable us to easily explore them without any or little understanding of their global properties. In fact, the likelihood of the data under complex stochastic models is often analytically or numerically intractable in many areas of sciences. This makes it even more important to simultaneously investigate the adequacy of these models-in absolute terms, against the data, rather than relative to the performance of other models-but no such procedure has been formally discussed when the likelihood is intractable. We provide a statistical interpretation to current developments in likelihood-free Bayesian inference that explicitly accounts for discrepancies between the model and the data, termed Approximate Bayesian Computation under model uncertainty (ABCmicro). We augment the likelihood of the data with unknown error terms that correspond to freely chosen checking functions, and provide Monte Carlo strategies for sampling from the associated joint posterior distribution without the need of evaluating the likelihood. We discuss the benefit of incorporating model diagnostics within an ABC framework, and demonstrate how this method diagnoses model mismatch and guides model refinement by contrasting three qualitative models of protein network evolution to the protein interaction datasets of Helicobacter pylori and Treponema pallidum. Our results make a number of model deficiencies explicit, and suggest that the T. pallidum network topology is inconsistent with evolution dominated by link turnover or lateral gene transfer alone.},
author = {Ratmann, O. and Andrieu, C. and Wiuf, C. and Richardson, S.},
doi = {10.1073/pnas.0807882106},
isbn = {1091-6490},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {26},
pages = {10576--10581},
pmid = {19525398},
title = {{Model criticism based on likelihood-free inference, with an application to protein network evolution}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0807882106},
volume = {106},
year = {2009}
}
@article{Sadegh2015,
abstract = {Many watershed models used within the hydrologic research community assume (by default) stationary conditions, that is, the key watershed properties that control water flow are considered to be time invariant. This assumption is rather convenient and pragmatic and opens up the wide arsenal of (multivariate) statistical and nonlinear optimization methods for inference of the (temporally fixed) model parameters. Several contributions to the hydrologic literature have brought into question the continued usefulness of this stationary paradigm for hydrologic modeling. This paper builds on the likelihood-free diagnostics approach of Vrugt and Sadegh (2013) and uses a diverse set of hydrologic summary metrics to test the stationary hypothesis and detect changes in the watersheds response to hydroclimatic forcing. Models with fixed parameter values cannot simulate adequately temporal variations in the summary statistics of the observed catchment data, and consequently, the DREAM(ABC) algorithm cannot find solutions that sufficiently honor the observed metrics. We demonstrate that the presented methodology is able to differentiate successfully between watersheds that are classified as stationary and those that have undergone significant changes in land use, urbanization, and/or hydroclimatic conditions, and thus are deemed nonstationary.},
archivePrefix = {arXiv},
arxivId = {10.1002/2014WR016527},
author = {Sadegh, Mojtaba and Vrugt, Jasper A. and Xu, Chonggang and Volpi, Elena},
doi = {10.1002/2014WR016805},
eprint = {2014WR016527},
isbn = {1944-7973},
issn = {19447973},
journal = {Water Resources Research},
keywords = {Nonstationarity,approximate Bayesian computation,process-based model evaluation},
number = {11},
pages = {9207--9231},
pmid = {1000287943},
primaryClass = {10.1002},
title = {{The stationarity paradigm revisited: Hypothesis testing using diagnostics, summary metrics, and DREAM(ABC)}},
volume = {51},
year = {2015}
}
@article{meeds2015hamiltonian,
author = {Meeds, Edward and Leenders, Robert and Welling, Max},
journal = {arXiv preprint arXiv:1503.01916},
title = {{Hamiltonian ABC}},
year = {2015}
}
@article{raynal2017abc,
author = {Raynal, Louis and Marin, Jean-Michel and Pudlo, Pierre and Ribatet, Mathieu and Robert, Christian P and Estoup, Arnaud},
journal = {arXiv preprint arXiv:1605.05537},
title = {{ABC random forests for Bayesian parameter inference}},
year = {2017}
}
@book{Cox2007,
abstract = {In this chapter, we discuss the fundamental principles behind two of the most frequently used statistical inference procedures: confidence interval estimation and hypothesis testing, both procedures are constructed on the sampling distributions that we have learned in previous chapters. To better understand these inference procedures, we focus on the logic of statistical decision making and the role that experimental data play in the decision process. Numerical examples are used to illustrate the implementation of the discussed procedures. This chapter also introduces some of the most important concepts associated with confidence interval estimation and hypothesis testing, including P values, significance level, power, sample size, and two types of errors. We conclude the chapter with a brief discussion on statistical and practical significance of test results.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cox, D.R.},
booktitle = {Methods in molecular biology (Clifton, N.J.)},
doi = {10.1007/978-1-59745-530-5_4},
eprint = {arXiv:1011.1669v3},
isbn = {9780521866736},
issn = {1064-3745},
keywords = {Brain,Brain: blood,Confidence Intervals,Data Interpretation,Heart Failure,Heart Failure: blood,Heart Failure: pathology,Humans,Male,Natriuretic Peptide,Prostate-Specific Antigen,Prostate-Specific Antigen: blood,Prostatic Neoplasms,Prostatic Neoplasms: blood,Prostatic Neoplasms: surgery,Statistical},
pmid = {18450045},
title = {{Principles of statistical inference.}},
year = {2007}
}
@book{Biegler2010,
abstract = {Strong style="mso-bidi-font-weight: normal;"Large-Scale Inverse Problems and Quantification of Uncertainty strong style="mso-bidi-font-weight: normal;"Editors strong style="mso-bidi-font-weight: normal;"Lorenz Biegler, Carnegie Mellon University, USA strong style="mso-bidi-font-weight: normal;"George Biros, Georgia Institute of Technology, USA strong style="mso-bidi-font-weight: normal;"Omar Ghattas, University of Texas at Austin, USA strong style="mso-bidi-font-weight: normal;"Matthias Heinkenschloss, Rice University, USA strong style="mso-bidi-font-weight: normal;"David Keyes, KAUST and Columbia University, USA strong style="mso-bidi-font-weight: normal;"Bani Mallick, Texas A$\backslash${\&}M University, USA strong style="mso-bidi-font-weight: normal;"Luis Tenorio, Colorado School of Mines, USA strong style="mso-bidi-font-weight: normal;"Bart van Bloemen Waanders, Sandia National Laboratories, USA strong style="mso-bidi-font-weight: normal;"Karen Wilcox, Massachusetts Institute of Technology, USA strong style="mso-bidi-font-weight: normal;"Youssef Marzouk, Massachusetts Institute of Technology, USA strong style="mso-bidi-font-weight: normal;" This book focuses on computational methods for large-scale statistical inverse problems and provides an introduction to statistical Bayesian and frequentist methodologies. Recent research advances for approximation methods are discussed, along with Kalman filtering methods and optimization-based approaches to solving inverse problems. The aim is to cross-fertilize the perspectives of researchers in the areas of data assimilation, statistics, large-scale optimization, applied and computational mathematics, high performance computing, and cutting-edge applications. The solution to large-scale inverse problems critically depends on methods to reduce computational cost. Recent research approaches tackle this challenge in a variety of different ways. Many of the computational frameworks highlighted in this book build upon state-of-the-art methods for simulation of the forward problem, such as, fast Partial Differential Equation (PDE) solvers, reduced-order models and emulators of the forward problem, stochastic spectral approximations, and ensemble-based approximations, as well as exploiting the machinery for large-scale deterministic optimization through adjoint and other sensitivity analysis methods. Key Features: Brings together the perspectives of researchers in areas of inverse problems and data assimilation. Assesses the current state-of-the-art and identify needs and opportunities for future research. Focuses on the computational methods used to analyze and simulate inverse problems. Written by leading experts of inverse problems and uncertainty quantification. Graduate students and researchers working in statistics, mathematics and engineering will benefit from this book.},
author = {Biegler, Lorenz and Biros, George and Ghattas, Omar and Heinkenschloss, Matthias and Keyes, David and Mallick, Bani and Marzouk, Youssef and Tenorio, Luis and {van Bloemen Waanders}, Bart and Willcox, Karen},
booktitle = {Large-Scale Inverse Problems and Quantification of Uncertainty},
doi = {10.1002/9780470685853},
isbn = {9780470697436},
pmid = {16326963},
title = {{Large-Scale Inverse Problems and Quantification of Uncertainty}},
year = {2010}
}
@article{Shapiro2002,
abstract = {We describe a method to invert surface wave dispersion data for a model of shear velocities with uncertainties in the crust and uppermost mantle. The inversion is a multistep process, constrained by a priori information, that culminates in a Markov-chain Monte-Carlo sampling of model space to yield an ensemble of acceptable models at each spatial node. The model is radially anisotropic in the uppermost mantle to an average depth of about 200 km and is isotropic elsewhere. The method is applied on a 2°× 2° grid globally to a large data set of fundamental mode surface wave group and phase velocities (Rayleigh group velocity, 16–200 s; Love group velocity, 16–150 s; Rayleigh and Love phase velocity, 40–150 s). The middle of the ensemble (Median Model) defines the estimated model and the half-width of the corridor of models provides the uncertainty estimate. Uncertainty estimates allow the identification of the robust features of the model which, typically, persist only to depths of ∼250 km. We refer to the features that appear in every member of the ensemble of acceptable models as ‘persistent'. Persistent features include sharper images of the variation of oceanic lithosphere and asthenosphere with age, continental roots, extensional tectonic features in the upper mantle, the shallow parts of subducted lithosphere, and improved resolution of radial anisotropy. In particular, we find no compelling evidence for ‘negative anisotropy' anywhere in the world's lithosphere.},
author = {Shapiro, N. M. and Ritzwoller, M. H.},
doi = {10.1046/j.1365-246X.2002.01742.x},
isbn = {1365-246X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Lithosphere,Monte-Carlo inversion,Shear velocity},
pmid = {178347900006},
title = {{Monte-Carlo inversion for a global shear-velocity model of the crust and upper mantle}},
year = {2002}
}
@article{Tavare1997,
author = {Tavare, S and Balding, D J and Griffiths, R C and Donnelly, P},
issn = {0016-6731},
journal = {Genetics},
number = {2},
pages = {505--518},
title = {{Inferring Coalescence Times from DNA Sequence Data}},
type = {Journal Article},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1207814/},
volume = {145},
year = {1997}
}
@book{Box1978,
abstract = {Introduces the philosophy of experimentation and the part that statistics play in experimentation. Emphasizes the need to develop a capability for ``statistical thinking'' by using examples drawn from actual case studies. TS - RIS},
author = {Box, G E P and Hunter, W J and Hunter, J S},
booktitle = {Wiley Series in Probability and Mathematical Statistics},
isbn = {0471093157},
pmid = {10317990},
title = {{Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building}},
year = {1978}
}
@article{vrugt2013toward,
author = {Vrugt, Jasper A and Sadegh, Mojtaba},
journal = {Water Resources Research},
number = {7},
pages = {4335--4345},
publisher = {Wiley Online Library},
title = {{Toward diagnostic model calibration and evaluation: Approximate Bayesian computation}},
volume = {49},
year = {2013}
}
@article{gutmann2016bayesian,
author = {Gutmann, Michael U and Corander, Jukka and Others},
journal = {Journal of Machine Learning Research},
publisher = {MICROTOME PUBL},
title = {{Bayesian optimization for likelihood-free inference of simulator-based statistical models}},
year = {2016}
}
@book{Tarantola2005,
author = {Tarantola, Albert},
isbn = {0898715725},
publisher = {SIAM},
title = {{Inverse problem theory and methods for model parameter estimation}},
type = {Book},
year = {2005}
}
@book{Kaipio2006,
author = {Kaipio, Jari and Somersalo, Erkki},
publisher = {Springer Science {\&} Business Media},
title = {{Statistical and computational inverse problems}},
year = {2006}
}
@article{Mosegaard1995,
abstract = {Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space. This probability distribution combines a priori informa- tion with new information obtained by measuring some observable parameters (data). As, in the general case, the theory linking data with model parameters is nonlinear, the a posteriori probability in the model space may not be easy to describe (it may be multimodal, some moments may not be defined, etc.). When analyzing an inverse problem, obtaining a maximum likelihood model is usu- ally not sufficient, as we normally also wish to have infor- mation on the resolution power of the data. In the general case we may have a large number of model parameters, and an inspection of the marginal probability densities of interest may be impractical, or even useless. But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distri- bution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator. This can be ac- complished by means of an efficient Monte Carlo method, even in cases where no explicit formula for the a priori distribution is available. The most well known impor- tance sampling method, the Metropolis algorithm, can be generalized, and this gives a method that allows anal- ysis of (possibly highly nonlinear) inverse problems with complex a priori information and data with an arbitrary noise distribution.},
author = {Mosegaard, Klaus and Tarantola, Albert},
doi = {10.1029/94JB03097},
isbn = {0148-0227},
issn = {0148-0227},
journal = {Journal of Geophysical Research},
title = {{Monte Carlo sampling of solutions to inverse problems}},
year = {1995}
}
@article{Mosegaard2002,
author = {Mosegaard, Klaus and Tarantola, Albert},
issn = {0074-6142},
journal = {International Geophysics},
pages = {237--265},
title = {{Probabilistic approach to inverse problems}},
type = {Journal Article},
volume = {81},
year = {2002}
}
@book{Menke2012,
abstract = {Please use extracts from reviews of first edition Key Features * Updated and thoroughly revised edition * additional material on geophysical/acoustic tomography * Detailed discussion of application of inverse theory to tectonic, gravitational and geomagnetic studies. {\textcopyright} 2012 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Menke, William},
booktitle = {Geophysical Data Analysis: Discrete Inverse Theory},
doi = {10.1016/C2011-0-69765-0},
eprint = {arXiv:1011.1669v3},
isbn = {9780123971609},
issn = {01697161},
pmid = {25246403},
title = {{Geophysical Data Analysis: Discrete Inverse Theory}},
year = {2012}
}
@book{Idier2013,
author = {Idier, Jerome},
publisher = {John Wiley $\backslash${\&} Sons},
title = {{Bayesian approach to inverse problems}},
year = {2013}
}
@article{Gelman1996,
author = {Gelman, Andrew and Roberts, Gareth},
journal = {Bayesian statistics},
pages = {599--608},
title = {{Efficient Metropolis jumping rules}},
volume = {5},
year = {1996}
}
@article{Blum2013,
abstract = {Approximate Bayesian computation (ABC) methods make use of comparisons between simulated and observed summary statistics to over-come the problem of computationally intractable likelihood functions. As the practical implementation of ABC requires computations based on vectors of summary statistics, rather than full data sets, a central question is how to derive low-dimensional summary statistics from the observed data with min-imal loss of information. In this article we provide a comprehensive review and comparison of the performance of the principal methods of dimension reduction proposed in the ABC literature. The methods are split into three nonmutually exclusive classes consisting of best subset selection methods, projection techniques and regularization. In addition, we introduce two new methods of dimension reduction. The first is a best subset selection method based on Akaike and Bayesian information criteria, and the second uses ridge regression as a regularization procedure. We illustrate the performance of these dimension reduction techniques through the analysis of three challeng-ing models and data sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.3819v3},
author = {Blum, M. G. B. and Nunes, M. A. and Prangle, D. and Sisson, S. A.},
doi = {10.1214/12-STS406},
eprint = {arXiv:1202.3819v3},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
number = {2},
pages = {189--208},
title = {{A Comparative Review of Dimension Reduction Methods in Approximate Bayesian Computation}},
url = {http://projecteuclid.org/euclid.ss/1369147911},
volume = {28},
year = {2013}
}
@article{Blum2010,
abstract = {Approximate Bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood is either mathematically or computa-tionally intractable. However the methods that use rejection suffer from the curse of dimensionality when the number of summary statistics is increased. Here we propose a machine-learning approach to the estimation of the posterior density by introducing two innovations. The new method fits a non-linear conditional heteroscedastic regression of the parame-ter on the summary statistics, and then adaptively improves estimation using importance sampling. The new algorithm is compared to the state-of-the-art approximate Bayesian methods, and achieves considerable reduction of the com-putational burden in two examples of inference in statistical genetics and in a queueing model.},
archivePrefix = {arXiv},
arxivId = {arXiv:0809.4178v2},
author = {Blum, Michael G B and Fran{\c{c}}ois, Olivier and Blum, M G B and Fran{\c{c}}ois, O},
doi = {10.1007/s11222-009-9116-0},
eprint = {arXiv:0809.4178v2},
isbn = {0960-3174$\backslash$n1573-1375},
issn = {09603174},
journal = {Stat Comput},
keywords = {Approximate,Bayesian computation {\textperiodcentered},Coalescent models {\textperiodcentered},Conditional density estimation {\textperiodcentered},Curse of dimensionality {\textperiodcentered},Feed forward neural networks {\textperiodcentered},Heteroscedasticity {\textperiodcentered},Implicit statistical models {\textperiodcentered},Importance sampling {\textperiodcentered},Indirect inference,Likelihood-free inference {\textperiodcentered},Non-linear regression {\textperiodcentered}},
pages = {63--73},
pmid = {273403900006},
title = {{Non-linear regression models for Approximate Bayesian Computation}},
url = {http://dx.doi.org/10.1007/s11222-009-9116-0},
volume = {20},
year = {2010}
}
@article{Ratmann2010,
abstract = {In their letter to PNAS and a comprehensive set of notes on arXiv [arXiv:0909.5673v2], Christian Robert, Kerrie Mengersen and Carla Chen (RMC) represent our approach to model criticism in situations when the likelihood cannot be computed as a way to "contrast several models with each other". In addition, RMC argue that model assessment with Approximate Bayesian Computation under model uncertainty (ABCmu) is unduly challenging and question its Bayesian foundations. We disagree, and clarify that ABCmu is a probabilistically sound and powerful too for criticizing a model against aspects of the observed data, and discuss further the utility of ABCmu.},
archivePrefix = {arXiv},
arxivId = {0912.3182},
author = {Ratmann, O. and Andrieu, C. and Wiuf, C. and Richardson, S.},
doi = {10.1073/pnas.0912887107},
eprint = {0912.3182},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {3},
pages = {E6--E7},
title = {{Reply to Robert et al.: Model criticism informs model choice and model comparison}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0912887107},
volume = {107},
year = {2010}
}
@article{Moorkamp2010,
abstract = {We present joint inversion of magnetotelluric, receiver function, and Raleigh wave dispersion data for a one-dimensional Earth using a multiobjective genetic algorithm (GA). The chosen GA produces not only a family of models that fit the data sets but also the trade-off between fitting the different data sets. The analysis of this trade-off gives insight into the compatibility between the seismic data sets and the magnetotelluric data and also the appropriate noise level to assume for the seismic data. This additional information helps to assess the validity of the joint model, and we demonstrate the use of our approach with synthetic data under realistic conditions. We apply our method to one site from the Slave Craton and one site from the Kaapvaal Craton. For the Slave Craton we obtain similar results to our previously published models from joint inversion of receiver functions and magnetotelluric data but with improved resolution and control on absolute velocities. We find a conductive layer at the bottom of the crust, just above the Moho; a low-velocity, low-resistivity zone in the lithospheric mantle, previously termed the Central Slave Mantle Conductor; and indications of the lithosphere-asthenosphere boundary in terms of a decrease in seismic velocity and resistivity. For the Kaapvaal Craton both the seismic and the MT data are of lesser quality, which prevents as detailed and robust an interpretation; nevertheless, we find an indication of a low-velocity low-resistivity zone in the mantle lithosphere. These two examples demonstrate the potential of joint inversion, particularly in combination with nonlinear optimization methods.},
author = {Moorkamp, M. and Jones, A. G. and Fishwick, S.},
doi = {10.1029/2009JB006369},
issn = {21699356},
journal = {Journal of Geophysical Research: Solid Earth},
title = {{Joint inversion of receiver functions, surface wave dispersion, and magnetotelluric data}},
year = {2010}
}
@misc{Hartig2011,
abstract = {Statistical models are the traditional choice to test scientific theories when observations, processes or boundary conditions are subject to stochasticity. Many important systems in ecology and biology, however, are difficult to capture with statistical models. Stochastic simulation models offer an alternative, but they were hitherto associated with a major disadvantage: their likelihood functions can usually not be calculated explicitly, and thus it is difficult to couple them to well-established statistical theory such as maximum likelihood and Bayesian statistics. A number of new methods, among them Approximate Bayesian Computing and Pattern-Oriented Modelling, bypass this limitation. These methods share three main principles: aggregation of simulated and observed data via summary statistics, likelihood approximation based on the summary statistics, and efficient sampling. We discuss principles as well as advantages and caveats of these methods, and demonstrate their potential for integrating stochastic simulation models into a unified framework for statistical modelling.},
author = {Hartig, Florian and Calabrese, Justin M. and Reineking, Bj{\"{o}}rn and Wiegand, Thorsten and Huth, Andreas},
booktitle = {Ecology Letters},
doi = {10.1111/j.1461-0248.2011.01640.x},
isbn = {1461-0248},
issn = {1461023X},
keywords = {Bayesian statistics,Indirect inference,Intractable likelihood,Inverse modelling,Likelihood approximation,Likelihood-free inference,Maximum likelihood,Model selection,Parameter estimation,Stochastic simulation},
number = {8},
pages = {816--827},
pmid = {21679289},
title = {{Statistical inference for stochastic simulation models - theory and application}},
volume = {14},
year = {2011}
}
@article{Botev2010,
abstract = {We present a new adaptive kernel density estimator based on linear diffusion processes. The proposed estimator builds on existing ideas for adaptive smoothing by incorporating information from a pilot density estimate. In addition, we propose a new plug-in bandwidth selection method that is free from the arbitrary normal reference rules used by existing methods. We present simulation examples in which the proposed approach outperforms existing methods in terms of accuracy and reliability.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.2602v1},
author = {Botev, Z. I. and Grotowski, J. F. and Kroese, D. P.},
doi = {10.1214/10-AOS799},
eprint = {arXiv:1011.2602v1},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Langevin process,Nonparametric density estimation,bandwidth selection,boundary bias,data sharpening,diffusion equation,heat kernel,normal reference rules,variable bandwidth},
title = {{Kernel density estimation via diffusion}},
year = {2010}
}
@article{Sisson2010a,
abstract = {To appear to MCMC handbook, S. P. Brooks, A. Gelman, G. Jones and X.-L. Meng (eds), Chapman {\&} Hall.},
archivePrefix = {arXiv},
arxivId = {arXiv:1001.2058v1},
author = {Sisson, S a and Fan, Y},
eprint = {arXiv:1001.2058v1},
journal = {Handbook of Markov chain Monte Carlo},
number = {Mcmc},
title = {{Likelihood-free Markov chain Monte Carlo}},
url = {http://arxiv.org/abs/1001.2058},
year = {2010}
}
@article{Robert2010,
abstract = {The new perspectives on ABC and Bayesian model criticisms presented in Ratmann et al.(2009) are challenging standard approaches to Bayesian model choice. We discuss here some issues arising from the authors' approach, including prior influence, model assessment and criticism, and the meaning of error in ABC.},
archivePrefix = {arXiv},
arxivId = {0909.5673},
author = {Robert, C. P. and Mengersen, K. and Chen, C.},
doi = {10.1073/pnas.0911260107},
eprint = {0909.5673},
isbn = {1091-6490 (Electronic)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {3},
pages = {E5--E5},
pmid = {20080613},
title = {{Model choice versus model criticism}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.0911260107},
volume = {107},
year = {2010}
}
@article{Wood2010,
abstract = {Chaotic ecological dynamic systems defy conventional statistical analysis. Systems with near-chaotic dynamics are little better. Such systems are almost invariably driven by endogenous dynamic processes plus demographic and environmental process noise, and are only observable with error. Their sensitivity to history means that minute changes in the driving noise realization, or the system parameters, will cause drastic changes in the system trajectory. This sensitivity is inherited and amplified by the joint probability density of the observable data and the process noise, rendering it useless as the basis for obtaining measures of statistical fit. Because the joint density is the basis for the fit measures used by all conventional statistical methods, this is a major theoretical shortcoming. The inability to make well-founded statistical inferences about biological dynamic models in the chaotic and near-chaotic regimes, other than on an ad hoc basis, leaves dynamic theory without the methods of quantitative validation that are essential tools in the rest of biological science. Here I show that this impasse can be resolved in a simple and general manner, using a method that requires only the ability to simulate the observed data on a system from the dynamic model about which inferences are required. The raw data series are reduced to phase-insensitive summary statistics, quantifying local dynamic structure and the distribution of observations. Simulation is used to obtain the mean and the covariance matrix of the statistics, given model parameters, allowing the construction of a 'synthetic likelihood' that assesses model fit. This likelihood can be explored using a straightforward Markov chain Monte Carlo sampler, but one further post-processing step returns pure likelihood-based inference. I apply the method to establish the dynamic nature of the fluctuations in Nicholson's classic blowfly experiments.},
author = {Wood, Simon N.},
doi = {10.1038/nature09319},
isbn = {1476-4687},
issn = {00280836},
journal = {Nature},
number = {7310},
pages = {1102--1104},
pmid = {20703226},
title = {{Statistical inference for noisy nonlinear ecological dynamic systems}},
volume = {466},
year = {2010}
}
@article{Li2017,
abstract = {Approximate Bayesian computation (ABC) refers to a family of inference methods used in the Bayesian analysis of complex models where evaluation of the likelihood is difficult. Conventional ABC methods often suffer from the curse of dimensionality, and a marginal adjustment strategy was recently introduced in the literature to improve the performance of ABC algorithms in high-dimensional problems. The marginal adjustment approach is extended using a Gaussian copula approximation. The method first estimates the bivariate posterior for each pair of parameters separately using a 2-dimensional Gaussian copula, and then combines these estimates together to estimate the joint posterior. The approximation works well in large sample settings when the posterior is approximately normal, but also works well in many cases which are far from that situation due to the nonparametric estimation of the marginal posterior distributions. If each bivariate posterior distribution can be well estimated with a low-dimensional ABC analysis then this Gaussian copula method can extend ABC methods to problems of high dimension. The method also results in an analytic expression for the approximate posterior which is useful for many purposes such as approximation of the likelihood itself. This method is illustrated with several examples.},
archivePrefix = {arXiv},
arxivId = {1504.04093},
author = {Li, J. and Nott, D. J. and Fan, Y. and Sisson, S. A.},
doi = {10.1016/j.csda.2016.07.005},
eprint = {1504.04093},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximate Bayesian Computation (ABC),Gaussian copula,Likelihood free inference,Marginal adjustment,Regression adjustment ABC},
pages = {77--89},
title = {{Extending approximate Bayesian computation methods to high dimensions via a Gaussian copula model}},
volume = {106},
year = {2017}
}
@article{Weiss1998a,
abstract = {We introduce an approach to revealing the likelihood of different population histories that utilizes an explicit model of sequence evolution for the DNA segment under study. Based on a phylogenetic tree reconstruction method we show that a Tamura-Nei model with heterogeneous mutation rates is a fair description of the evolutionary process of the hypervariable region I of the mitochondrial DNA from humans. Assuming this complex model still allows the estimation of population history parameters, we suggest a likelihood approach to conducting statistical inference within a class of expansion models. More precisely, the likelihood of the data is based on the mean pairwise differences between DNA sequences and the number of variable sites in a sample. The use of likelihood ratios enables comparison of different hypotheses about population history, such as constant population size during the past or an increase or decrease of population size starting at some point back in time. This method was applied to show that the population of the Basques has expanded, whereas that of the Biaka pygmies is most likely decreasing. The Nuu-Chah-Nulth data are consistent with a model of constant population.},
author = {Weiss, G and von Haeseler, A},
isbn = {0016-6731 (Print)},
issn = {0016-6731},
journal = {Genetics},
number = {July},
pages = {1539--1546},
pmid = {9649540},
title = {{Inference of population history using a likelihood approach.}},
volume = {149},
year = {1998}
}
@article{Naesseth2017,
abstract = {Variational inference underlies many recent advances in large scale probabilistic modeling. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions; and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, variational sequential Monte Carlo (VSMC), and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. VSMC is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters.},
archivePrefix = {arXiv},
arxivId = {1705.11140},
author = {Naesseth, Christian A. and Linderman, Scott W. and Ranganath, Rajesh and Blei, David M.},
eprint = {1705.11140},
journal = {arXiv},
title = {{Variational Sequential Monte Carlo}},
url = {http://arxiv.org/abs/1705.11140},
year = {2017}
}
@article{Pudlo2015,
abstract = {Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques. We propose a novel approach based on a machine learning tool named random forests to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with random forests and postponing the approximation of the posterior probability of the predicted MAP for a second stage also relying on random forests. Compared with earlier implementations of ABC model choice, the ABC random forest approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least fifty), and (iv) it includes an approximation of the posterior probability of the selected model. The call to random forests will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. The proposed methodologies are implemented in the R package abcrf available on the CRAN.},
archivePrefix = {arXiv},
arxivId = {1406.6288},
author = {Pudlo, Pierre and Marin, Jean Michel and Estoup, Arnaud and Cornuet, Jean Marie and Gautier, Mathieu and Robert, Christian P.},
doi = {10.1093/bioinformatics/btv684},
eprint = {1406.6288},
issn = {14602059},
journal = {Bioinformatics},
number = {6},
pages = {859--866},
pmid = {26589278},
title = {{Reliable ABC model choice via random forests}},
volume = {32},
year = {2015}
}
@article{blum2017regression,
author = {Blum, Michael G B},
journal = {arXiv preprint arXiv:1707.01254},
title = {{Regression approaches for Approximate Bayesian Computation}},
year = {2017}
}
@article{Turek2017,
abstract = {Markov chain Monte Carlo (MCMC) sampling is an important and commonly used tool for the analysis of hierarchical models. Nevertheless, prac-titioners generally have two options for MCMC: utilize existing software that generates a black-box " one size fits all " algorithm, or the challenging (and time consuming) task of implementing a problem-specific MCMC algorithm. Either choice may result in inefficient sampling, and hence researchers have become ac-customed to MCMC runtimes on the order of days (or longer) for large models. We propose an automated procedure to determine an efficient MCMC block-sampling algorithm for a given model and computing platform. Our procedure dynami-cally determines blocks of parameters for joint sampling that result in efficient MCMC sampling of the entire model. We test this procedure using a diverse suite of example models, and observe non-trivial improvements in MCMC efficiency for many models. Our procedure is the first attempt at such, and may be generalized to a broader space of MCMC algorithms. Our results suggest that substantive improvements in MCMC efficiency may be practically realized using our auto-mated blocking procedure, or variants thereof, which warrants additional study and application.},
archivePrefix = {arXiv},
arxivId = {1503.05621},
author = {Turek, Daniel and de Valpine, Perry and Paciorek, Christopher J. and Anderson-Bergman, Clifford},
doi = {10.1214/16-BA1008},
eprint = {1503.05621},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Block sampling,Integrated autocorrelation time,MCMC,Metropolis-Hastings,Mixing,NIMBLE},
number = {2},
pages = {465--490},
title = {{Automated parameter blocking for efficient Markov chain monte carlo sampling}},
volume = {12},
year = {2017}
}
@book{Sprott2008,
author = {Sprott, David A.},
publisher = {Springer Science {\&} Business Media},
title = {{Statistical Inference in Science}},
year = {2008}
}
@article{Beaumont2009,
abstract = {Sequential techniques can enhance the efficiency of the approximate Bayesian computation algorithm, as in Sisson et al.'s (2007) partial rejection control version. While this method is based upon the theoretical works of Del Moral et al. (2006), the application to approximate Bayesian computation results in a bias in the approximation to the posterior. An alternative version based on genuine importance sampling arguments bypasses this difficulty, in connection with the population Monte Carlo method of Capp´e et al. (2004), and it includes an automatic scaling of the forward kernel. When applied to a population genetics example, it compares favourably with two other versions of the approximate algorithm.},
archivePrefix = {arXiv},
arxivId = {0805.2256},
author = {Beaumont, M. A. and Cornuet, J.-M. and Marin, J.-M. and Robert, C. P.},
doi = {10.1093/biomet/asp052},
eprint = {0805.2256},
isbn = {0006-3444},
issn = {0006-3444},
journal = {Biometrika},
number = {4},
pages = {983--990},
pmid = {272179100018},
title = {{Adaptive approximate Bayesian computation}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asp052},
volume = {96},
year = {2009}
}
@article{Haario2001,
abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
doi = {10.2307/3318737},
isbn = {1350-7265},
issn = {13507265},
journal = {Bernoulli},
keywords = {adaptive markov chain monte,carlo,comparison,convergence,ergodicity,hastings algorithm,markov chain,metropolis,monte carlo},
number = {2},
pages = {223},
pmid = {3318737},
title = {{An Adaptive Metropolis Algorithm}},
url = {http://www.jstor.org/stable/3318737?origin=crossref},
volume = {7},
year = {2001}
}
@article{khan2007joint,
author = {Khan, Amir and Connolly, J A D and Maclennan, J and Mosegaard, Klaus},
journal = {Geophysical Journal International},
number = {1},
pages = {243--258},
publisher = {Blackwell Publishing Ltd Oxford, UK},
title = {{Joint inversion of seismic and gravity data for lunar composition and thermal state}},
volume = {168},
year = {2007}
}
@book{Casella1993,
abstract = {This book builds theoretical statistics from the first principles of probability theory. Starting from the basics of probability, the authors develop the theory of statistical inference using techniques, definitions, and concepts that are statistical and are natural extensions and consequences of previous concepts. Intended for first-year graduate students, this book can be used for students majoring in statistics who have a solid mathematics background. It can also be used in a way that stresses the more practical uses of statistical theory, being more concerned with understanding basic statistical concepts and deriving reasonable statistical procedures for a variety of situations, and less concerned with formal optimality investigations.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Casella, G and Berger, G L},
booktitle = {Book},
doi = {10.1057/pt.2010.23},
eprint = {9605103},
isbn = {0-534-24312-6},
issn = {0307-4463},
pmid = {2359},
primaryClass = {cs},
title = {{Statistical inference}},
year = {1993}
}
@phdthesis{Laine2008,
abstract = {This work presents new, efficient Markov chain Monte Carlo (MCMC) simulation methods for statistical analysis in various modell ing applications. When using MCMC methods, the model is simulated repeatedly to explore the probability distrib u- tion describing the uncertainties in model parameters and predictions. In adaptive MCMC methods based on the Metropolis - Hastings algorithm, the proposal distribution needed by the algorithm learns from the target distribution as the simulation proceeds. Adaptive MCMC methods have been subject of intensive r e search lately, as they open a way for essentially easier use of the methodology. The lack o f user - friendly computer programs has been a main obstacle for wider acceptance of the methods. This work provides two new adaptive MCMC methods: DRAM and AARJ. The DRAM method has been built especially to work in high dimensional and non - linear problems. The AARJ method is an extension to DRAM for model sele c tion problems, where the mathematical formulation of the model is uncertain and we want simultaneously to fit several di f ferent models to the same observations. The methods were developed while keepi ng in mind the needs of modelling applications typical in environmental sciences. The development work has been pursued while working with several application projects. The applications presented in this work are: a winter time oxygen concentration model f or Lake Tuusulanj{\"{a}}rvi and adaptive control of the aerator; a nutrition model for Lake Pyh{\"{a}}j{\"{a}}rvi and lake management planning; validation of the algorithms of the GOMOS ozone r e mote sensing instrument on board the Envisat satellite of European Space Agency and the study of the effects of aerosol model selection on the GOMOS algorithm.},
author = {Laine, Marko},
school = {Lapeenranta University of Technology},
title = {{Adaptive MCMC Methods with Applications in Environmental and Geophysical Models}},
url = {https://www.doria.fi/bitstream/handle/10024/36631/isbn9789516976627.pdf},
year = {2008}
}
@incollection{Prangle2017,
abstract = {This document is due to appear as a chapter of the forthcoming Handbook of Approximate Bayesian Computation (ABC) edited by S. Sisson, Y. Fan, and M. Beaumont. Since the earliest work on ABC, it has been recognised that using summary statistics is essential to produce useful inference results. This is because ABC suffers from a curse of dimensionality effect, whereby using high dimensional inputs causes large approximation errors in the output. It is therefore crucial to find low dimensional summaries which are informative about the parameter inference or model choice task at hand. This chapter reviews the methods which have been proposed to select such summaries, extending the previous review paper of Blum et al. (2013) with recent developments. Related theoretical results on the ABC curse of dimensionality and sufficiency are also discussed.},
archivePrefix = {arXiv},
arxivId = {1512.05633},
author = {Prangle, Dennis},
booktitle = {Handbook of approximate Bayesian computation},
eprint = {1512.05633},
pages = {320},
title = {{Summary statistics in approximate Bayesian computation}},
url = {http://arxiv.org/abs/1512.05633},
year = {2017}
}
@article{Bodin2012,
abstract = {We present a novel method for joint inversion of receiver functions and surface wave dispersion data, using a transdimensional Bayesian formulation. This class of algorithm treats the number of model parameters (e.g. number of layers) as an unknown in the problem. The dimension of the model space is variable and a Markov chain Monte Carlo (McMC) scheme is used to provide a parsimonious solution that fully quantifies the degree of knowledge one has about seismic structure (i.e constraints on the model, resolution, and trade-offs). The level of data noise (i.e. the covariance matrix of data errors) effectively controls the information recoverable from the data and here it naturally determines the complexity of the model (i.e. the number of model parameters). However, it is often difficult to quantify the data noise appropriately, particularly in the case of seismic waveform inversion where data errors are correlated. Here we address the issue of noise estimation using an extended Hierarchical Bayesian formulation, which allows both the variance and covariance of data noise to be treated as unknowns in the inversion. In this way it is possible to let the data infer the appropriate level of data fit. In the context of joint inversions, assessment of uncertainty for different data types becomes crucial in the evaluation of the misfit function. We show that the Hierarchical Bayes procedure is a powerful tool in this situation, because it is able to evaluate the level of information brought by different data types in the misfit, thus removing the arbitrary choice of weighting factors. After illustrating the method with synthetic tests, a real data application is shown where teleseismic receiver functions and ambient noise surface wave dispersion measurements from the WOMBAT array (South-East Australia) are jointly inverted to provide a probabilistic 1D model of shear-wave velocity beneath a given station.},
author = {Bodin, T. and Sambridge, M. and Tkal{\'{c}}i{\'{c}}, H. and Arroucau, P. and Gallagher, K. and Rawlinson, N.},
doi = {10.1029/2011JB008560},
isbn = {2156-2202},
issn = {21699356},
journal = {Journal of Geophysical Research: Solid Earth},
title = {{Transdimensional inversion of receiver functions and surface wave dispersion}},
year = {2012}
}
@article{Zhao1996,
abstract = {The simulated annealing method has recently been applied to several multiparameter optimization problems, including those of geophysical inversion, A new variant of simulated annealing, called very fast simulated annealing (VFSA), overcomes some of the drawbacks of a conventional simulated annealing; it has been found to be a practical tool for geophysical inversion (Sen {\&} Stoffa 1995). The method is particularly useful for non-linear problems with multiple parameters where a grid-search method is prohibitively expensive. Here, we have applied VFSA to retrieve the crustal structure beneath seismic stations in Tibet using teleseismic body-waveform data. Our approach is to compare the radial-component records with generalized ray synthetics directly in the time domain. For any given crustal structure, we have formulated a direct relationship between the radial- and the vertical-component signal, so that we synthetic radial-component data from recorded vertical-component seismograms (Zhao {\&} Frohlich 1996). We have tested the VFSA inversion algorithm using synthetics and confirmed that it works very well, i.e. it finds solutions very close to the true solution. Using the algorithm, we have determined the crustal structures beneath 11 stations in Tibet. From south to north, the total crustal thickness is quite uniform. Our tectonic interpretation of these crustal models is that they may represent crust from the Eurasian plate injected beneath the crust of the converging Indian plate. Certain features of the models are consistent with the presence of a small convection cell or plume beneath north-central Tibet, as suggested by Molnar (1990).},
author = {Zhao, Lian She and Sen, Mrinal K. and Stoffa, Paul and Frohlich, Cliff},
doi = {10.1111/j.1365-246X.1996.tb00004.x},
isbn = {0956-540X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Crustal structure,Inversion,Tibet},
title = {{Application of very fast simulated annealing to the determination of the crustal structure beneath Tibet}},
year = {1996}
}
@article{Sadegh2014,
abstract = {Sadegh, M., and J. A. Vrugt (2014), Approximate Bayesian Computation using Markov Chain Monte Carlo simulation: DREAM(ABC), Water Resour. Res., 50, 6767–6787, doi:10.1002/ 2014WR015386. Received},
author = {Sadegh, M. and Vrugt, J. A.},
doi = {10.1002/2014WR015386.Received},
issn = {00431397},
journal = {Water Resources Research},
number = {2},
pages = {6767--6787},
title = {{Approximate Bayesian Computation using Markov Chain Monte Carlo simulation}},
volume = {10},
year = {2014}
}
@article{Leuenberger2010a,
abstract = {Until recently, the use of Bayesian inference was limited to a few cases because for many realistic probability models the likelihood function cannot be calculated analytically. The situation changed with the advent of likelihood-free inference algorithms, often subsumed under the term approximate Bayesian computation (ABC). A key innovation was the use of a postsampling regression adjustment, allowing larger tolerance values and as such shifting computation time to realistic orders of magnitude. Here we propose a reformulation of the regression adjustment in terms of a general linear model (GLM). This allows the integration into the sound theoretical framework of Bayesian statistics and the use of its methods, including model selection via Bayes factors. We then apply the proposed methodology to the question of population subdivision among western chimpanzees, Pan troglodytes verus.},
author = {Leuenberger, Christoph and Wegmann, Daniel},
doi = {10.1534/genetics.109.109058},
isbn = {0016-6731},
issn = {00166731},
journal = {Genetics},
number = {1},
pages = {243--252},
pmid = {19786619},
title = {{Bayesian computation and model selection without likelihoods}},
volume = {184},
year = {2010}
}
@book{Gilks1995,
author = {Gilks, Walter R and Richardson, Sylvia and Spiegelhalter, David},
isbn = {0412055511},
publisher = {CRC press},
title = {{Markov chain Monte Carlo in practice}},
type = {Book},
year = {1995}
}
@misc{sisson2016handbook,
author = {Sisson, S A and Fan, Y and Beaumont, M},
publisher = {Taylor {\&} Francis},
title = {{Handbook of Approximate Bayesian Computation}},
year = {2016}
}
@misc{Marjoram2006,
abstract = {An explosive growth is occurring in the quantity, quality and complexity of molecular variation data that are being collected. Historically, such data have been analysed by using model-based methods. Models are useful for sharpening intuition, for explanation and for prediction: they add to our understanding of how the data were formed, and they can provide quantitative answers to questions of interest. We outline some of these model-based approaches, including the coalescent, and discuss the applicability of the computational methods that are necessary given the highly complex nature of current and future data sets.},
author = {Marjoram, Paul and Tavar{\'{e}}, Simon},
booktitle = {Nature Reviews Genetics},
doi = {10.1038/nrg1961},
isbn = {1471-0056},
issn = {14710056},
number = {10},
pages = {759--770},
pmid = {16983372},
title = {{Modern computational approaches for analysing molecular genetic variation data}},
volume = {7},
year = {2006}
}
@article{Bortot2007,
abstract = {In the production of clean steels, the occurrence of imperfections - so-called "inclusions" - is unavoidable. The strength of a clean steel block is largely dependent on the size of the largest imperfection that it contains, so inference on extreme inclusion size forms an important part of quality control. Sampling is generally done by measuring imperfections on planar slices, leading to an extreme value version of a standard stereological problem: how to make inference on large inclusions using only the sliced observations. Under the assumption that inclusions are spherical, this problem has been tackled previously using a combination of extreme value models, stereological calculations, a Bayesian hierarchical model, and standard Markov chain Monte Carlo (MCMC) techniques. Our objectives in this article are twofold: (1) to assess the robustness of such inferences with respect to the assumption of spherical inclusions, and (2) to develop an inference procedure that is valid for nonspherical inclusions. We investigate both of these aspects by extending the spherical family for inclusion shapes to a family of ellipsoids. We then address the issue of robustness by assessing the performance of the spherical model when fitted to measurements obtained from a simulation of ellipsoidal inclusions. The issue of inference is more difficult, because likelihood calculation is not feasible for the ellipsoidal model. To handle this aspect, we propose a modification to a recently developed likelihood-free MCMC algorithm. After verifying the viability and accuracy of the proposed algorithm through a simulation study, we analyze a real inclusion dataset. comparing the inference obtained under the ellipsoidal inclusion model with that previously obtained assuming spherical inclusions. {\textcopyright} 2007 American Statistical Association.},
author = {Bortot, P. and Coles, S. G. and Sisson, S. A.},
doi = {10.1198/016214506000000988},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Approximate bayesian computation,Extreme value theory,Markov chain Monte Carlo,Simulated tempering,Steel inclusion,Stereology},
number = {477},
pages = {84--92},
title = {{Inference for stereological extremes}},
volume = {102},
year = {2007}
}
@article{Fu1997,
abstract = {We present a simple Monte Carlo method for estimating the age of the most recent common ancestor (MRCA) of a sample of DNA sequences. We show that Templeton's (1993) estimator of the age of the MRCA based on the maximum number of nucleotide differences between two sequences in a sample is inaccurate, and we demonstrate the new method by reanalyzing a sample of DNA sequences from human Y chromosomes and a sample of human Alu sequences.},
author = {Fu, Yun Xin and Li, Wen Hsiung},
doi = {10.1093/oxfordjournals.molbev.a025753},
isbn = {0737-4038},
issn = {07374038},
journal = {Molecular Biology and Evolution},
keywords = {Alu sequence,Y chromosome,age estimation,coalescent approach,common ancestor},
number = {2},
pages = {195--199},
pmid = {9029798},
title = {{Estimating the age of the common ancestor of a sample of DNA sequences}},
volume = {14},
year = {1997}
}
@article{Beaumont2002,
author = {Beaumont, Mark A and Zhang, Wenyang and Balding, David J},
journal = {Genetics},
number = {4},
pages = {2025--2035},
title = {{Approximate Bayesian Computation in Population Genetics}},
type = {Journal Article},
volume = {162},
year = {2002}
}
@book{Box1973,
abstract = {THE CLASSIC on Bayesian methods, has a great deal on using Bayesian posteriors for inference- this book will probably be very useful when I begin to make inferences-only problem is there may be another book that is more up to date and easier to use},
author = {Box, George E P and Tiao, George C},
booktitle = {Book},
doi = {10.1002/9781118033197},
isbn = {0471574287},
issn = {0040-1706},
keywords = {Bayesian Statistics,Hypothesis testing,Inference,Posterior,Prior,Probability,Statistical methods,Subjective probability distributions,Uncertainty,reference},
title = {{Bayesian Inference in Statistical Analysis}},
year = {1973}
}
@article{Mira2001,
abstract = {The class of Metropolis-Hastings algorithms can be modified by delaying the rejection of proposed moves. The new samplers are proved to perform better than the original ones in terms of asymptotic variance of the estimates on a sweep by sweep basis. The delaying rejection algorithms also allow some space for local adaptation of the proposal distribution. We give an iterative formula for the acceptance probability at the i-th iteration of the delaying process. A special case is discussed in detail: the delaying rejection algorithm with symmetric proposal distribution},
author = {Mira, Antonietta},
issn = {00261424},
journal = {Metron},
keywords = {Asymptotic variance,Markov chain Monte Carlo Methods,Metropolis-Hastings algorithm,Peskun ordering},
title = {{On Metropolis-Hastings algorithms with delayed rejection}},
year = {2001}
}
@article{afonso2016,
author = {Afonso, Juan Carlos and Rawlinson, Nicholas and Yang, Yingjie and Schutt, Derek L and Jones, Alan G and Fullea, Javier and Griffin, William L},
journal = {Journal of Geophysical Research: Solid Earth},
number = {10},
pages = {7337--7370},
publisher = {Wiley Online Library},
title = {{3-D multiobservable probabilistic inversion for the compositional and thermal structure of the lithosphere and upper mantle: III. Thermochemical tomography in the Western-Central US}},
volume = {121},
year = {2016}
}
@article{afonso2013a,
author = {Afonso, J C and Fullea, J and Griffin, W L and Yang, Y and Jones, A G and {D Connolly}, J A and O'Reilly, S Y},
journal = {Journal of Geophysical Research: Solid Earth},
number = {5},
pages = {2586--2617},
publisher = {Wiley Online Library},
title = {{3-D multiobservable probabilistic inversion for the compositional and thermal structure of the lithosphere and upper mantle. I: A priori petrological information and geophysical observables}},
volume = {118},
year = {2013}
}
@article{Lintusaari2016,
abstract = {—Bayesian inference plays an important role in phylogenetics, evolutionary biology, and in many other branches of science. It provides a principled framework for dealing with uncertainty and quantifying how it changes in the light of new evidence. For many complex models and inference problems, however, only approximate quantitative answers are obtainable. Approximate Bayesian computation (ABC) refers to a family of algorithms for approximate inference that makes a minimal set of assumptions by only requiring that sampling from a model is possible. We explain here the fundamentals of ABC, review the classical algorithms, and highlight recent developments.},
author = {Lintusaari, Jarno and Gutmann, Michael U and Dutta, Ritabrata and Kaski, Samuel and Corander, Jukka},
doi = {10.1093/sysbio/syw077},
issn = {1063-5157},
journal = {Syst. Biol},
keywords = {Bayesian inference,[ABC,approximate Bayesian computation,likelihood-free inference,phylogenetics,simulator-based models,stochastic simulation models,tree-based models]},
number = {0},
pages = {1--17},
pmid = {27798401},
title = {{Fundamentals and Recent Developments in Approximate Bayesian Computation}},
volume = {00},
year = {2016}
}
@article{Trampert2004,
abstract = {We obtained likelihoods in the lower mantle for long-wavelength models of bulk sound and shear wave speed, density, and boundary topography, compatible with gravity constraints, from normal mode splitting functions and surface wave data. Taking into account the large uncertainties in Earth's thermodynamic reference state and the published range of mineral physics data, we converted the tomographic likelihoods into probability density functions for temperature, perovskite, and iron variations. Temperature and composition can be separated, showing that chemical variations contribute to the overall buoyancy and are dominant in the lower 1000 kilometers of the mantle.},
author = {Trampert, Jeannot and Deschamps, Fr{\'{e}}d{\'{e}}ric and Resovsky, Joseph and Yuen, Dave},
doi = {10.1126/science.1101996},
isbn = {0036-8075},
issn = {00368075},
journal = {Science},
pmid = {15514153},
title = {{Probabilistic tomography maps chemical heterogeneities throughout the lower mantle}},
year = {2004}
}
@misc{Stark2018,
abstract = {Instead of arguing about whether results hold up, let's push to provide enough information for others to repeat the experiments, says Philip Stark},
author = {Stark, Philip B.},
booktitle = {Nature},
doi = {10.1038/d41586-018-05256-0},
issn = {14764687},
keywords = {Publishing,Research data,Research management},
pmid = {29795524},
title = {{Before reproducibility must come preproducibility world-view}},
year = {2018}
}
@article{Sambridge1999,
abstract = {Monte Carlo direct search methods, such as genetic algorithms, simulated annealing, etc., are often used to explore a finite-dimensional parameter space. They require the solving of the forward problem many times, that is, making predictions of observables from an earth model. The resulting ensemble of earth models represents all ‘information' collected in the search process. Search techniques have been the subject of much study in geophysics; less attention is given to the appraisal of the ensemble. Often inferences are based on only a small subset of the ensemble, and sometimes a single member. This paper presents a new approach to the appraisal problem. To our knowledge this is the first time the general case has been addressed, that is, how to infer information from a complete ensemble, previously generated by any search method. The essence of the new approach is to use the information in the available ensemble to guide a resampling of the parameter space. This requires no further solving of the forward problem, but from the new ‘resampled' ensemble we are able to obtain measures of resolution and trade-off in the model parameters, or any combinations of them. The new ensemble inference algorithm is illustrated on a highly non-linear wave-form inversion problem. It is shown how the computation time and memory requirements scale with the dimension of the parameter space and size of the ensemble. The method is highly parallel, and may easily be distributed across several computers. Since little is assumed about the initial ensemble of earth models, the technique is applicable to a wide variety of situations. For example, it may be applied to perform ‘error analysis' using the ensemble generated by a genetic algorithm, or any other direct search method.},
author = {Sambridge, Malcolm},
doi = {10.1046/j.1365-246X.1999.00900.x},
isbn = {1365-246X},
issn = {0956540X},
journal = {Geophysical Journal International},
keywords = {Numerical techniques,Receiver functions,Waveform inversion},
title = {{Geophysical inversion with a neighbourhood algorithm--II. Appraising the ensemble}},
year = {1999}
}
@incollection{Buckheit1995,
abstract = {Wavelab is a library of wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines.},
address = {New York, NY},
author = {Buckheit, Jonathan B and Donoho, David L},
booktitle = {Wavelets and Statistics},
doi = {10.1007/978-1-4612-2544-7_5},
editor = {Antoniadis, Anestis and Oppenheim, Georges},
isbn = {978-1-4612-2544-7},
pages = {55--81},
publisher = {Springer New York},
title = {{WaveLab and Reproducible Research}},
url = {https://doi.org/10.1007/978-1-4612-2544-7{\_}5},
year = {1995}
}
@article{Khan2011,
abstract = {We jointly invert local fundamental-mode and higher-order surface-wave phase-velocities for radial models of the thermo-chemical and anisotropic physical structure of the Earth's mantle to {\&}{\#}8764;1000 km depth beneath the North American continent. Inversion for thermo-chemical state relies on a self-consistent thermodynamic method whereby phase equilibria and physical properties (P-, S-wave velocity and density) are computed as functions of composition (in the Na2O-CaO-FeO-MgO-Al2O3-SiO2 model system), pressure and temperature. We employ a sampling-based strategy to solve the non-linear inverse problem relying on a Markov Chain Monte Carlo method to sample the posterior distribution in the model space. A range of models fitting the observations within uncertainties are obtained from which any statistics can be estimated. To further refine sampled models we compute geoid anomalies for a collection of these and compare with observations, exemplifying a posteriori filtering through the use of additional data. Our thermo-chemical maps reveal the tectonically stable older eastern parts of North America to be chemically depleted (high Mg{\#}) and colder ({\textgreater}200°C) relative to the active younger regions (western margin and oceans). In the transition zone the thermo-chemical structure decouples from that of the upper mantle, with a relatively hot thermal anomaly appearing beneath the cratonic area that likely extends into the lower mantle. In the lower mantle no consistent large-scale thermo-chemical heterogeneities are observed, although our results do suggest distinct upper and lower mantle compositions. Concerning anisotropy structure, we find evidence for a number of distinct anisotropic layers pervading the mantle, including transition zone and upper-most lower mantle.},
author = {Khan, A. and Zunino, A. and Deschamps, F.},
doi = {10.1029/2011JB008380},
issn = {21699356},
journal = {Journal of Geophysical Research: Solid Earth},
title = {{The thermo-chemical and physical structure beneath the North American continent from Bayesian inversion of surface-wave phase velocities}},
year = {2011}
}
@article{Akeret2015,
abstract = {Bayesian inference is often used in cosmology and astrophysics to derive constraints on model parameters from observations. This approach relies on the ability to compute the likelihood of the data given a choice of model parameters. In many practical situations, the likelihood function may however be unavailable or intractable due to non-gaussian errors, non-linear measurements processes, or complex data formats such as catalogs and maps. In these cases, the simulation of mock data sets can often be made through forward modeling. We discuss how Approximate Bayesian Computation (ABC) can be used in these cases to derive an approximation to the posterior constraints using simulated data sets. This technique relies on the sampling of the parameter set, a distance metric to quantify the difference between the observation and the simulations and summary statistics to compress the information in the data. We first review the principles of ABC and discuss its implementation using a Population Monte-Carlo (PMC) algorithm and the Mahalanobis distance metric. We test the performance of the implementation using a Gaussian toy model. We then apply the ABC technique to the practical case of the calibration of image simulations for wide field cosmological surveys. We find that the ABC analysis is able to provide reliable parameter constraints for this problem and is therefore a promising technique for other applications in cosmology and astrophysics. Our implementation of the ABC PMC method is made available via a public code release.},
archivePrefix = {arXiv},
arxivId = {1504.07245},
author = {Akeret, Jo?l and Refregier, Alexandre and Amara, Adam and Seehars, Sebastian and Hasner, Caspar},
doi = {10.1088/1475-7516/2015/08/043},
eprint = {1504.07245},
issn = {14757516},
journal = {Journal of Cosmology and Astroparticle Physics},
keywords = {cosmological simulations,non-gaussianity,weak gravitational lensing},
number = {8},
title = {{Approximate Bayesian computation for forward modeling in cosmology}},
volume = {2015},
year = {2015}
}
@article{Dutta2017,
abstract = {A numerical model that quantitatively describes how platelets in a shear flow adhere and aggregate on a deposition surface has been recently developed in Chopard et al. (2015); Chopard et al. (2017). Five parameters specify the deposition process and are relevant for a biomedical understanding of the phenomena. Experiments give observations, at five time intervals, on the average size of the aggregation clusters, their number per {\$}\backslashmbox{\{}mm{\}}{\^{}}2{\$}, the number of platelets and the ones activated per {\$}\backslashmu\backslashell{\$} still in suspension. By comparing {\$}in-vitro{\$} experiments with simulations, the model parameters can be manually tuned (Chopard et al. (2015); Chopard et al. (2017)). Here, we use instead approximate Bayesian computation (ABC) to calibrate the parameters in a data-driven automatic manner. ABC requires a prior distribution for the parameters, which we take to be uniform over a known range of plausible parameter values. ABC requires the generation of many pseudo-data by expensive simulation runs, we have thus developed an high performance computing (HPC) ABC framework, taking into account accuracy and scalability. The present approach can be used to build a new generation of platelets functionality tests for patients, by combining {\$}in-vitro{\$} observation, mathematical modeling, Bayesian inference and high performance computing.},
archivePrefix = {arXiv},
arxivId = {1710.01054},
author = {Dutta, Ritabrata and Chopard, Bastien and L{\"{a}}tt, Jonas and Dubois, Frank and Boudjeltia, Karim Zouaoui and Mira, Antonietta},
eprint = {1710.01054},
journal = {Arxiv},
pages = {1--15},
title = {{Parameter estimation of platelets deposition: Approximate Bayesian computation with high performance computing}},
url = {http://arxiv.org/abs/1710.01054},
year = {2017}
}
